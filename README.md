# How does Transformers without PE Learn Positional Information?

## Overview
---
This repository contains the code for our investigation of positional encoding in Transformers.

### Notes on the implementation 
---
This codebase forked from [teaching arithmetic](https://github.com/lee-ny/teaching_arithmetic)
- [Here](teaching_arithmetic_pe/pe_info) contains our modifications.
- NOPE has been implemented.
- Control on Layerwise positional encoding has been added.