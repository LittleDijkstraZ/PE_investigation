{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "\n",
    "# from model import GPTConfig, GPT\n",
    "from pe_info.model_nope import GPTConfig as GPTConfig_nope, GPT as GPT_nope\n",
    "from main_utils import *\n",
    "from vis_utils import *\n",
    "\n",
    "\n",
    "# refresh parames from here\n",
    "\n",
    "from vis_params import * # import the default parameters\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith(\n",
    "    '_') and isinstance(v, (int, float, bool, str, type(None)))]\n",
    "# exec(open('./pe_info/config2_pe/modp/jason_train_addition_bal.py').read()) # overrides from command line or config file\n",
    "# exec(open('./pe_info/config2_pe/mod3/jason_train_addition_bal.py').read()) # overrides from command line or config file\n",
    "\n",
    "# exec(open('./pe_info/config2_pe/paridy/jason_train_addition_bal.py').read()) # overrides from command line or config file\n",
    "\n",
    "# overrides from command line or config file\n",
    "\n",
    "# exec(open('./pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py').read())\n",
    "# exec(open('./pe_info/config2_pe/order/jason_train_addition_bal.py').read())\n",
    "# exec(open('./pe_info/config2_pe/rev/rev16.py').read())\n",
    "# exec(open('./pe_info/config2_pe/identify/identify9.py').read())\n",
    "# exec(open('./pe_info/config2_pe/identify/identify9.py').read())\n",
    "\n",
    "exec(open('./pe_info/config2_pe/wherex/wherex9.py').read())\n",
    "\n",
    "\n",
    "\n",
    "# exec(open('./pe_info/config2_pe/wherex/wherex78x7.py').read())\n",
    "\n",
    "# exec(open('./pe_info/config2_pe/wherex/wherex78x7.py').read())\n",
    "\n",
    "# exec(open('./pe_info/config2_pe/rev/add3.py').read())\n",
    "\n",
    "\n",
    "# exec(open('./pe_info/config2_pe/modclean/jason_train_addition_bal.py').read())\n",
    "\n",
    "\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "val_data_path = start.split(':')[-1] # get the test data path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating meta file for all reasonable characters...\n",
      "all the unique characters: \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "vocab size: 96\n",
      "data has 218,925 tokens\n",
      "data has 218,885 tokens\n"
     ]
    }
   ],
   "source": [
    "# for later use in torch.autocast\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32,\n",
    "           'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "    device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "if data_type == 'binary':\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data = np.memmap(os.path.join(\n",
    "        data_dir, train_data_path), dtype=np.uint16, mode='r')\n",
    "    val_data = np.memmap(os.path.join(\n",
    "        data_dir, val_data_path), dtype=np.uint16, mode='r')\n",
    "    if train_both:\n",
    "        train_data2 = np.memmap(os.path.join(\n",
    "            data_dir, train_data_path2), dtype=np.uint16, mode='r')\n",
    "        val_data2 = np.memmap(os.path.join(\n",
    "            data_dir, val_data_path2), dtype=np.uint16, mode='r')\n",
    "    if eval_text:\n",
    "        if eval_text_data_path is None:\n",
    "            print(\n",
    "                'eval_text_data_path is None!!! No binary file to evaluate perplexity on.')\n",
    "        eval_text_data = np.memmap(\n",
    "            eval_text_data_path, dtype=np.uint16, mode='r')\n",
    "    # test_data_str = None # test_data for addition testing will be handled with \"start\"\n",
    "    meta_path = None\n",
    "else:\n",
    "    # check for data_format\n",
    "    if data_type == 'text':\n",
    "        if ('reverse' in data_format and not reverse_c) or (reverse_c and 'reverse' not in data_format):\n",
    "            raise ValueError(\n",
    "                'reverse_c must be True for data_format == \"reverse\"')\n",
    "        elif (data_format == 'algo_reasoning' and not algo_reason) or (algo_reason and data_format != 'algo_reasoning'):\n",
    "            raise ValueError(\n",
    "                'algo_reason must be True for data_format == \"algo_reasoning\"')\n",
    "    meta_path_specified = False\n",
    "\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data_path = os.path.join(data_dir, train_data_path)\n",
    "    # val_data = os.path.join(data_dir, val_data_path)\n",
    "    train_data_list = get_data_list(\n",
    "        train_data_path, operator=operator)  # a list of (x, y, op)\n",
    "    val_data_list = get_data_list(filename=val_data_path, operator=operator)\n",
    "    train_data_str = generate_data_str(train_data_list, operator=operator, format=data_format, train=True,\n",
    "                                       shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    val_data_str = generate_data_str(val_data_list, operator=operator, format=data_format, train=True,\n",
    "                                     shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    meta, meta_path, data_encoder, data_decoder = create_meta_file(\n",
    "        vocabulary=vocabulary, input_data_str=train_data_str, tokenizer=tokenizer_type)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    train_data = data_encoder(train_data_str)\n",
    "    val_data = data_encoder(val_data_str)\n",
    "    if eval_addition_train and start_train is None:\n",
    "        # specify the start_train to be our train data file\n",
    "        start_train = f\"FILE:{train_data_path}\"\n",
    "\n",
    "    if train_both:\n",
    "        # This is for the case where we use two different datasets for training\n",
    "        # we sample from both with a specified ratio - data_ratio\n",
    "        # TODO: let's leave this here for now.\n",
    "        train_data2 = np.memmap(os.path.join(\n",
    "            data_dir, train_data_path2), dtype=np.uint16, mode='r')\n",
    "        val_data2 = np.memmap(os.path.join(\n",
    "            data_dir, val_data_path2), dtype=np.uint16, mode='r')\n",
    "\n",
    "    if eval_text:\n",
    "        # eval_text_data = np.memmap(eval_text_data_path, dtype=np.uint16, mode='r')\n",
    "        text_data_list = get_data_list(eval_text_data_path, operator='text')\n",
    "        text_data_str = generate_data_str(\n",
    "            text_data_list, operator='text', format=data_format, train=False, shuffle=False)\n",
    "        eval_text_data = data_encoder(text_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1 tokens\n",
      "data has 1 tokens\n",
      "data has 1 tokens\n",
      "data has 1 tokens\n"
     ]
    }
   ],
   "source": [
    "from numpy import block\n",
    "\n",
    "\n",
    "space_token = data_encoder(' ')[0]\n",
    "switch_line_token = data_encoder('\\n')[0]\n",
    "equal_token = data_encoder('=')[0]\n",
    "dollar_token = data_encoder('$')[0]\n",
    "# non_causal_fix_length = 15\n",
    "# non_causal_fix_length = 27\n",
    "answer_length = 1\n",
    "\n",
    "\n",
    "def get_batch(split, autoregressive_training=False, batch_size=batch_size):\n",
    "    global causal_training\n",
    "    attn_mask = None\n",
    "    w = None\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    if train_both:\n",
    "        data2 = train_data2 if split == 'train' else val_data2\n",
    "        batch_size2 = int(batch_size*data_ratio)\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size-batch_size2,))\n",
    "        ix2 = torch.randint(len(data2) - block_size, (batch_size2,))\n",
    "    else:\n",
    "        if causal_training:\n",
    "            ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        else:\n",
    "            split_points = np.where(data == (switch_line_token))[0]\n",
    "            answer_split_points = np.where(data == (equal_token))[0]\n",
    "            answer_length_list = split_points - answer_split_points - 1\n",
    "            split_points = split_points + 1  # i should have had this\n",
    "            split_points = np.hstack([np.array([0]), split_points.flatten()])\n",
    "\n",
    "            sample_length_list = np.diff(split_points)\n",
    "            start_points = split_points[:-1]\n",
    "\n",
    "            randidx = np.random.permutation(len(start_points))[:batch_size]\n",
    "            ix = start_points[randidx]\n",
    "            sample_length_list = sample_length_list[randidx]\n",
    "            answer_length_list = answer_length_list[randidx]\n",
    "\n",
    "    if causal_training:\n",
    "        x = torch.stack(\n",
    "            [torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack(\n",
    "            [torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    else:\n",
    "        remove_dollar_count = 1 if dollar_token in data else 0\n",
    "        if not autoregressive_training:\n",
    "            cur_answer_length_list = np.random.randint(\n",
    "                1+remove_dollar_count, answer_length_list+1) + 1\n",
    "        else:\n",
    "            cur_answer_length_list = answer_length_list + 1 + 4\n",
    "        x = [data[ix[i]:ix[i]+sample_length_list[i]-cur_answer_length_list[i]\n",
    "                  ].astype(np.int64) for i in range(len(ix))]\n",
    "        x_len = [len(x[i]) for i in range(len(x))]\n",
    "        pad_to_length = max(x_len)\n",
    "        min_length = min(x_len)\n",
    "        # only do padding when the length is not equal\n",
    "        if pad_to_length > min_length:\n",
    "            x = np.vstack([np.pad(x[i], (pad_to_length-len(x[i]), 0), mode='constant',\n",
    "                          constant_values=space_token) for i in range(len(x))])\n",
    "            attn_mask = np.ones_like(x)\n",
    "            # mask out the paddings\n",
    "            attn_mask[x == space_token] = 0\n",
    "            attn_mask = attn_mask[..., None]\n",
    "            attn_mask = attn_mask @ attn_mask.transpose(0, 2, 1)\n",
    "            attn_mask = attn_mask.astype(bool)\n",
    "            if (attn_mask == 1).all():\n",
    "                attn_mask = None\n",
    "            else:\n",
    "                attn_mask = torch.from_numpy(attn_mask)\n",
    "        else:\n",
    "            x = np.vstack(x)\n",
    "            attn_mask = None\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        # predict the next digit\n",
    "        if not autoregressive_training:\n",
    "            y = torch.stack([torch.from_numpy((data[ix[i]+sample_length_list[i]-cur_answer_length_list[i]:ix[i] +\n",
    "                            1+sample_length_list[i]-cur_answer_length_list[i]]).astype(np.int64)) for i in range(len(ix))])\n",
    "        else:\n",
    "            y = [torch.from_numpy((data[ix[i]+sample_length_list[i]-cur_answer_length_list[i]:ix[i]-1+sample_length_list[i]]).astype(np.int64)) for i in range(len(ix))]\n",
    "            max_len_y = max([len(y[i]) for i in range(len(y))])\n",
    "            y = np.vstack([np.pad(y[i], (0, max_len_y-len(y[i])), mode='constant',\n",
    "                          constant_values=space_token) for i in range(len(y))])\n",
    "            y = torch.from_numpy(y)\n",
    "            w = torch.ones_like(y)\n",
    "            w[y == space_token] = 0\n",
    "\n",
    "    if train_both:\n",
    "        x2 = torch.stack(\n",
    "            [torch.from_numpy((data2[i:i+block_size]).astype(np.int64)) for i in ix2])\n",
    "        y2 = torch.stack(\n",
    "            [torch.from_numpy((data2[i+1:i+1+block_size]).astype(np.int64)) for i in ix2])\n",
    "        x = torch.cat([x, x2])\n",
    "        y = torch.cat([y, y2])\n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(\n",
    "            device, non_blocking=True)\n",
    "        if autoregressive_training:\n",
    "            w = w.pin_memory().to(device, non_blocking=True)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "    # attn_mask = None\n",
    "    return x, y, attn_mask, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20]) torch.Size([1, 1])\n",
      "wherex(585739270,7)= 3\n",
      "w,h,e,r,e,x,(,5,8,5,7,3,9,2,7,0,,,7,),=,\n"
     ]
    }
   ],
   "source": [
    "causal_training = False\n",
    "x, y, z, w = get_batch('test', autoregressive_training=False, batch_size=1)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "if z is not None:\n",
    "    print(z.shape)\n",
    "\n",
    "for hidx in range(min(10, len(x))):\n",
    "    print(data_decoder(x[hidx].cpu().numpy()), data_decoder(y[hidx].cpu().numpy()))\n",
    "    for xi in x[hidx].cpu().numpy():\n",
    "        print(data_decoder(xi[..., None]), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "Loading meta from meta_all_ascii_chars.pkl...\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "best_perplexity = 1e9  # on text data\n",
    "best_accuracy = -1  # on addition data\n",
    "\n",
    "if meta_path_specified:\n",
    "    # attempt to derive vocab_size from the dataset\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "    else:\n",
    "        meta_path = None\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout)  # start with model_args from command line\n",
    "\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "# determine the vocab size we'll use for from-scratch training\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "\n",
    "encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Flash Attention\n",
      "Block 0: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 1: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 2: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 3: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 4: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 5: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "PE in use: original\n",
      "number of parameters: 10.66M\n",
      "test_run\n",
      "0 \n",
      "1 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(96, 384)\n",
      "    (drop): Dropout(p=0.2, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (pre_att_identity): Identity()\n",
      "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
      "          (identity): Identity()\n",
      "          (iq): Identity()\n",
      "          (ik): Identity()\n",
      "          (iv): Identity()\n",
      "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_identity): Identity()\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "    (wpe): Embedding(256, 384)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=384, out_features=96, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, use_flash=use_flash,\n",
    "                  use_residual=use_residual, use_pe=use_pe,\n",
    "                  no_att_residual=no_att_residual,\n",
    "                  no_mlp_residual=no_mlp_residual,\n",
    "                  layerwise_pe=layerwise_pe,\n",
    "                  permute=permute,\n",
    "                  not_causal=not_causal\n",
    "                  )  # jason's change\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "# if use_pe=='original':\n",
    "#     gptconf = GPTConfig(**model_args)\n",
    "#     model = GPT(gptconf)\n",
    "# elif use_pe == 'nope':\n",
    "gptconf = GPTConfig_nope(**model_args)\n",
    "model = GPT_nope(gptconf)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5, device='cuda:0')\n",
      "wherex(708982578\n",
      "tensor([[13, 24, 10, 30, 17]], device='cuda:0')\n",
      ",7)=0\n",
      "H/3,k\n",
      "tensor(4.5483, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z, w = get_batch('valid', autoregressive_training=True, batch_size=1)\n",
    "print(w.sum())\n",
    "with torch.no_grad():\n",
    "    # model.autoregressive_training(x, y, max_new_tokens=y.shape, attn_mask=z)\n",
    "    model.train()\n",
    "    # outs, loss = model(x, y, attn_mask=z, causal_training=causal_training)\n",
    "    outs, loss = model.autoregressive_training(\n",
    "        x, y, w,  max_new_tokens=y.shape[-1], attn_mask=z)\n",
    "\n",
    "    print(decode(x[:10].flatten().detach().cpu().numpy()))\n",
    "    print(y[:10])\n",
    "    print(decode(y[:10].flatten().detach().cpu().numpy()))\n",
    "    print(decode(outs[:10].argmax(-1).flatten().detach().cpu().numpy()))\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_path, \n",
    "                    model_config, \n",
    "                    model_type, \n",
    "                    device='cuda', \n",
    "                    return_config=False, \n",
    "                    return_model=True,\n",
    "                    init=False,\n",
    "                    init_additional_config={},):\n",
    "    # load ckpt into model\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    model_args = checkpoint['model_args']\n",
    "    # for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    # model_args[k] = checkpoint_model_args[k]\n",
    "    # for k in checkpoint_model_args:\n",
    "    #         model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    original_gptconf = model_config(**model_args)\n",
    "    gptconf = model_config(**model_args)\n",
    "    if return_model:\n",
    "        if not init:\n",
    "            model = model_type(original_gptconf)\n",
    "            \n",
    "            state_dict = checkpoint['model']\n",
    "            # fix the keys of the state dictionary :(\n",
    "            # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "            unwanted_prefix = '_orig_mod.'\n",
    "            for k, v in list(state_dict.items()):\n",
    "                if k.startswith(unwanted_prefix):\n",
    "                    state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            # override with keys\n",
    "            for k in init_additional_config:\n",
    "                original_gptconf.__dict__[k] = init_additional_config[k]\n",
    "            model = model_type(original_gptconf)\n",
    "   \n",
    "    if return_config:\n",
    "        if return_model:\n",
    "            return model, gptconf\n",
    "        else: \n",
    "            return gptconf\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def generate_output(model, \n",
    "                    prompt, \n",
    "                    max_new_tokens=5, \n",
    "                    attn_mask=None, \n",
    "                    top_k=None):\n",
    "    # temperature = 0.8\n",
    "    # top_k = 200\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        model = torch.compile(model)  # requires PyTorch 2.0 (optional)\n",
    "    # run generation\n",
    "\n",
    "    start_ids = encode(prompt)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_samples = 1\n",
    "        for k in range(num_samples):\n",
    "\n",
    "            attn_mask = None\n",
    "\n",
    "            y = model.generate(x, max_new_tokens,\n",
    "                               attn_mask=attn_mask, top_k=top_k)\n",
    "\n",
    "    return decode(y[0].tolist())\n",
    "\n",
    "\n",
    "def PCA_analysis(prompt, embs, out_text, config_dir):\n",
    "    pca = PCA(n_components=2)\n",
    "    new_x = pca.fit_transform(embs.cpu().numpy())\n",
    "    data = []\n",
    "    for i, (text, pt) in enumerate(zip(prompt, new_x)):\n",
    "        trace = go.Scatter(\n",
    "            x=[pt[0]],\n",
    "            y=[pt[1]],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=10),  # Adjust the size of the points\n",
    "            text=[str(i+1)],\n",
    "            textposition='middle center',  # Center the text within the marker\n",
    "            name=text,\n",
    "            textfont=dict(\n",
    "                family='Times New Rotman',  # Specify the font family\n",
    "                size=18,  # Adjust the font size\n",
    "                color='black',  # Adjust the font color\n",
    "            ),\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        xaxis=dict(title='Principal Component 1'),\n",
    "        yaxis=dict(title='Principal Component 2'),\n",
    "        title=f'PCA visualization for {prompt}'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n",
    "#   out_num = out_text.split('=')[-1][:-1]\n",
    "#   eqn = out_text.split('=')[0]\n",
    "#   out_text = eqn+'='+out_num[::-1]+out_text[-1]\n",
    "    print(out_text)\n",
    "    # print(new_x)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    import plotly.io as pio\n",
    "    pio.write_html(fig, f'./{config_dir}/{prompt}.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# config_dir = \"out2/addition_reverse/\"\n",
    "\n",
    "# ckpt = f\"{config_dir}/ckpt_10000_final.pt\"\n",
    "# import yaml\n",
    "# with open(f'{config_dir}/addition_reverse/config.yaml') as f:\n",
    "#   config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# gptconf = GPTConfig_nope(**model_args)\n",
    "# model = GPT_nope(gptconf)\n",
    "# model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope)\n",
    "# model = load_checkpoint(ckpt, GPTConfig, GPT)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import glob\n",
    "import yaml\n",
    "\n",
    "\n",
    "exp_list = []\n",
    "# exp_list\n",
    "# exp_list = glob.glob('./outputs/residual_exp/*') + glob.glob('./outputs/nope_residual_exp/*')\n",
    "# exp_list = glob.glob('./outputs/modp_nope_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/modp_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/modp_nc_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/modp_nc_nope_residual_exp/*') \n",
    "# exp_list = glob.glob('./outputs_ref/modclean_nope_nope/*')\n",
    "\n",
    "\n",
    "# exp_list = glob.glob('./outputs/paridy_nope_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/paridy_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/paridy_nc_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/paridy_nc_nope_residual_exp/*') \n",
    "\n",
    "\n",
    "# exp_list = glob.glob('./outputs/mod3_nope_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/mod3_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/mod3_nc_residual_exp/*') \\\n",
    "#     + glob.glob('./outputs/mod3_nc_nope_residual_exp/*') \n",
    "\n",
    "# exp_list = glob.glob('./outputs/paridy_nope_residual_exp/*')\n",
    "\n",
    "# exp_list = glob.glob('./outputs/residual_exp/*')\n",
    "# exp_list = glob.glob('./outputs_permute/add3_nope*/*') \\\n",
    "#     + glob.glob('./outputs_permute/add3_residual*/*') \\\n",
    "#     + glob.glob('./outputs_permute/add3_shuffle_6_*/*') \n",
    "# exp_list = glob.glob('./outputs_permute/add3_shuffle_6_*/*')\n",
    "\n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_16_*/*') \\\n",
    "#     + glob.glob('./outputs_permute/add3_remove_16_lwp_residual*/*') \n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_8_*/*')\n",
    "\n",
    "\n",
    "# ===== removing residual connections\n",
    "# exp_list = glob.glob('./outputs/nope_residual_exp/*')\n",
    "# exp_list = [p for p in exp_list if 'sd222' in p]\n",
    "\n",
    "# exp_list += [p for p in glob.glob('./outputs_permute/add3_remove_8_rotary_rotary/*') if 'lwpTrue' not in p]\n",
    "# exp_list += glob.glob('./outputs_permute/add3_remove_8_T5_T5/*')\n",
    "# exp_list += [p for p in glob.glob('./outputs_permute/add3_remove_8_sin_residual_exp/*') if 'lwpTrue' not in p]\n",
    "# exp_list += glob.glob('./outputs_permute/add3_remove_8_residual_exp/*')\n",
    "\n",
    "\n",
    "# ===== 6-12 layers\n",
    "exp_list = glob.glob('./outputs_ref_*/rev16*/*') \n",
    "# exp_list = glob.glob('./outputs_ref_*/*/*') \n",
    "\n",
    "# exp_list = glob.glob('./outputs_ref_6/order*/*') \n",
    "# exp_list = glob.glob('./outputs_ref_6/add*/*') \n",
    "# exp_list = glob.glob('./outputs_ref/add*/*') \n",
    "\n",
    "# exp_list = glob.glob('./outputs_ref_6/wherex9_nc_original_abs/*')\n",
    "# exp_list = glob.glob('./outputs_ref_6/identify*/*') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exp_list = glob.glob('./outputs_ref_2/rev16*/*') \n",
    "\n",
    "# exp_list = glob.glob('./outputs_ref_2/wherex78x7*/*') \n",
    "\n",
    "\n",
    "    # glob.glob('./outputs_ref/wherex*/*')\n",
    "# exp_list = [p for p in exp_list if 'order' in p]\n",
    "# exp_list = [p for p in exp_list if 'rev16' in p]\n",
    "\n",
    "\n",
    "# exp_list = glob.glob('./outputs_permute/add3_lwp_residual_exp/*')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_8_sin_sin/*')\n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_8_*/*')\n",
    "# exp_list = exp_list[len(exp_list)//2:]\n",
    "\n",
    "\n",
    "\n",
    "# exp_list = ['./outputs/nope_residual_exp/addition_reverse_sd111_T2401311659_nope_res=[1, 2, 3, 4, 5]']\n",
    "\n",
    "# exp_list = [p for p in exp_list if '[0' not in p]\n",
    "# exp_list  = [p for p in exp_list if 'res=[0, 1, 2, 3, 4, 5]' in p or 'res=[3, 4, 5]' in p]\n",
    "# exp_list  = [p for p in exp_list if 'res=[0, 1, 2, 3, 4, 5]' in p and '246' not in p and '247' not in p]\n",
    "# exp_list  = [p for p in exp_list if 'res=[2, 3, 4, 5]' in p]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exp_list = [[x, x.split('/')[-1]] for x in exp_list] \n",
    "print(len(exp_list))\n",
    "# calc ratio\n",
    "# increase contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a9c42cd45644c5a3c3c6982a36eef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded everything: 30\n",
      "./outputs_ref_6/rev16_nope_nope/rev16_nope_sd240_T2408251628_nope/ckpt_10000_acc.pt\n",
      "causal_training is False\n",
      "torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "from IPython.utils import io\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# fixed_length = 10\n",
    "task_type = 'wherex9'\n",
    "fixed_length = {\n",
    "    'add':9,\n",
    "    'rev16':22,\n",
    "    'order':19,\n",
    "    'wherex7':17,\n",
    "    'wherex9':20,\n",
    "    'identify':22,\n",
    "}[task_type]\n",
    "# fixed_length = 10\n",
    "\n",
    "# total_tokens = 2048\n",
    "total_samples = 32\n",
    "# sample_num = 1024\n",
    "sample_num = 32\n",
    "\n",
    "all_level_input_act_list = []\n",
    "all_out_list = []\n",
    "\n",
    "rand_perm = False\n",
    "if rand_perm:\n",
    "    print(\"permutation added\")\n",
    "equal_distancing_exp = False\n",
    "use_1_sample = False\n",
    "model_init = False\n",
    "if model_init:\n",
    "    print(\"model init\")\n",
    "    init_additional_config = {\n",
    "        'n_head':1,\n",
    "    }\n",
    "    # put additional configs that needs to be initialized here\n",
    "else:\n",
    "    init_additional_config = {}\n",
    "\n",
    "small_dim = False\n",
    "if small_dim:\n",
    "    print(\"small dim\")\n",
    "\n",
    "\n",
    "ablation_config = {\n",
    "    \"V_out\": False,\n",
    "    \"no_K\": False,\n",
    "    \"no_V\": False,\n",
    "    \"no_Q\": False,\n",
    "    \"shrink_x\": False,\n",
    "    # \"c_proj\": True,\n",
    "    # only one at a time\n",
    "    \"func_config\": {\n",
    "        0: None,\n",
    "        1:\"nodiv\", \n",
    "        2:\"softmax\", \n",
    "        3:\"abs\", \n",
    "        4:\"relu\", \n",
    "        5:\"divsum\", \n",
    "        6:\"gelu\",\n",
    "        7:\"gelu_divabs\",\n",
    "        8:\"sigmoid\",\n",
    "        9:\"same\"}[0],\n",
    "}\n",
    "for k in ablation_config:\n",
    "    if ablation_config[k]:\n",
    "        if k == \"func_config\":\n",
    "            print(f\"{k}: {ablation_config[k]}\")\n",
    "        else:\n",
    "            print(f\"{k} is on\")\n",
    "\n",
    "model_list = []\n",
    "useful_name_list = []\n",
    "for idx, (config_dir, model_config_fold) in enumerate(tqdm(exp_list)):\n",
    "    glob_dir = config_dir.replace(\"[\", \"*\").replace(\"]\", \"*\")\n",
    "    try:\n",
    "        yaml_path = glob.glob(f\"{glob_dir}/**/config.yaml\")[0]\n",
    "        csv_path = glob.glob(f\"{glob_dir}/**/result.csv\")[0]\n",
    "        revised_glob_dir = \"/\".join(yaml_path.split(\"/\")[:-2])\n",
    "        exp_list[idx][0] = revised_glob_dir\n",
    "        exp_list[idx][1] = revised_glob_dir.split(\"/\")[-1]\n",
    "\n",
    "        config_dir = \"/\".join(yaml_path.split(\"/\")[:-2])\n",
    "        with open(yaml_path) as f:\n",
    "            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # ckpt = f\"{config_dir}/ckpt_10000_acc.pt\"\n",
    "        glob_dir = config_dir.replace(\"[\", \"*\").replace(\"]\", \"*\")\n",
    "        try:\n",
    "            all_ckpts = sorted(glob.glob(f\"{glob_dir}/ckpt_*.pt\"), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        except:\n",
    "            all_ckpts = [gdir for gdir in glob.glob(f\"{glob_dir}/ckpt_*.pt\") if '_acc' in gdir]\n",
    "        # ckpt = f\"{config_dir}/ckpt_2000_acc.pt\"\n",
    "    \n",
    "        gptconfig = load_checkpoint(all_ckpts[0], GPTConfig_nope, GPT_nope, device='cuda', return_config=True, return_model=False)\n",
    "        if gptconfig.n_embd != 384: continue\n",
    "        if gptconfig.n_layer != 6 : continue\n",
    "        \n",
    "        # add the initialized model\n",
    "        set_seed(np.random.randint(0,200))\n",
    "        for init_scheme in [\n",
    "                            'default',\n",
    "                            # # \"('normal',0,0.002)\", \n",
    "                            # \"('normal',0,0.02)\",\n",
    "                            # # \"('normal',0,0.2)\",\n",
    "                            # # \"('normal',0,2)\", \n",
    "                            # # \"('normal',4,0.02)\",\n",
    "                            # # \"('normal',8,0.02)\",\n",
    "                            # # \"('normal',100,0.02)\",                        \n",
    "                            # '(\"uniform\",-0.02,0.02)', \n",
    "                            # '(\"uniform\",-0.001,0.001)', \n",
    "\n",
    "                            # '(\"uniform\",-1,1)', \n",
    "                            # '(\"uniform\",-10,10)', \n",
    "\n",
    "                            # '(xavier,1)'\n",
    "                            ]:\n",
    "        # for init_scheme in ['default']:\n",
    "\n",
    "            ckpt = all_ckpts[0]\n",
    "            print(ckpt)\n",
    "            with io.capture_output() as captured:\n",
    "                model, gptconfig = load_checkpoint(\n",
    "                    ckpt,\n",
    "                    GPTConfig_nope,\n",
    "                    GPT_nope,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    return_config=True,\n",
    "                    init=True,\n",
    "                    # init_additional_config={\n",
    "                    #     'init_scheme': init_scheme,\n",
    "                    # },\n",
    "                )\n",
    "\n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            model_list.append(model)\n",
    "            try:\n",
    "                iter_num = int(ckpt.split('_')[-1].split('.')[0])\n",
    "                convergence = 0\n",
    "                cur_model_name = 'acc='+str(int(convergence))+ '_' \\\n",
    "                    + '/'.join(ckpt.split('/')[3:]).replace(str(iter_num), '0') \\\n",
    "                    + f'_nembd={gptconfig.n_embd}_nlayers={gptconfig.n_layer}' + f'_{init_scheme}'\n",
    "            except:\n",
    "                cur_model_name = 'acc=0'+ '_' \\\n",
    "                    + '/'.join(ckpt.split('/')[3:]) \\\n",
    "                    + f'_nembd={gptconfig.n_embd}_nlayers={gptconfig.n_layer}' + f'_Init_{init_scheme}'\n",
    "            useful_name_list.append(cur_model_name)\n",
    "            \n",
    "        # for ckpt in all_ckpts[len(all_ckpts)//2:len(all_ckpts)//2+1] + all_ckpts[-1:]:\n",
    "\n",
    "        '''get trained model results'''\n",
    "        # for ckpt in all_ckpts[-1:]:\n",
    "\n",
    "        for ckpt in all_ckpts:\n",
    "\n",
    "            with io.capture_output() as captured:\n",
    "                # model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda')\n",
    "                # model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "                model, gptconfig = load_checkpoint(\n",
    "                    ckpt,\n",
    "                    GPTConfig_nope,\n",
    "                    GPT_nope,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    return_config=True,\n",
    "                    init=model_init,\n",
    "                    init_additional_config=init_additional_config,\n",
    "                )\n",
    "                if small_dim:\n",
    "                    gptconfig.n_head = 1\n",
    "                    # gptconfig.not_causal = True\n",
    "                    gptconfig.n_embd = 16\n",
    "                    model = GPT_nope(gptconfig)\n",
    "                                \n",
    "                model.eval()\n",
    "                model.to(device)\n",
    "                model_list.append(model)\n",
    "                try:\n",
    "                    iter_num = int(ckpt.split('_')[-1].split('.')[0])\n",
    "                    convergence = df.loc[df['iter']==iter_num, 'test_acc'].values[0]\n",
    "                  \n",
    "                except:\n",
    "                    convergence = df.loc[:, 'test_acc'].values.max()\n",
    "                cur_model_name = 'acc='+str(int(convergence))+ '_' \\\n",
    "                        + '/'.join(ckpt.split('/')[3:]) \\\n",
    "                        + f'_nembd={gptconfig.n_embd}_nlayers={gptconfig.n_layer}' + '_trained'\n",
    "                useful_name_list.append(cur_model_name)\n",
    "\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"no model {glob_dir}\")\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "n_embd = model_list[0].config.n_embd\n",
    "equation_num = 1\n",
    "causal_training = False # for add3\n",
    "total_tokens = total_samples * 2 * fixed_length * equation_num\n",
    "diviser = 256 if causal_training else fixed_length\n",
    "samples_to_generate = total_tokens//fixed_length + total_tokens%fixed_length\n",
    "fixed_length = equation_num * fixed_length\n",
    "print('causal_training is', causal_training)\n",
    "\n",
    "\n",
    "X = get_batch(\"valid\", batch_size=samples_to_generate)[0]\n",
    "X = \"\\n\".join([decode(X[i].tolist()) for i in range(X.shape[0])])[:total_tokens] # truncate x to lower computation\n",
    "equations = X.split('\\n')\n",
    "X_n = []\n",
    "for eidx in range(0, len(equations), equation_num):\n",
    "    full_eqn = '\\n'.join(equations[eidx:eidx+equation_num])\n",
    "    # print(full_eqn)\n",
    "    if task_type=='add' and not full_eqn.startswith('$'): continue\n",
    "    if len(full_eqn)<fixed_length:\n",
    "        if len(full_eqn) < fixed_length//4: continue\n",
    "        full_eqn = '\\n' * (fixed_length-len(full_eqn))  + full_eqn\n",
    "    full_eqn = full_eqn[:fixed_length]\n",
    "    X_n.append(full_eqn)\n",
    "X_n = X_n[:total_samples]\n",
    "X = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "print(X.shape)\n",
    "\n",
    "# X_n = np.array(list(X[:len(X) // fixed_length * fixed_length])).reshape(-1, fixed_length)\n",
    "\n",
    "if rand_perm: # shuffle in input sequence\n",
    "    for xidx in range(X.shape[0]):\n",
    "        X[xidx] = X[xidx, torch.randperm(X[xidx].shape[0])]\n",
    "\n",
    "X = X.to(device)\n",
    "if use_1_sample:\n",
    "    X = X[:1]\n",
    "\n",
    "for level in range(0, 12):\n",
    "    level = level - 1\n",
    "    input_act1_list = [list() for _ in range(len(model_list))]\n",
    "    out_list = [list() for _ in range(len(model_list))]\n",
    "\n",
    "    for midx, model in enumerate(model_list):\n",
    "        if len(model.transformer.h)<=level:\n",
    "            continue\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "    \n",
    "\n",
    "        # register forward hooks on the layers of choice\n",
    "        if level >= 0:\n",
    "            # h1 = model.transformer.h[level].register_forward_hook(\n",
    "            #     getActivation(f\"layer_{level}\")\n",
    "            # )\n",
    "            # if hook_location == 'pre_attn':\n",
    "            # h1 = model.transformer.h[level].attn.pre_att_identity.register_forward_hook(\n",
    "            #     getActivation(f\"layer_{level}\")\n",
    "            # )\n",
    "            # elif hook_location == 'post_attn':\n",
    "            h1 = model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "                getActivation(f\"layer_{level}\")\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # if 'nope' in exp_list[midx][0]:\n",
    "            # h1 = model.transformer.drop.register_forward_hook(\n",
    "            #     getActivation(f\"layer_{level}\")\n",
    "            # )\n",
    "            h1 = model.transformer.drop.register_forward_hook(\n",
    "                getActivation(f\"layer_{level}\")\n",
    "            )\n",
    "            # else:\n",
    "            #     h1 = model.transformer.wte.register_forward_hook(\n",
    "            #         getActivation(f\"layer_{level}\")\n",
    "            #     )\n",
    "\n",
    "\n",
    "        h2 = model.transformer.ln_f.register_forward_hook(\n",
    "            getActivation(\"x_out\")\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out0 = model(\n",
    "                X, equal_distancing_exp=equal_distancing_exp, ablation_config=ablation_config,\n",
    "            )[0]\n",
    " \n",
    "        h1.remove()\n",
    "        h2.remove()\n",
    "\n",
    "        acts = activation[f\"layer_{level}\"].detach().cpu().numpy()\n",
    "        # print('sh',acts.shape)\n",
    "        \n",
    "        input_act1_list[midx].append(acts)\n",
    "        out_arg = out0.argmax(-1).detach().cpu().numpy()\n",
    "        decoded_out = list(decode(out_arg.flatten()))\n",
    "        \n",
    "        out_list[midx] = decoded_out\n",
    "\n",
    "    for hidx in range(len(input_act1_list)):\n",
    "        if len(input_act1_list[hidx]) == 0:\n",
    "            # input_act1_list[hidx] \n",
    "            # print(f\"no activation for \")\n",
    "            pass\n",
    "        else:\n",
    "            cur_input_act1 = np.concatenate(input_act1_list[hidx], axis=0)\n",
    "            bs, l, dim = cur_input_act1.shape\n",
    "            # print(cur_input_act1.shape)\n",
    "            input_act1_list[hidx] = cur_input_act1.reshape(bs, l, dim)\n",
    "            # out_list[hidx] = out_list[hidx]\n",
    "\n",
    "    all_level_input_act_list.append(input_act1_list)\n",
    "    all_out_list.append(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_level_input_act_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc=0_rev16_nope_sd240_T2408251628_nope/ckpt_10000_acc.pt_nembd=384_nlayers=6_Init_default',\n",
       " 'acc=99_rev16_nope_sd240_T2408251628_nope/ckpt_10000_acc.pt_nembd=384_nlayers=6_trained']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using function cosine_similarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c1c140a28944718eed67e1dde62911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='useful_name_list', options=(['acc=0_rev16_nope_sd240_T2408251628_nâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function vis_utils.get_corr.get_corr(useful_name_list, all_level_input_act_list, sim_func, X_n, folder_name, fixed_length, all_out_list, equal_distancing_exp=False, sample_idx=0, level=0, standard=6, drop_down=None, drop_down2=None, drop_down3=None, plot_type=['dot', 'corr'], all_models=False, individual_sample=True, before_after=['training', False, 'attention'], save=False, abs=False, save_all=False, accumulate_all=False, subplot_layers=True, show_vals=False)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from re import T\n",
    "import gc\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from requests import get\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size':32})\n",
    "plt.rcParams['axes.labelsize'] =32  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] =32  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] =32  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] =32  # Legend\n",
    "plt.rcParams['axes.titlesize'] =32  # Title\n",
    "\n",
    "\n",
    "sim_func = cosine_similarity\n",
    "# sim_func = lambda x, y: np.dot(x, y.T)\n",
    "# sim_func = lambda x, y: (x[None, ...] - y[:, None, ...]).sum(axis=-1)\n",
    "show_vals = False\n",
    "print(f\"using function {repr(sim_func).split(' ')[1]}\")\n",
    "\n",
    "def standardize_rows(matrix):\n",
    "    \"\"\"Standardize each row of the matrix.\"\"\"\n",
    "    mean = matrix.mean(axis=1, keepdims=True)\n",
    "    std = matrix.std(axis=1, keepdims=True)\n",
    "    return (matrix - mean) / std\n",
    "\n",
    "level_corr_mat_accum = []\n",
    "# corr_mat = []\n",
    "u1, u2, u1_name, u2_name = None, None, None, None\n",
    "\n",
    "task_name = 'add_'\n",
    "folder_name = f'corr_{task_name}' if not equal_distancing_exp else f'dot_{task_name}'\n",
    "folder_name += '_trained' if not model_init else '_init'\n",
    "\n",
    "for k in ablation_config:\n",
    "    if ablation_config[k]:\n",
    "        if k == 'func_config':\n",
    "            folder_name += f'_{ablation_config[k]}'\n",
    "        else: \n",
    "            folder_name += f'_{k}' \n",
    "from vis_utils import get_PE_tendency, generate_tendency_map\n",
    "\n",
    "all_stats = None\n",
    "\n",
    "def get_corr(sample_idx=0, \n",
    "             level=0, \n",
    "             standard=6,\n",
    "             drop_down=[], \n",
    "             drop_down2=None, \n",
    "             drop_down3=None, \n",
    "             plot_type = ['dot', 'corr'],\n",
    "             all_models = False,\n",
    "             individual_sample = True,\n",
    "             before_after = ['training', False,  'attention'],\n",
    "             save=False,  \n",
    "             abs=False, \n",
    "             save_all=False, \n",
    "             accumulate_all=False, \n",
    "             subplot_layers=True,):\n",
    "    # global corr_mat, input_act1_list\n",
    "    global u1, u2, u1_name, u2_name\n",
    "    global all_stats\n",
    "\n",
    "    idx1 = useful_name_list.index(drop_down)\n",
    "    idx2 = useful_name_list.index(drop_down2) if drop_down2 is not None else None\n",
    "    idx3 = useful_name_list.index(drop_down3) if drop_down3 is not None else None\n",
    "\n",
    "    # model_layers = [l[idx1] for l in all_level_input_act_list if len(l[idx1])!=0]\n",
    "    # level = min(level, len(model_layers)-1)\n",
    "    # print(level, idx1, useful_name_list[idx1])\n",
    "\n",
    "    for l in all_level_input_act_list:\n",
    "        if l == []:\n",
    "            return\n",
    "\n",
    "    # individual_sample = individual_sample if not save_all else False\n",
    "    if individual_sample :\n",
    "        if not equal_distancing_exp:\n",
    "            print(X_n[sample_idx:sample_idx+1])\n",
    "        else:\n",
    "            print('eqx')\n",
    "    save = True if save_all else save\n",
    "    model_range = range(idx1, idx1+1) if not save_all else range(len(useful_name_list))\n",
    "    all_stats = dict()\n",
    "    for idx1 in tqdm(model_range):\n",
    "        eqn_text = X_n[sample_idx] if not equal_distancing_exp else 'eqx'\n",
    "        \n",
    "        models_corr_mat_list = []\n",
    "        models_cm_tend_list = []\n",
    "        \n",
    "        model_name = useful_name_list[idx1]\n",
    "        acc = model_name.split('_')[0]\n",
    "        rest = '_'.join(model_name.split('_')[1:])\n",
    "        try:\n",
    "            iteration = int(re.search(r'acc_(\\d+).pt', model_name).group(1))\n",
    "        except:\n",
    "            if 'Init' in model_name:\n",
    "                iteration = 0\n",
    "            else:\n",
    "                iteration = 5000\n",
    "        imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + ' ' + eqn_text\n",
    "\n",
    "        cur_model_act_list1 = [l[idx1] for l in all_level_input_act_list if len(l[idx1])!=0]\n",
    "        models_to_plot = [cur_model_act_list1]\n",
    "        \n",
    "        has_model = True\n",
    "        if before_after=='training':\n",
    "            rest = '_'.join(rest.split('.')[0].split('_')[:-1])\n",
    "            if acc.split('=')[-1]!='0' or iteration!=0:\n",
    "                print(acc, iteration)\n",
    "                has_model = False\n",
    "                continue\n",
    "            trained_model_name = None\n",
    "            for m_idx, m_name in enumerate(useful_name_list):\n",
    "                # if rest in m_name and not (m_name==model_name):\n",
    "                if rest in m_name and 'trained' in m_name:\n",
    "                    trained_model_name = m_name\n",
    "                    # trained_cur_model = model_list[m_idx]\n",
    "                    m_idx = useful_name_list.index(m_name)\n",
    "                    cur_model_act_list2 = [l[m_idx] for l in all_level_input_act_list if len(l[m_idx])!=0]\n",
    "                    break\n",
    "            assert trained_model_name is not None, 'no trained model found'\n",
    "            print(all_out_list[0][midx][sample_idx])\n",
    "            models_to_plot.append(cur_model_act_list2)\n",
    "\n",
    "        elif before_after=='attention':\n",
    "            models_to_plot.append(cur_model_act_list1)\n",
    "        else:\n",
    "            print(all_out_list[0][idx1][sample_idx])\n",
    "        \n",
    "        print('output:', all_out_list[0][idx1][sample_idx])\n",
    "\n",
    "\n",
    "        if drop_down2 is not None:\n",
    "            cur_model_act_list2 = [l[idx2] for l in all_level_input_act_list if len(l[idx2])!=0]\n",
    "            models_to_plot.append(cur_model_act_list2)\n",
    "        if drop_down3 is not None:\n",
    "            cur_model_act_list3 = [l[idx3] for l in all_level_input_act_list if len(l[idx3])!=0]\n",
    "            models_to_plot.append(cur_model_act_list3)\n",
    "        \n",
    "\n",
    "        for cur_model_act_list in models_to_plot:\n",
    "            corr_mat_list = []\n",
    "            level_cm_tend_list = []\n",
    "            for level in tqdm(range(len(cur_model_act_list))):\n",
    "                input_act1_list = cur_model_act_list[level]\n",
    "                # global level_corr_mat_accum\n",
    "\n",
    "                u1, u2 = cur_model_act_list[level], cur_model_act_list[level]\n",
    "                # u1 = u1.T\n",
    "                # u2 = u2.T\n",
    "                if individual_sample:\n",
    "                    u1, u2 = u1[sample_idx:sample_idx+1], u2[sample_idx:sample_idx+1]\n",
    "             \n",
    "                if plot_type=='corr':\n",
    "                    corr_mat = []\n",
    "                    for b1, b2 in zip(u1, u2):\n",
    "                        u1_standardized = standardize_rows(u1)\n",
    "                        u2_standardized = standardize_rows(u2)\n",
    "                        corr_mat.append(np.dot(u1_standardized, u2_standardized.T) / (u1.shape[1]))\n",
    "                elif plot_type=='dot':\n",
    "                    corr_mat = []\n",
    "                    for b1, b2 in zip(u1, u2):\n",
    "                        corr_mat.append(sim_func(b1, b2))\n",
    "                    corr_mat_all = np.array(corr_mat)\n",
    "                    corr_mat = corr_mat_all.mean(axis=0) \n",
    "\n",
    "                if abs:\n",
    "                    corr_mat = np.abs(corr_mat)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "                cm_tend_list = []\n",
    "                for _sidx in range(len(corr_mat_all)):\n",
    "                    cm_tend_list.append(get_PE_tendency(corr_mat_all[_sidx], as_list=True)[standard])\n",
    "                           \n",
    "                # if accumulate_all:\n",
    "                #     level_corr_mat_accum.append(corr_mat[None, ...])\n",
    "\n",
    "                \n",
    "                corr_mat_list.append(corr_mat)\n",
    "                level_cm_tend_list.append(cm_tend_list)               \n",
    "\n",
    "\n",
    "                if not subplot_layers:\n",
    "                    vec_dim = corr_mat.shape[0]//fixed_length\n",
    "                    total_sum = np.abs(corr_mat).sum()\n",
    "                    block_sum = 0\n",
    "                    for i in range(0, len(corr_mat), vec_dim):\n",
    "                        block_sum += np.abs(corr_mat[i:i+vec_dim, i:i+vec_dim]).sum()\n",
    "                    ratio = block_sum/(total_sum-block_sum)\n",
    "\n",
    "                    plt.figure(figsize=(6, 5), dpi=120)\n",
    "                    plt.imshow(corr_mat, cmap='Reds') #, interpolation='nearest')\n",
    "\n",
    "                    extra_text = 'Absolute ' if abs else ''\n",
    "                    plt.colorbar(label=f'{extra_text}Correlation Coefficient')\n",
    "\n",
    "                    os.makedirs(f'./saved_plots_{folder_name}/', exist_ok=True)\n",
    "                    plt.savefig(f'./saved_plots_{folder_name}/{useful_name_list[idx1]}_avg_{abs}.pdf')\n",
    "                    plt.close()\n",
    "            models_corr_mat_list.append(corr_mat_list)\n",
    "\n",
    "            models_cm_tend_list.append(level_cm_tend_list)\n",
    "        # corr_mat_list = models_corr_mat_list\n",
    "        print(all_out_list[0][idx1][sample_idx])\n",
    "        \n",
    "        if subplot_layers and has_model:\n",
    "            handel=f'{before_after}_{standard}_indi={individual_sample}/'\n",
    "            folder_dir = f'./saved_plots_sim_test/{handel}'\n",
    "            # if save:\n",
    "            #     if f'{folder_dir}/{imgname}.pdf' in glob.glob(f'{folder_dir}/{imgname}.pdf'):\n",
    "            #         continue\n",
    "            fig, axs = plt.subplots(len(models_corr_mat_list), len(models_corr_mat_list[0]), figsize=(6*len(models_corr_mat_list[0]), 5.6*len(models_corr_mat_list)))  # 6 subplots in a row, adjust size as needed\n",
    "            \n",
    "            for level in range(len(models_corr_mat_list[0])):\n",
    "\n",
    "                for cm_idx, corr_mat_list in enumerate(models_corr_mat_list):\n",
    "                    try:\n",
    "                        corr_mat = corr_mat_list[level]\n",
    "                    except:\n",
    "                        print(corr_mat_list)\n",
    "                        raise ValueError\n",
    "\n",
    "                    # vec_dim = n_embd\n",
    "                    cm_tend = get_PE_tendency(corr_mat, as_list=True)[standard]\n",
    "\n",
    "                    vec_dim = corr_mat.shape[0]//fixed_length\n",
    "\n",
    "                    total_sum = np.abs(corr_mat).sum()\n",
    "                    block_sum = 0\n",
    "                    for i in range(0, len(corr_mat), vec_dim):\n",
    "                        block_sum += np.abs(corr_mat[i:i+vec_dim, i:i+vec_dim]).sum()\n",
    "                    ratio = block_sum/(total_sum-block_sum)\n",
    "\n",
    "                    axloc = axs[level] if len(models_corr_mat_list)==1 else axs[cm_idx, level]\n",
    "\n",
    "                    cm = axloc.imshow(corr_mat, cmap='Reds') #, interpolation='nearest')\n",
    "                    # show the number on each block\n",
    "                    if show_vals:\n",
    "                        for i in range(0, len(corr_mat), vec_dim):\n",
    "                            for j in range(0, len(corr_mat), vec_dim):\n",
    "                                text = axloc.text(j, i, f'{corr_mat[i:i+vec_dim, j:j+vec_dim].sum():.02f}',\n",
    "                                                    ha=\"center\", va=\"center\", color=\"black\", fontsize=6)\n",
    "\n",
    "                    training_status = ''\n",
    "                    if before_after=='training':\n",
    "                        training_status = 'Trained ' if cm_idx==1 else 'Init '\n",
    "                    title_text = f'{training_status}Layer {level} ({cm_tend})' if level > 0 \\\n",
    "                        else f'Embeddings ({cm_tend})'                  \n",
    "                    axloc.set_title(title_text)\n",
    "                    cbar = plt.colorbar(cm, ax=axloc, orientation='vertical', fraction=0.05, )\n",
    "                    # Get the scalar formatter from the colorbar\n",
    "                    from matplotlib.ticker import FormatStrFormatter, MaxNLocator\n",
    "                    formatter = FormatStrFormatter('%.1f')\n",
    "                    axloc.yaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "                    axloc.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "\n",
    "                    axloc.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "\n",
    "                    # Set the desired format (e.g., rounding to 2 decimal places)\n",
    "                    # formatter.set_useOffset(False)  # Disable offset if necessary\n",
    "\n",
    "                    # Apply the formatter to the colorbar\n",
    "                    cbar.formatter = formatter\n",
    "                    cbar.update_ticks()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            fig.subplots_adjust(left=0.03, right=0.975, top=0.95, bottom=0.06, hspace=0.25)\n",
    "            if save:\n",
    "                os.makedirs(f'{folder_dir}', exist_ok=True)\n",
    "\n",
    "                plt.savefig(f'{folder_dir}/{imgname} sample.pdf', format='pdf')\n",
    "                plt.close()\n",
    "                gc.collect()\n",
    "            else:\n",
    "                plt.show()\n",
    "            \n",
    "            '''Plotting the Distribution'''\n",
    "            \n",
    "            fig, axs = plt.subplots(len(models_corr_mat_list), len(models_corr_mat_list[0]), figsize=(6*len(models_corr_mat_list[0]), 5.6*len(models_corr_mat_list)))  # 6 subplots in a row, adjust size as needed\n",
    "            levels_stats = []\n",
    "            for level in range(len(models_corr_mat_list[0])):\n",
    "                for cm_idx, level_cm_tend_list in enumerate(models_cm_tend_list):\n",
    "                    axloc = axs[level] if len(models_corr_mat_list)==1 else axs[cm_idx, level]\n",
    "                    axloc.hist(level_cm_tend_list[level], bins=32)\n",
    "                    training_status = ''\n",
    "                    if before_after=='training':\n",
    "                        training_status = 'Trained ' if cm_idx==1 else 'Init '\n",
    "                    mean_tend = round(np.mean(level_cm_tend_list[level]), 2)\n",
    "                    std_tend = round(np.std(level_cm_tend_list[level]), 2)\n",
    "                    levels_stats.append(mean_tend)\n",
    "                    title_text = f'{training_status}Layer {level} ({mean_tend}, {std_tend})' if level > 0 \\\n",
    "                        else f'Embeddings ({mean_tend}, {std_tend})'                \n",
    "                    axloc.set_title(title_text)                    \n",
    "                    axloc.set_xlim(-1.05, 1.05)\n",
    "\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            fig.subplots_adjust(left=0.03, right=0.975, top=0.92, bottom=0.08, hspace=0.25)\n",
    "            dist_name = f'{folder_dir}/{imgname} dist.pdf'\n",
    "            if save:\n",
    "                os.makedirs(f'{folder_dir}', exist_ok=True)\n",
    "                plt.savefig(dist_name, format='pdf')\n",
    "                plt.close()\n",
    "                gc.collect()\n",
    "            else:\n",
    "                plt.show()\n",
    "            \n",
    "            all_stats[dist_name]=levels_stats\n",
    "\n",
    "        # if idx1 >=10:\n",
    "        #     raise ValueError\n",
    "        if accumulate_all:\n",
    "            level_corr_mat_accum = np.vstack(level_corr_mat_accum)\n",
    "            print(level_corr_mat_accum.shape)\n",
    "            level_corr_mat_accum = level_corr_mat_accum.mean(axis=0)\n",
    "            # Create a heatmap of corr_mat\n",
    "            if not subplot_layers:\n",
    "                plt.figure(figsize=(6, 5), dpi=120)\n",
    "                # plt.imshow(corr_mat, cmap='seismic') #, interpolation='nearest')\n",
    "                plt.imshow(level_corr_mat_accum, cmap='Reds') #, interpolation='nearest')\n",
    "\n",
    "                extra_text = 'Absolute ' if abs else ''\n",
    "                plt.colorbar(label=f'{extra_text}Correlation Coefficient')\n",
    "                plt.show()\n",
    "            corr_mat  =  level_corr_mat_accum\n",
    "            level_corr_mat_accum = []\n",
    "        return all_stats\n",
    "        \n",
    "           \n",
    "from functools import partial\n",
    "from vis_utils import get_corr\n",
    "\n",
    "# get_corr = partial(get_corr, \n",
    "#                         useful_name_list=useful_name_list,\n",
    "#                         all_level_input_act_list=all_level_input_act_list,\n",
    "#                         sim_func=sim_func,\n",
    "#                         X_n=X_n,\n",
    "#                         folder_name='saved_plots_test',\n",
    "#                         fixed_length=fixed_length,\n",
    "#                         equal_distancing_exp=equal_distancing_exp,\n",
    "#                         all_out_list=all_out_list,)\n",
    "\n",
    "# all_drop_down = [e[1] for e in useful_name_list]\n",
    "# from vis_utils import get_corr\n",
    "import ipywidgets as widgets\n",
    "widgets.interact(get_corr, \n",
    "            useful_name_list=[useful_name_list],\n",
    "            all_level_input_act_list=[all_level_input_act_list],\n",
    "            sim_func=[sim_func],\n",
    "            X_n=[X_n],\n",
    "            folder_name=['saved_plots_test'],\n",
    "            fixed_length=[fixed_length],\n",
    "            equal_distancing_exp=[equal_distancing_exp],\n",
    "            all_out_list=[all_out_list],\n",
    "            \n",
    "            sample_idx=(0, len(X_n)-1), \n",
    "            drop_down=useful_name_list, \n",
    "            drop_down2=[None] + useful_name_list, \n",
    "            drop_down3=[None] + useful_name_list, \n",
    "            level=(0,12),\n",
    "            standard=(0,6))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc=0_wherex9_nc_original_sd240_T2408310224_ncTrue/ckpt_10000_acc.pt_nembd=384_nlayers=6_Init_default',\n",
       " 'acc=92_wherex9_nc_original_sd240_T2408310224_ncTrue/ckpt_10000_acc.pt_nembd=384_nlayers=6_trained']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list ['acc=0_wherex9_nc_original_sd240_T2408310224_ncTrue/ckpt_10000_acc.pt_nembd=384_nlayers=6_Init_default', 'acc=92_wherex9_nc_original_sd240_T2408310224_ncTrue/ckpt_10000_acc.pt_nembd=384_nlayers=6_trained']\n",
      "down acc=0_wherex9_nc_original_sd240_T2408310224_ncTrue/ckpt_10000_acc.pt_nembd=384_nlayers=6_Init_default\n",
      "['wherex(299517340,9)=']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f4326ccef9481f8229b07fddf5e1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cabf833da1a4963aefe7cb4a2fa811a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb86244766324c31af692fded025e961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting the Correlation Matrix\n",
      "Plotting the Distribution\n",
      "Plotting the Norm\n",
      "Plotting the Norm Distribution\n"
     ]
    }
   ],
   "source": [
    "content_dict = get_corr(useful_name_list=useful_name_list,\n",
    "            all_level_input_act_list=all_level_input_act_list,\n",
    "            sim_func=sim_func,\n",
    "            X_n=X_n,\n",
    "            folder_name='saved_plots_test',\n",
    "            fixed_length=fixed_length,\n",
    "            equal_distancing_exp=equal_distancing_exp,\n",
    "            all_out_list=all_out_list,\n",
    "\n",
    "            sample_idx=10,\n",
    "            level=0,\n",
    "            standard=6,\n",
    "            drop_down=useful_name_list[0],\n",
    "            drop_down2=None,\n",
    "            drop_down3=None,\n",
    "            plot_type='dot',\n",
    "            all_models=False,\n",
    "            individual_sample=True,\n",
    "            before_after='training',\n",
    "            save=True,\n",
    "            abs=False,\n",
    "            save_all=False,\n",
    "            accumulate_all=False,\n",
    "            subplot_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_dist = {k: v for k, v in content_dict.items() if 'dist.pdf' in k}\n",
    "# norm_dist = {k: v for k, v in content_dict.items() if 'dist_norm.pdf' in k}\n",
    "cm_dist, norm_dist = {}, {}\n",
    "for k in content_dict:\n",
    "    v = content_dict[k]\n",
    "    v = [f'{e[0]}\\u00B1{e[1]}' for e in v]\n",
    "    v = np.array(v)\n",
    "    v = v.reshape(7, -1).T\n",
    "    cur_dict = cm_dist if 'dist.pdf' in k else norm_dist\n",
    "    cur_dict[k+' init'] = v[0]\n",
    "    if len(v) > 1:\n",
    "        cur_dict[k+' trained'] = v[1]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df_cm = pd.DataFrame(cm_dist)\n",
    "df_cm = pd.DataFrame(df_cm.values.T[:, :], columns=['Embeddings', 'Layer 1', 'Layer 2', 'Layer 3', 'Layer 4', 'Layer 5', 'Layer 6'],\n",
    "                     index=df_cm.columns)\n",
    "\n",
    "df_norm = pd.DataFrame(norm_dist)\n",
    "df_norm = pd.DataFrame(df_norm.values.T[:, :], columns=['Embeddings', 'Layer 1', 'Layer 2', 'Layer 3', 'Layer 4', 'Layer 5', 'Layer 6'],\n",
    "                     index=df_norm.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91025335 0.86780759 0.92045869]\n",
      "[0.91016574 0.8677466  0.92040316]\n",
      "[0.90163697 0.85973646 0.91321696]\n",
      "[0.54706847 0.52096506 0.56933248]\n",
      "[0.19130326 0.1811027  0.19976518]\n",
      "[0.17053029 0.15544022 0.16121777]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x700 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.rcParams['axes.labelsize'] =12  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] =12  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] =12  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] =12  # Legend\n",
    "plt.rcParams['axes.titlesize'] =12  # Title\n",
    "\n",
    "\n",
    "vs = np.random.randn(400, 128)\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def my_rand(n, std=0.1):\n",
    "    #return np.random.uniform(-3, 3, n)\n",
    "    # return np.random.poisson(3, n)-3.0\n",
    "    return np.random.normal(0, std, n)\n",
    "\n",
    "\n",
    "def rep(idx_range = [4,5,6] , std=0.1):\n",
    "\n",
    "\n",
    "    alpha = my_rand(idx_range[0], std)\n",
    "    #alpha = alpha / np.sum(alpha)\n",
    "    alpha = softmax(alpha)\n",
    "\n",
    "\n",
    "    beta = my_rand(idx_range[1], std) \n",
    "    # beta[ 0: 4] += 1.8*alpha \n",
    "    #beta = beta / np.sum(beta)\n",
    "    beta = softmax(beta) \n",
    "\n",
    "    gamma = my_rand(idx_range[2], std)\n",
    "    #gamma = gamma / np.sum(gamma)\n",
    "    #gamma[ 0: 4] += 0.8*alpha \n",
    "    # gamma[0: 5] += 1.8*beta\n",
    "    gamma = softmax(gamma) \n",
    "\n",
    "    # a = alpha[0]*vs[0] + alpha[1]*vs[1] + alpha[2]*vs[2] + alpha[3]*vs[3]\n",
    "    # b = beta[0]*vs[0] + beta[1]*vs[1] + beta[2]*vs[2] + beta[3]*vs[3] + beta[4]*vs[4]\n",
    "    # c = gamma[0]*vs[0] + gamma[1]*vs[1] + gamma[2]*vs[2] + gamma[3]*vs[3] + gamma[4]*vs[4] + gamma[5]*vs[5] + gamma[6]*vs[6] + gamma[7]*vs[7] + gamma[8]*vs[8] + gamma[9]*vs[9]\n",
    "    a = alpha @ vs[:len(alpha)]\n",
    "    b = beta @ vs[:len(beta)]\n",
    "    c = gamma @ vs[:len(gamma)]\n",
    "\n",
    "    return np.array([cos_sim(a, b), cos_sim(a, c), cos_sim(b, c)])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 6, figsize=(21, 7))\n",
    "\n",
    "\n",
    "bins = 100\n",
    "\n",
    "for sidx, std in enumerate([0.001, 0.01, 0.1, 1, 10, 100]):\n",
    "\n",
    "    \n",
    "    res = np.array([rep(std=std) for i in range(10000)])  \n",
    "    \n",
    "    stat1 = res[:, 0]-res[:, 1]\n",
    "\n",
    "    axs[0, sidx].hist(stat1, bins=bins)\n",
    "    # axs[0, sidx].set_title(f'mean={stat1.mean():.2f}, std={stat1.std():.2f}')\n",
    "    axs[0, sidx].set_title(f'Init Std={std} ({stat1.mean():.2f}, {stat1.std():.2f})')\n",
    "\n",
    "    sz = stat1.mean() + 3*stat1.std() + 0.05\n",
    "    sz = min(sz, 1)\n",
    "    axs[0, sidx].set_xlim(-sz, sz)\n",
    "\n",
    "\n",
    "    stat2 = res[:, 2]-res[:, 1]\n",
    "    \n",
    "    axs[1, sidx].hist(stat2, bins=bins)\n",
    "    axs[1, sidx].set_title(f'Init Std={std} ({stat2.mean():.2f}, {stat2.std():.2f})')\n",
    "    sz = stat2.mean() + 3*stat2.std()  + 0.05\n",
    "    sz = min(sz, 1)\n",
    "    axs[1, sidx].set_xlim(-sz, sz)\n",
    "    # print(res)\n",
    "\n",
    "    print(np.mean(res, axis=0))\n",
    "fig.savefig(f'./saved_plots_test/rep_3.pdf', format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7e5a9bf5b54f9887d7751ec99bfb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd5ba7d5bd0>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_means = []\n",
    "for st in tqdm(range(4,300,4)):\n",
    "    means = []\n",
    "    means2 = []\n",
    "    for i in range(50):\n",
    "        # for sidx, std in enumerate(np.arange(0.01, 2, 0.01)):\n",
    "        std = 0.001\n",
    "            \n",
    "        res = np.array([rep(idx_range=[st,st+1,st+2], std=std) for i in range(100)])  \n",
    "        \n",
    "        stat1 = res[:, 0]-res[:, 1]\n",
    "\n",
    "        means.append(stat1.mean())\n",
    "\n",
    "        stat2 = res[:, 2]-res[:, 1]\n",
    "        \n",
    "        means2.append(stat2.mean())\n",
    "\n",
    "    # plt.hist(means, bins=100)\n",
    "    # plt.show()\n",
    "    # plt.hist(means2, bins=100)\n",
    "    # plt.show()\n",
    "    means = np.array(means)\n",
    "    means2 = np.array(means2)\n",
    "    means_diff = means2.mean()-means.mean()\n",
    "    all_means.append(means_diff)\n",
    "\n",
    "plt.plot(all_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the difference in projection for the same vector\n",
    "# # x = torch.rand(1, 1, 384).to(device)\n",
    "# font_path = './timr45w.ttf'  # Update this path\n",
    "# from matplotlib import font_manager\n",
    "# # Add the font to Matplotlib's font manager\n",
    "# font_manager.fontManager.addfont(font_path)\n",
    "# prop = font_manager.FontProperties(fname=font_path)\n",
    "# plt.rcParams['font.family'] = prop.get_name()\n",
    "# plt.rcParams['font.family'] = prop.get_name()\n",
    "# plt.rcParams.update({'font.size': 14})\n",
    "# plt.rcParams['axes.labelsize'] = 14  # Axis labels\n",
    "# plt.rcParams['xtick.labelsize'] = 14  # X-axis tick labels\n",
    "# plt.rcParams['ytick.labelsize'] = 14  # Y-axis tick labels\n",
    "# plt.rcParams['legend.fontsize'] = 14  # Legend\n",
    "# plt.rcParams['axes.titlesize'] = 14  # Title\n",
    "\n",
    "# n_embd = 384\n",
    "# x = torch.zeros(1, 1, n_embd).to(device)\n",
    "# sc = torch.rand(1, 1, n_embd).to(device)\n",
    "\n",
    "# idx = np.random.randint(0, n_embd)\n",
    "\n",
    "# x[0, 0, idx] = 1\n",
    "# x += sc\n",
    "# # y = model_list[17].transformer.h[1].attn.c_attn(x)\n",
    "# # input_idx = torch.tensor(encode('12modp(123)=')).to(device)[None,...]\n",
    "# # input_idx = torch.tensor(encode('$123+456=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('$123+456=')).to(device)[None,...]\n",
    "\n",
    "\n",
    "# # eqx = torch.tensor(model.create_equal_distancing_vecotrs(8, n_embd, small_component=0.1)[0]).to(device).to(torch.float32)[None,...] * 0.1\n",
    "\n",
    "# eqx = torch.tensor(model.create_equal_distancing_vecotrs(9, n_embd, small_component=0.0)[0]).to(device).to(torch.float32)[None,...] * 0.1\n",
    "\n",
    "# # Specify the path to your Times New Roman font file\n",
    "# font_path = './timr45w.ttf'  # Update this path\n",
    "# from matplotlib import font_manager\n",
    "# # Add the font to Matplotlib's font manager\n",
    "# font_manager.fontManager.addfont(font_path)\n",
    "# prop = font_manager.FontProperties(fname=font_path)\n",
    "# plt.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "\n",
    "# def plot_att(model_name, layer_idx=0, plot_all=False, save_map=False):\n",
    "\n",
    "#     acc = model_name.split('_')[0]\n",
    "#     rest = '_'.join(model_name.split('_')[1:])\n",
    "#     imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + '_' + acc\n",
    "\n",
    "#     # cur_model = model_list[12]\n",
    "#     # cur_model = model_list[2]\n",
    "#     model_idx = useful_name_list.index(model_name)\n",
    "#     cur_model = model_list[model_idx]\n",
    "    \n",
    "#     layer_range = range(layer_idx, layer_idx+1) if not plot_all else range(len(cur_model.transformer.h))\n",
    "    \n",
    "\n",
    "#     for level in layer_range:\n",
    "#         activation = {}\n",
    "\n",
    "#         def getActivation(name):\n",
    "#             # the hook signature\n",
    "#             def hook(model, input, output):\n",
    "#                 activation[name] = output.detach()\n",
    "\n",
    "#             return hook\n",
    "\n",
    "#         h1 = cur_model.transformer.h[level].attn.c_attn.register_forward_hook(\n",
    "#             getActivation(f\"layer_{level}\")\n",
    "#         )\n",
    "        \n",
    "\n",
    "#         h2 = cur_model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "#             getActivation(f\"layer_{level}_iden\")\n",
    "#         )\n",
    "\n",
    "#         with torch.no_grad():\n",
    "\n",
    "\n",
    "#             out = cur_model(input_idx)\n",
    "#             # decode and print out\n",
    "#             y = decode([out[0].detach().cpu().numpy().argmax()])\n",
    "#             print(y)\n",
    "#             _ = cur_model(eqx, direct_input_modification=True)\n",
    "#             # _ = cur_model(x, direct_input_modification=True)\n",
    "\n",
    "\n",
    "#         h1.remove()\n",
    "#         h2.remove()\n",
    "#         y = activation[f\"layer_{level}\"]\n",
    "\n",
    "#         q, k, v  = y.split(n_embd, dim=2)\n",
    "#         plt.figure(figsize=(12, 3))\n",
    "#         plt.plot(q.detach().cpu().numpy().flatten())\n",
    "#         plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "#         plt.show()\n",
    "\n",
    "#         fig, ax = plt.subplots(1, cur_model.config.n_head, figsize=(14, 2))\n",
    "#         q_reshape = q.reshape(1, -1, cur_model.config.n_head, n_embd//cur_model.config.n_head)\n",
    "#         k_reshape = k.reshape(1, -1, cur_model.config.n_head, n_embd//cur_model.config.n_head)\n",
    "#         for i in range(cur_model.config.n_head):\n",
    "#             calc = (torch.nn.Softmax(dim=-1)(q_reshape[0, :, i, :]@k_reshape[0, :, i, :].transpose(0,1)))\n",
    "#             calc = torch.ones_like(calc) * calc.mean()\n",
    "#             result = calc.detach().cpu().numpy()\n",
    "\n",
    "#             mat = ax[i].imshow(result, cmap='Reds', interpolation='nearest')\n",
    "#             ax[i].set_title(f'head {i}')\n",
    "#             plt.colorbar(mat, ax=ax[i], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "#         plt.show()\n",
    "            \n",
    "\n",
    "#         fig, ax = plt.subplots(1, 2, figsize=(5, 2))\n",
    "#         plt.imshow((torch.nn.Softmax(dim=-1)(q@k.transpose(1, 2))).detach().cpu().numpy()[0], cmap='Reds', interpolation='nearest')\n",
    "        \n",
    "#         calc = torch.nn.Softmax(dim=-1)(q@k.transpose(1, 2))\n",
    "#         calc = torch.ones_like(calc) * calc.mean()\n",
    "#         result = calc.detach().cpu().numpy()\n",
    "#         mat = ax[0].imshow(result[0], cmap='Reds', interpolation='nearest')\n",
    "\n",
    "#         plt.colorbar(mat, ax=ax[0], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "\n",
    "\n",
    "#         q_ = q.detach().cpu().numpy()[0]\n",
    "#         k_ = k.detach().cpu().numpy()[0]\n",
    "#         mat = cosine_similarity(q_, k_)\n",
    "\n",
    "#         mat = ax[1].imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "#         plt.colorbar(mat, ax=ax[1], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "#         print(x.sum(), q.abs().sum(), k.abs().sum(), v.abs().sum())\n",
    "\n",
    "#         y = activation[f\"layer_{level}_iden\"].detach().cpu().numpy()\n",
    "#         plt.figure(figsize=(12, 3))\n",
    "#         plt.plot(y.flatten())\n",
    "#         plt.show()\n",
    "\n",
    "#         plt.figure(figsize=(6, 5))\n",
    "#         mat = cosine_similarity(y[0], y[0])\n",
    "#         plt.imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "#         plt.colorbar()\n",
    "                \n",
    "#         # maskout the upper triangle\n",
    "#         calc = torch.nn.Softmax(dim=-1)(q@k.transpose(1, 2))\n",
    "#         calc = torch.ones_like(calc) * calc.mean()\n",
    "#         mask = np.triu(np.ones_like(calc[0].detach().cpu().numpy(), dtype=bool))[None, ...]\n",
    "#         mask = torch.tensor(mask).to(device)\n",
    "\n",
    "#         digit = calc[0, 0, 0].detach().cpu().clone()\n",
    "#         calc[mask] = -np.inf\n",
    "#         calc[0].diagonal().fill_(digit)\n",
    "#         calc = torch.nn.Softmax(dim=-1)(calc)\n",
    "\n",
    "\n",
    "#         fig = plt.figure(figsize=(5.7, 10))\n",
    "#         gs = fig.add_gridspec(2, 1, height_ratios=[1, 1])  # Adjust hspace as needed\n",
    "\n",
    "#         ax1 = fig.add_subplot(gs[0, 0])\n",
    "#         att_map = calc[0].detach().cpu().numpy()\n",
    "#         cm1 = ax1.imshow(att_map, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "#         for i in range(len(att_map)):\n",
    "#             for j in range(len(att_map)):\n",
    "#                 content = f'{att_map[i, j]:.02f}' if att_map[i, j] != 0 else '0'\n",
    "#                 ax1.text(j, i, content, ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "#         cbar1 = plt.colorbar(cm1, ax=ax1, orientation='vertical', fraction=0.05, )\n",
    "#         cbar1.ax.tick_params(labelsize=12)\n",
    "#         ax1.set_title('Causal Attention Matrix', fontdict={'fontsize':14})\n",
    "#         ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "#         ax2 = fig.add_subplot(gs[1, 0])\n",
    "#         y = calc.cpu() @ v.cpu()\n",
    "#         y = y.numpy()\n",
    "#         mat = cosine_similarity(y[0], y[0])\n",
    "#         cm2 = ax2.imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "#         for i in range(len(mat)):\n",
    "#             for j in range(len(mat)):\n",
    "#                 ax2.text(j, i, f'{mat[i, j]:.02f}', ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "#         cbar2 = plt.colorbar(cm2, ax=ax2, orientation='vertical', fraction=0.05)\n",
    "#         cbar2.ax.tick_params(labelsize=12)\n",
    "#         ax2.set_title('Self-Cosine-Similarity Matrix', fontdict={'fontsize':14})\n",
    "#         ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "#         ax1.annotate('(a)', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=14, fontproperties=prop)\n",
    "#         ax2.annotate('(b)', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=14, fontproperties=prop)\n",
    "\n",
    "#         fig.subplots_adjust(left=0.0, right=0.92, top=0.97, bottom=0.07, hspace=0.2)\n",
    "        \n",
    "#         if save_map:\n",
    "#             os.makedirs(f'./saved_att_plots/', exist_ok=True)\n",
    "#             # fig.savefig(f'./saved_att_plots/{imgname}_layer={layer_idx+1}.svg')\n",
    "#             fig.savefig(f'./saved_att_plots/{imgname}_layer={layer_idx+1}.pdf', format='pdf')\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "#         calc = torch.nn.Softmax(dim=-1)(torch.einsum('bhid,bhjd->bhij', q_reshape, k_reshape))\n",
    "#         calc = torch.ones_like(calc) * calc.mean()\n",
    "#         mask = np.triu(np.ones_like(calc[0].detach().cpu().numpy(), dtype=bool))[None, ...]\n",
    "#         mask = torch.tensor(mask).to(device)\n",
    "\n",
    "#         digit = calc[0, 0, 0,0 ].detach().cpu().clone()\n",
    "#         print(digit)\n",
    "#         calc[mask] = -np.inf \n",
    "#         # for i in range(calc.shape)\n",
    "#         #     calc[0, 0].diagonal().fill_(digit)\n",
    "#         T = calc.shape[-1]\n",
    "#         calc[:,:,np.arange(T),np.arange(T)] = digit\n",
    "#         calc = torch.nn.Softmax(dim=-1)(calc)\n",
    "\n",
    "#         plt.figure(figsize=(6, 5))\n",
    "#         plt.imshow(calc[0,0].detach().cpu().numpy(), cmap='Reds', interpolation='nearest')\n",
    "#         plt.colorbar()\n",
    "#         plt.show()\n",
    "\n",
    "#         # raise ValueError\n",
    "# widgets.interact(plot_att, model_name=useful_name_list, layer_idx=(0, 5), plot_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most responsible neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the difference in projection for the same vector\n",
    "# # x = torch.rand(1, 1, 384).to(device)\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import font_manager\n",
    "# from ast import literal_eval\n",
    "# import ipywidgets as widgets\n",
    "\n",
    "# font_path = './timr45w.ttf'  # Update this path\n",
    "# from matplotlib import font_manager\n",
    "# # Add the font to Matplotlib's font manager\n",
    "# font_manager.fontManager.addfont(font_path)\n",
    "# prop = font_manager.FontProperties(fname=font_path)\n",
    "# plt.rcParams['font.family'] = prop.get_name()\n",
    "# plt.rcParams['font.family'] = prop.get_name()\n",
    "# plt.rcParams.update({'font.size': 12})\n",
    "# plt.rcParams['axes.labelsize'] = 12  # Axis labels\n",
    "# plt.rcParams['xtick.labelsize'] = 12  # X-axis tick labels\n",
    "# plt.rcParams['ytick.labelsize'] = 12  # Y-axis tick labels\n",
    "# plt.rcParams['legend.fontsize'] = 12  # Legend\n",
    "# plt.rcParams['axes.titlesize'] = 12  # Title\n",
    "\n",
    "# n_embd = 384\n",
    "# sz = 12\n",
    "# x = torch.zeros(1, sz, n_embd).to(device)\n",
    "# sc = torch.rand(1, sz, n_embd).to(device)\n",
    "\n",
    "# # idx = np.random.randint(0, n_embd)\n",
    "# idx = np.random.randint(0, n_embd, size=(sz))\n",
    "\n",
    "# x[0, :, idx] = 1\n",
    "# x += sc\n",
    "# # y = model_list[17].transformer.h[1].attn.c_attn(x)\n",
    "# # input_idx = torch.tensor(encode('12modp(123)=')).to(device)[None,...]\n",
    "# # input_idx = torch.tensor(encode('$123+45=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('$333+444=777$\\n$123+54=')).to(device)[None,...]\n",
    "# input_idx2 = torch.tensor(encode('$123+54=')).to(device)[None,...]\n",
    "# input_idx3 = torch.tensor(encode('$992+299=')).to(device)[None,...]\n",
    "\n",
    "# eqx = torch.tensor(model.create_equal_distancing_vecotrs(sz, n_embd, small_component=0.01)[0]).to(device).to(torch.float32)[None,...] * 0.1\n",
    "\n",
    "# inputs_dict = {\n",
    "#     'sample_1': input_idx,\n",
    "#     'sample_2': input_idx2,\n",
    "#     'sample_3': input_idx3,\n",
    "#     'eqx': eqx,\n",
    "#     'randx': x,\n",
    "# } \n",
    "\n",
    "# # Specify the path to your Times New Roman font file\n",
    "# font_path = './timr45w.ttf'  # Update this path\n",
    "# # Add the font to Matplotlib's font manager\n",
    "# font_manager.fontManager.addfont(font_path)\n",
    "# prop = font_manager.FontProperties(fname=font_path)\n",
    "# plt.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "# from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# def levenshteinDistance(s1, s2):\n",
    "#     if len(s1) > len(s2):\n",
    "#         s1, s2 = s2, s1\n",
    "\n",
    "#     distances = range(len(s1) + 1)\n",
    "#     for i2, c2 in enumerate(s2):\n",
    "#         distances_ = [i2+1]\n",
    "#         for i1, c1 in enumerate(s1):\n",
    "#             if c1 == c2:\n",
    "#                 distances_.append(distances[i1])\n",
    "#             else:\n",
    "#                 distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "#         distances = distances_\n",
    "#     return distances[-1]\n",
    "\n",
    "# def get_PE_tendency(mat):\n",
    "#     r = 0\n",
    "#     r2 = 0\n",
    "#     r3 = 0\n",
    "#     r4 = 0\n",
    "#     r5 = 0\n",
    "#     r5_counts = 0\n",
    "#     r6 = 0\n",
    "#     r6_count = 0\n",
    "#     mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "#     for lidx, layer in enumerate(mat[1:]):\n",
    "#         r += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "#         r3 += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "#         r3 -= (np.diff(layer[:lidx+1], axis=0) <= 0).sum()\n",
    "\n",
    "#         c2p = sum([((layer[j+1:lidx+1]-layer[j]) > 0).sum() for j in range(lidx+1)])\n",
    "#         c2m = sum([((layer[j+1:lidx+1]-layer[j]) <= 0).sum() for j in range(lidx+1)])\n",
    "#         r2 += c2p\n",
    "#         r4 += c2p - c2m\n",
    "\n",
    "#         valid_layer = layer[:lidx+1]\n",
    "#         if len(valid_layer)<3: \n",
    "#             continue\n",
    "\n",
    "#         # cur_order = valid_layer\n",
    "#         # original_order = valid_layer[np.argsort(valid_layer)]\n",
    "#         # corr = spearmanr(original_order, cur_order).correlation\n",
    "#         # if np.isnan(corr):\n",
    "#             # continue\n",
    "#         # r5 += spearmanr(original_order, cur_order).correlation \n",
    "\n",
    "\n",
    "#         right_order = np.argsort(valid_layer-np.arange(len(valid_layer))*1e-5)\n",
    "#         right_right_order = np.argsort(right_order)\n",
    "#         original_order = np.arange(len(valid_layer))\n",
    "#         edit_distance = levenshteinDistance(right_order, original_order)\n",
    "#         r5 += edit_distance\n",
    "#         r5_counts += len(valid_layer)\n",
    "#         r_value = pearsonr(original_order, right_right_order)[0]\n",
    "#         if np.isnan(r_value): continue\n",
    "#         r6 += pearsonr(original_order, right_right_order)[0]\n",
    "#         r6_count += 1\n",
    "#         # r5 += pearsonr(original_order, cur_order)[0]\n",
    "\n",
    "#     max_r = np.arange(mat.shape[0]-1).sum()\n",
    "#     t1 = round(r/max_r, 2)\n",
    "\n",
    "#     max_r2 = sum([ np.arange(n+1).sum() for n in np.arange(mat.shape[0]-1)])\n",
    "#     t2 = round(r2/max_r2, 2)\n",
    "\n",
    "#     t3 = round(r3/max_r, 2)\n",
    "\n",
    "#     t4 = round(r4/max_r2, 2)\n",
    "    \n",
    "#     t5 =  round((r5_counts-r5) / r5_counts, 2)\n",
    "\n",
    "#     t6 = round(r6/r6_count, 2)\n",
    "#     return f'({t1},{t2},{t3},{t4},{t5},{t6})'\n",
    "\n",
    "\n",
    "# def generate_tendency_map(mat):\n",
    "#     mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "#     empty_mat = np.zeros_like(mat)\n",
    "#     for lidx, layer in enumerate(mat):\n",
    "#         if lidx == 0:\n",
    "#             continue\n",
    "#         counter = 0\n",
    "#         for j in range(1, lidx+1):\n",
    "#             if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "#                 counter = 0\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "#             empty_mat[lidx, j] = counter\n",
    "    \n",
    "#     return empty_mat\n",
    "\n",
    "# def generate_tendency_map(mat):\n",
    "#     mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "#     empty_mat = np.zeros_like(mat)\n",
    "#     for lidx, layer in enumerate(mat):\n",
    "      \n",
    "#         counter = 0\n",
    "#         for j in range(0, lidx+1):\n",
    "#             if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "#                 counter = 0\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "#             empty_mat[lidx, j] = counter\n",
    "#             # empty_mat[lidx, j] = np.log(mat[lidx, j])\n",
    "    \n",
    "#     return empty_mat\n",
    "\n",
    "    \n",
    "\n",
    "# def plot_att(model_name, \n",
    "#              layer_idx=0, \n",
    "#              plot_all=0, \n",
    "#              save_map=False, \n",
    "#              find_best=True, \n",
    "#              search_all_heads=False,\n",
    "#              metric_id = 4,\n",
    "#              bestk = 4,\n",
    "#              clean_plot = True,\n",
    "#              input_name = sorted(list(inputs_dict.keys())), \n",
    "#              amp_head=2,):\n",
    "\n",
    "#     acc = model_name.split('_')[0]\n",
    "#     rest = '_'.join(model_name.split('_')[1:])\n",
    "#     imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + '_' + acc\n",
    "\n",
    "#     # cur_model = model_list[12]\n",
    "#     # cur_model = model_list[2]\n",
    "#     model_idx = useful_name_list.index(model_name)\n",
    "#     cur_model = model_list[model_idx]\n",
    "    \n",
    "#     layer_range = range(layer_idx, layer_idx+1) if plot_all==0 else range(plot_all)\n",
    "    \n",
    "#     if len(layer_range) > len(cur_model.transformer.h):\n",
    "#         layer_range = range(len(cur_model.transformer.h))\n",
    "\n",
    "#     for level in layer_range:\n",
    "#         activation = {}\n",
    "\n",
    "#         def getActivation(name):\n",
    "#             # the hook signature\n",
    "#             def hook(model, input, output):\n",
    "#                 activation[name] = output.detach()\n",
    "\n",
    "#             return hook\n",
    "\n",
    "#         h1 = cur_model.transformer.h[level].attn.c_attn.register_forward_hook(\n",
    "#             getActivation(f\"layer_{level}\")\n",
    "#         )\n",
    "        \n",
    "#         h1q = cur_model.transformer.h[level].attn.iq.register_forward_hook(\n",
    "#             getActivation(f\"q{level}\")\n",
    "#         ) \n",
    "#         h1k = cur_model.transformer.h[level].attn.ik.register_forward_hook(\n",
    "#             getActivation(f\"k{level}\")\n",
    "#         ) \n",
    "#         h1v = cur_model.transformer.h[level].attn.iv.register_forward_hook(\n",
    "#             getActivation(f\"v{level}\")\n",
    "#         ) \n",
    "\n",
    "#         h2 = cur_model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "#             getActivation(f\"layer_{level}_iden\")\n",
    "#         )\n",
    "\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             input_values = inputs_dict[input_name]\n",
    "#             if 'sample' in input_name:\n",
    "#                 out = cur_model(input_values)\n",
    "#                 y = decode([out[0].detach().cpu().numpy().argmax()])\n",
    "#                 print(y)\n",
    "#             else: # eqx or randx\n",
    "#                 _ = cur_model(input_values, direct_input_modification=True)\n",
    "      \n",
    "\n",
    "#         h1.remove()\n",
    "#         h1q.remove()\n",
    "#         h1k.remove()\n",
    "#         h1v.remove()\n",
    "#         h2.remove()\n",
    "\n",
    "\n",
    "#         '''Plot the attention maps'''\n",
    "#         xw = activation[f\"layer_{level}\"]\n",
    "#         q, k, v  = xw.split(n_embd, dim=2)\n",
    "#         q_reshape = activation[f\"q{level}\"]\n",
    "#         k_reshape = activation[f\"k{level}\"]\n",
    "#         v_reshape = activation[f\"v{level}\"]\n",
    "\n",
    "#         if not clean_plot:\n",
    "#             # plt.figure(figsize=(12, 3))\n",
    "#             k_numpy = k_reshape.detach().cpu().numpy()\n",
    "#             bs, T, n_head, head_dim = k_numpy.shape\n",
    "#             k_numpy = k_numpy.reshape(bs, T, -1)\n",
    "#             print(k_numpy.shape)\n",
    "\n",
    "#             fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "#             axes[0].plot(k_numpy[:, 0].flatten())\n",
    "#             axes[0].set_title('key 0')\n",
    "#             axes[1].hist(k_numpy[:, 0].flatten(), bins=100)\n",
    "#             axes[1].set_title('key 0 distribution')\n",
    "#             # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "#             # plt.show()\n",
    "\n",
    "#             # fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "#             k_last = k_numpy[:, -1].flatten()\n",
    "#             axes[2].plot(k_last)\n",
    "#             axes[2].set_title(f'key {T}')\n",
    "#             axes[3].hist(k_last, bins=100)\n",
    "#             axes[3].set_title(f'key {T} distribution')\n",
    "#             # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "#             plt.show()\n",
    "\n",
    "#             v_numpy = v_reshape.detach().cpu().numpy()\n",
    "#             bs, T, n_head, head_dim = v_numpy.shape\n",
    "#             v_numpy = v_numpy.reshape(bs, T, -1)\n",
    "\n",
    "#             fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "#             axes[0].plot(v_numpy[:, 0].flatten())\n",
    "#             axes[0].set_title('value 0')\n",
    "#             axes[1].hist(v_numpy[:, 0].flatten(), bins=100)\n",
    "#             axes[1].set_title('value 0 distribution')\n",
    "#             # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "#             # plt.show()\n",
    "\n",
    "#             # fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "#             v_last = v_numpy[:, -1].flatten()\n",
    "#             axes[2].plot(v_last)\n",
    "#             axes[2].set_title(f'value {T}')\n",
    "#             axes[3].hist(v_last, bins=100)\n",
    "#             axes[3].set_title(f'value {T} distribution')\n",
    "#             # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "#             plt.show()\n",
    "\n",
    "#         from itertools import combinations\n",
    "#         head_dim = n_embd//cur_model.config.n_head\n",
    "#         idx_comb = [np.arange(head_dim)] + [[i] for i in range(head_dim)]\n",
    "\n",
    "#         n_arrays = cur_model.config.n_head\n",
    "#         all_records = [[] for _ in range(n_arrays)] \n",
    "#         all_maps = [[] for _ in range(n_arrays)] \n",
    "#         all_idxsets = [[] for _ in range(n_arrays)] \n",
    "#         original_score = []\n",
    "#         head2 = []\n",
    "\n",
    "#         run_search = True\n",
    "#         for cur_idxset in idx_comb:\n",
    "#             if not run_search:\n",
    "#                 break\n",
    "#             if not find_best:\n",
    "#                 run_search = False\n",
    "#                 cur_idxset = range(head_dim)\n",
    "\n",
    "#             if not run_search:\n",
    "#                 fig, ax = plt.subplots(4, cur_model.config.n_head, figsize=(18, 10))\n",
    "\n",
    "#             for hidx in range(cur_model.config.n_head):\n",
    "                \n",
    "\n",
    "#                 attmap = (torch.nn.Softmax(dim=-1)(q_reshape[0, hidx, :, cur_idxset]@k_reshape[0, hidx, :, cur_idxset].transpose(0,1)))\n",
    "#                 mask = np.triu(np.ones_like(attmap.detach().cpu().numpy(), dtype=bool))\n",
    "#                 mask = torch.tensor(mask).to(device)\n",
    "\n",
    "#                 digit = attmap.flatten()[0].detach().cpu().clone()\n",
    "#                 attmap[mask] = -np.inf \n",
    "#                 T = attmap.shape[-1]\n",
    "#                 attmap[np.arange(T), np.arange(T)] = digit\n",
    "\n",
    "#                 attmap_np = attmap.detach().cpu().numpy()\n",
    "#                 attmap_tend = get_PE_tendency(attmap_np)\n",
    "\n",
    " \n",
    "                \n",
    "#                 s_attmap = torch.nn.Softmax(dim=-1)(attmap)\n",
    "#                 # calc = torch.ones_like(calc) * calc.mean()\n",
    "#                 s_attmap_np = s_attmap.detach().cpu().numpy()\n",
    "#                 tendency_map = generate_tendency_map(s_attmap_np)\n",
    "#                 mat_tend = get_PE_tendency(s_attmap_np)\n",
    "\n",
    "#                 y_out = s_attmap.cpu() @ v_reshape[0, hidx, :, cur_idxset].cpu()\n",
    "#                 sim_map = cosine_similarity(y_out, y_out)\n",
    "\n",
    "\n",
    "#                 if hidx == amp_head or (search_all_heads and find_best):\n",
    "#                     if find_best:\n",
    "#                         adj_score = literal_eval(attmap_tend)[metric_id]\n",
    "                                                \n",
    "#                         if len(cur_idxset) == head_dim:\n",
    "#                             original_score.append(adj_score)\n",
    "#                             head2.append(attmap_np)\n",
    "#                         else:\n",
    "#                             all_records[hidx].append(adj_score)\n",
    "#                             all_maps[hidx].append(attmap_np)\n",
    "#                             all_idxsets[hidx].append(cur_idxset)\n",
    "#                     else:\n",
    "#                         head2.append(attmap_np) \n",
    "\n",
    "\n",
    "#                 if not run_search: \n",
    "#                     mat = ax[0, hidx].imshow(attmap_np, cmap='Reds', interpolation='nearest')\n",
    "#                     ax[0, hidx].set_title(f'head {hidx}={attmap_tend}')\n",
    "#                     plt.colorbar(mat, ax=ax[0, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    \n",
    "#                     mat = ax[1, hidx].imshow(s_attmap_np, cmap='Reds', interpolation='nearest')\n",
    "#                     ax[1, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "#                     plt.colorbar(mat, ax=ax[1, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "#                     mat = ax[2, hidx].imshow(tendency_map, cmap='Reds', interpolation='nearest')\n",
    "#                     ax[2, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "#                     plt.colorbar(mat, ax=ax[2, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "#                     mat = ax[3, hidx].imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "#                     sim_tend = get_PE_tendency(sim_map)\n",
    "\n",
    "#                     ax[3, hidx].set_title(f'head {hidx}={sim_tend}')\n",
    "#                     plt.colorbar(mat, ax=ax[3, hidx], orientation='vertical', fraction=0.06,)\n",
    "\n",
    "#             if not run_search: \n",
    "#                 plt.subplots_adjust(hspace=.4)\n",
    "#                 fig.suptitle(f'{input_name}_{imgname}_layer={level+1}', fontsize=16, y=0.96)\n",
    "#                 if save_map:\n",
    "#                     os.makedirs(f'./saved_heads_plots/', exist_ok=True)\n",
    "#                     fig.savefig(f'./saved_heads_plots/{input_name}_{imgname}_layer={level+1}.svg')\n",
    "#                 plt.show()\n",
    "\n",
    "#         if find_best:\n",
    "#             fig, ax = plt.subplots(2, len(head2), figsize=(3.5*len(head2), 6))\n",
    "#             for amidx, attnmap_np in enumerate(head2):\n",
    "#                 ax_loc_0 = ax[0, amidx] if len(head2)>1 else ax[0]\n",
    "#                 ax_loc_1 = ax[1, amidx] if len(head2)>1 else ax[1]\n",
    "\n",
    "#                 mat = ax_loc_0.imshow(attnmap_np, cmap='Reds', interpolation='nearest')\n",
    "#                 head2_PE_scores = get_PE_tendency(attnmap_np)\n",
    "#                 ax_loc_0.set_title(f'head {amp_head}={head2_PE_scores}')\n",
    "#                 plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "#                 tendmap_head2 = generate_tendency_map(attnmap_np)\n",
    "#                 mat = ax_loc_1.imshow(tendmap_head2, cmap='Reds', interpolation='nearest')\n",
    "#                 original_score[amidx]\n",
    "#                 ax_loc_1.set_title(f'head {amp_head}={original_score[amidx]}')\n",
    "#                 plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "#             plt.show()\n",
    "            \n",
    "#             fig, ax = plt.subplots(2, len(head2), figsize=(3.5*len(head2), 6))\n",
    "\n",
    "#             for hridx, head_rec in enumerate(all_records):\n",
    "\n",
    "#                 if len(head_rec) == 0: \n",
    "#                     continue\n",
    "\n",
    "#                 topk_records_idx = np.argsort(head_rec)[-bestk:]\n",
    "\n",
    "#                 best_maps = np.array([all_maps[hridx][i] for i in topk_records_idx])\n",
    "#                 best_map = best_maps.mean(axis=0)\n",
    "#                 best_score = np.mean([head_rec[i] for i in topk_records_idx])\n",
    "#                 best_idxcomb = [all_idxsets[hridx][i] for i in topk_records_idx]\n",
    "\n",
    "\n",
    "#                 ax_loc_0 = ax[0, hridx] if len(head2)>1 else ax[0]\n",
    "#                 ax_loc_1 = ax[1, hridx] if len(head2)>1 else ax[1]\n",
    "\n",
    "#                 mat = ax_loc_0.imshow(best_map, cmap='Reds', interpolation='nearest')\n",
    "#                 # best_PE_scores = get_PE_tendency(best_map)\n",
    "#                 ax_loc_0.set_title(f'best avg={best_score:.02f}')\n",
    "#                 plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "#                 tendmap_best = generate_tendency_map(best_map)\n",
    "#                 mat = ax_loc_1.imshow(tendmap_best, cmap='Reds', interpolation='nearest')\n",
    "#                 ax_loc_1.set_title(f'best ={best_idxcomb}')\n",
    "#                 plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "            \n",
    "#             plot_name = f'bestk={bestk}_{input_name}_{imgname}_layer={level+1}'\n",
    "#             fig.suptitle(plot_name, fontsize=16, y=0.96)\n",
    "#             if save_map:\n",
    "#                 os.makedirs(f'./saved_heads_plots/', exist_ok=True)\n",
    "#                 fig.savefig(f'./saved_heads_plots/{plot_name}.svg')\n",
    "#             plt.show()\n",
    "                \n",
    "\n",
    "#         # ================================================================\n",
    "        \n",
    "#         # if not clean_plot:\n",
    "\n",
    "#             # fig, ax = plt.subplots(1, 1, figsize=(20, 12.5))\n",
    "\n",
    "\n",
    "#             # # map_to_show = result[0]\n",
    "#             # map_to_show = head2[0]\n",
    "#             # mat = ax.imshow(map_to_show, cmap='Reds', interpolation='nearest')\n",
    "#             # value_amp = 20\n",
    "#             # for hidx in range(len(map_to_show)):\n",
    "#             #     for j in range(len(map_to_show)):\n",
    "#             #         ax.text(j, hidx, f'{map_to_show[hidx, j]*value_amp:.02f}', ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "#             # plt.colorbar(mat, ax=ax, orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "#             # ax.set_title(f'showing head {amp_head}, values Ã— {value_amp}')\n",
    "\n",
    "\n",
    "#             # # q_ = q.detach().cpu().numpy()[0]\n",
    "#             # # k_ = k.detach().cpu().numpy()[0]\n",
    "#             # # sim_map = cosine_similarity(q_, k_)\n",
    "\n",
    "#             # # mat = ax[1].imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "#             # # plt.colorbar(mat, ax=ax[1], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "#             # # plt.show()\n",
    "\n",
    "\n",
    "#             # # print(x.sum(), q.abs().sum(), k.abs().sum(), v.abs().sum())\n",
    "\n",
    "#             # y = activation[f\"layer_{level}_iden\"].detach().cpu().numpy()\n",
    "#             # plt.figure(figsize=(12, 3))\n",
    "#             # plt.plot(y.flatten())\n",
    "#             # plt.show()\n",
    "\n",
    "#             # plt.figure(figsize=(6, 5))\n",
    "#             # mat = cosine_similarity(y[0], y[0])\n",
    "#             # mat_tend = get_PE_tendency(mat)\n",
    "#             # plt.imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "#             # plt.title(f'tendency={mat_tend}')\n",
    "#             # plt.colorbar()\n",
    "            \n",
    "\n",
    "# widgets.interact(plot_att, \n",
    "#                  model_name=useful_name_list, \n",
    "#                  layer_idx=(0, 11), \n",
    "#                  plot_all=(0, 6), \n",
    "#                  amp_head=(0, 5),\n",
    "#                  metric_id = (0, 5),\n",
    "#                  bestk=(1, 16))\n",
    "# # widgets.interact(plot_att, model_name=useful_name_list, layer_idx=(0, 5), plot_all=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PyPDF2 import PdfMerger \n",
    "# import cairosvg\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# def convert_svg_to_pdf(svg_files, output_pdf):\n",
    "#     temp_pdfs = []\n",
    "\n",
    "#     # Convert each SVG to a temporary PDF\n",
    "#     for svg_file in svg_files:\n",
    "#         pdf_file = f\"{svg_file}.pdf\"\n",
    "#         cairosvg.svg2pdf(url=svg_file, write_to=pdf_file)\n",
    "#         temp_pdfs.append(pdf_file)\n",
    "\n",
    "#     # Merge all temporary PDFs into a single PDF\n",
    "#     merger = PdfMerger ()\n",
    "#     for pdf in temp_pdfs:\n",
    "#         merger.append(pdf)\n",
    "\n",
    "#     # Write out the final merged PDF\n",
    "#     merger.write(output_pdf)\n",
    "#     merger.close()\n",
    "\n",
    "#     # Clean up temporary PDF files\n",
    "#     for pdf in temp_pdfs:\n",
    "#         os.remove(pdf)\n",
    "\n",
    "\n",
    "\n",
    "# # Usage example\n",
    "# root_dir = './saved_heads_plots'\n",
    "# svg_files = sorted(glob.glob(root_dir + '/*.svg'))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# f_dict = {}\n",
    "# for fpath in svg_files:\n",
    "#     fname = fpath.split('/')[-1]\n",
    "#     model_name = '_'.join(fname.split('_')[:-1])\n",
    "#     if model_name not in f_dict:\n",
    "#         f_dict[model_name] = []\n",
    "#     f_dict[model_name].append(fpath)\n",
    "# for model_name in tqdm(f_dict):\n",
    "#     sorted_files = sorted(f_dict[model_name])\n",
    "#     output_pdf = f'{root_dir}_pdf/{model_name}.pdf'\n",
    "#     convert_svg_to_pdf(sorted_files, output_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83141543fd0e412fbe643f93ebc5fc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_name', options=('acc=0_wherex9_nc_original_sd240_T2408310224â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.prob_model(model_name, model2_name=None, save_all_models=False, save=False, layer_idx=0, plot_all=False, rand_perm=True, before_after=[False, 'training', 'attention'], total_samples=512, max_epochs=20, fixed_length=4, input_type=['x', 'x1*x2', 'x1-x2', '[x1, x2]'], data_type=['sample', 'same', 'randx', 'eqx'], scaler=0.1, linear_mode=1, loss_type=['MSE', 'Cross Entropy'], hook_location='after_attn', plot_type=['violin', 'both', 'tsne'])>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.stats import pearsonr, linregress\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "import re\n",
    "from thop import profile, clever_format\n",
    "\n",
    "\n",
    "# Specify the path to your Times New Roman font file\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rcParams['axes.labelsize'] = 27  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 27  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 27  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 27  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 27  # Title\n",
    "# Define the probe model\n",
    "class PositionalProbe(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 linear_mode=0,\n",
    "                 n_outs=1):\n",
    "        super(PositionalProbe, self).__init__()\n",
    "        # Only a single output unit as it predicts one position at a time\n",
    "        if linear_mode==0:\n",
    "            self.linear = nn.Linear(embedding_dim, n_outs)\n",
    "        elif linear_mode in [1, 2]:\n",
    "            self.linear = nn.Linear(embedding_dim, 64)\n",
    "            self.linear2 = nn.Linear(64, n_outs)\n",
    "        elif linear_mode in [3, 4]:\n",
    "            self.linear = nn.Linear(embedding_dim, 256)\n",
    "            self.linear2 = nn.Linear(256, 64)\n",
    "            self.linear3 = nn.Linear(64, n_outs)\n",
    "        elif linear_mode==5:\n",
    "            self.linear = nn.Linear(embedding_dim, 512)\n",
    "            self.linear2 = nn.Linear(512, 384)\n",
    "            self.linear3 = nn.Linear(384, 128)\n",
    "            self.linear4 = nn.Linear(128, n_outs)\n",
    "    \n",
    "                \n",
    "        self.n_outs = n_outs\n",
    "        self.linear_mode = linear_mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the output to match the target dimension which is [batch_size * seq_len]\n",
    "        if self.linear_mode==0:\n",
    "            x = self.linear(x)\n",
    "        elif self.linear_mode==1:\n",
    "            x = self.linear(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.linear2(x)\n",
    "        elif self.linear_mode==2:\n",
    "            x = self.linear(x)\n",
    "            x = torch.exp(x)  \n",
    "            x = self.linear2(x)\n",
    "        elif self.linear_mode==3:\n",
    "            x = self.linear(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.linear2(x)\n",
    "            x = torch.exp(x) # make it so that the coef is meaningful\n",
    "            x = self.linear3(x)\n",
    "\n",
    "        elif self.linear_mode==4:\n",
    "            x = self.linear(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.linear2(x)\n",
    "            # x = torch.log(x) # make it so that the coef is meaningful\n",
    "            # log_x = torch.where(x > 0, torch.log(x), 0)\n",
    "            x = self.linear3(x)\n",
    "\n",
    "        elif self.linear_mode==5:\n",
    "            x = self.linear(x)\n",
    "            x = torch.selu(x)\n",
    "            x = self.linear2(x)\n",
    "            x = torch.exp(x) # make it so that the coef is meaningful\n",
    "            x = self.linear3(x)\n",
    "            x = torch.selu(x)\n",
    "            x = self.linear4(x)\n",
    "\n",
    "        if self.n_outs!=1:\n",
    "            x = torch.nn.functional.softmax(x, dim=-1)\n",
    "            return x\n",
    "        else:\n",
    "            return x.squeeze(-1)\n",
    "\n",
    "# Register hooks to capture the outputs from the transformer layer\n",
    "def get_activation_hook(activations, name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def set_ticks(num_ticks, min_value=0):\n",
    "    if 20 > num_ticks-min_value > 10:\n",
    "        step = 2\n",
    "    elif 40>num_ticks-min_value > 20:\n",
    "        step = 5\n",
    "    elif num_ticks-min_value > 40:\n",
    "        step = 10\n",
    "    else:\n",
    "        step = 1\n",
    "\n",
    "    if min_value !=0:\n",
    "        min_value = (int(min_value/step))*step\n",
    "\n",
    "    return np.arange(min_value, num_ticks, step)\n",
    "    \n",
    "causal_training=True # set this global variable to true so that get batch will return (bs, 256) \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_tsne_colored_by_position(embeddings, labels, ax=None, training_status='',\n",
    "                                  perplexity=32):\n",
    "    \"\"\"\n",
    "    Takes a numpy array of embeddings of shape (bs, T, d) and plots the TSNE visualization,\n",
    "    with colors representing the position index within each sequence before reshaping and a detailed legend.\n",
    "    If an axis (ax) is provided, the plot will be drawn on it.\n",
    "    \"\"\"\n",
    "    # Reshape the embeddings from (bs, T, d) to (-1, d)\n",
    "    bs, T, d = embeddings.shape\n",
    "    flat_embeddings = embeddings.reshape(-1, d)\n",
    "    flat_embeddings = (flat_embeddings - flat_embeddings.mean(axis=0)) / flat_embeddings.std(axis=0)\n",
    "    labels = labels.reshape(-1)\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=500, n_jobs=-1)\n",
    "    tsne_results = tsne_model.fit_transform(flat_embeddings)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # tsne_results = tsne_results.reshape(bs, T, 2)\n",
    "    # for t in range(T):\n",
    "        # ax.scatter(tsne_results[:, t, 0], tsne_results[:, t, 1], marker='o', s=8, alpha=0.5)\n",
    "    # scatter = ax.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels.astype(int), cmap='tab10', marker='o', s=8, alpha=0.5)\n",
    "    scatter = ax.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels.astype(int), cmap='tab10', marker='o', s=8, alpha=0.5)\n",
    "\n",
    "    ax.set_title(f'{training_status}TSNE Plot')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "\n",
    "    # Create a legend with exact color for each index\n",
    "    legend_elements = []\n",
    "    cmap = plt.cm.viridis\n",
    "    for idx in range(T):\n",
    "        legend_elements.append(\n",
    "            plt.Line2D([0], [0], marker='o', color=cmap(idx / (T - 1)), label=str(idx + 1), markersize=2, linestyle='')\n",
    "        )\n",
    "    \n",
    "    ax.legend(handles=legend_elements, title_fontsize=10, fontsize=10, loc='upper right', framealpha=0.5)\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def prob_model(model_name, \n",
    "    model2_name=None, \n",
    "    save_all_models=False,\n",
    "    save = False,\n",
    "    layer_idx=0, \n",
    "    plot_all=False, \n",
    "    rand_perm = True,\n",
    "    before_after=[False, 'training', 'attention'],\n",
    "    total_samples = 512,\n",
    "    max_epochs = 20,\n",
    "    fixed_length = 4,\n",
    "    input_type = ['x', 'x1*x2', 'x1-x2', '[x1, x2]'],\n",
    "    # data_type = ['sample', 'eqx'],\n",
    "    data_type = ['sample', 'same', 'randx', 'eqx', ],\n",
    "    scaler = 0.1,\n",
    "    linear_mode = 1,\n",
    "    loss_type = ['MSE', 'Cross Entropy'],\n",
    "    hook_location = 'after_attn',\n",
    "    plot_type = [ 'violin', 'both', 'tsne'],\n",
    "    ):\n",
    "\n",
    "\n",
    "    total_tokens = total_samples * fixed_length\n",
    "    samples_to_generate = total_tokens//256 + total_tokens%256\n",
    "\n",
    "    probe_dim = 384\n",
    "    probe = PositionalProbe(embedding_dim=384,\n",
    "                            linear_mode=linear_mode,\n",
    "                            n_outs=1)\n",
    "                \n",
    "    # count n params\n",
    "    n_params = sum(p.numel() for p in probe.parameters() if p.requires_grad)\n",
    "    print('probe n params:', n_params, end=' ')\n",
    "    probe.eval()\n",
    "    with torch.no_grad():\n",
    "        flops, params = profile(probe, inputs=(torch.rand(1,1,probe_dim),), verbose=False)\n",
    "        flops, params = clever_format([flops, params], \"%.3f\")\n",
    "        print(f'FLOPS: {flops}, params: {params}')\n",
    "\n",
    "\n",
    "    if save_all_models:\n",
    "        models_to_show = useful_name_list\n",
    "    else:\n",
    "        models_to_show = [model_name]\n",
    "\n",
    "    \n",
    "    for model_name in tqdm(models_to_show):\n",
    "        \n",
    "        acc = model_name.split('_')[0]\n",
    "        try:\n",
    "            iteration = int(re.search(r'acc_(\\d+).pt', model_name).group(1))\n",
    "        except:\n",
    "            if 'untrained' in model_name:\n",
    "                iteration = 0\n",
    "            else:\n",
    "                iteration = 2000\n",
    "        rest = '_'.join(model_name.split('_')[1:])\n",
    "        \n",
    "        model_idx = useful_name_list.index(model_name)\n",
    "        cur_model = model_list[model_idx]\n",
    "        if model2_name is not None:\n",
    "            model2_idx = useful_name_list.index(model2_name)\n",
    "            cur_model2 = model_list[model2_idx]\n",
    "            \n",
    "        cur_models = [cur_model]\n",
    "\n",
    "        if before_after=='training':\n",
    "            rest = '_'.join(rest.split('.')[0].split('_')[:-1])\n",
    "            if acc.split('=')[-1]!='0' or iteration!=0:\n",
    "                print(acc, iteration)\n",
    "                continue\n",
    "            trained_model_name = None\n",
    "            trained_cur_model = None\n",
    "            for m_idx, m_name in enumerate(useful_name_list):\n",
    "                if rest in m_name and not (m_name==model_name):\n",
    "                    trained_model_name = m_name\n",
    "                    trained_cur_model = model_list[m_idx]\n",
    "                    break\n",
    "\n",
    "            assert trained_model_name is not None, 'no trained model found'\n",
    "            cur_models.append(trained_cur_model)\n",
    "\n",
    "        elif before_after=='attention':\n",
    "            cur_models.append(cur_model)\n",
    "\n",
    "        addons = '_' + acc if not before_after=='training' else ''\n",
    "        imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + addons\n",
    "        dir_name = f'./saved_probe_plots_{before_after}/{plot_type}-{hook_location}-{data_type}/{input_type}_{total_samples}_{fixed_length}_{linear_mode}'\n",
    "        if save_all_models:\n",
    "            if f'{dir_name}/{imgname}.pdf' in glob.glob( f'{dir_name}/{imgname}.pdf'):\n",
    "                continue        \n",
    "\n",
    "        \n",
    "        layer_range = range(layer_idx, layer_idx+1) if not (plot_all or save_all_models) \\\n",
    "            else range(len(cur_model.transformer.h))\n",
    "        \n",
    "        plot_height = 2 if plot_type == 'both' else 1\n",
    "        nrows = 1 if not before_after else 2\n",
    "        fig, axs = plt.subplots(nrows*plot_height, len(layer_range), figsize=(6 * len(layer_range), 12.3/2*nrows*plot_height))\n",
    "\n",
    "        \n",
    "        for cm_idx, cur_model in enumerate(cur_models):\n",
    "            for lidx, layer in enumerate(tqdm(layer_range)):\n",
    "                if data_type == 'sample':\n",
    "                    # # X = get_batch(\"train\", batch_size=4096)[0]\n",
    "                    # # X_test = get_batch(\"valid\", batch_size=4096)[0]\n",
    "                    # X = get_batch(\"train\", batch_size=samples_to_generate)[0]\n",
    "                    # X_test = get_batch(\"valid\", batch_size=samples_to_generate//4)[0] # use a bit less for testing in order to plot QAQA\n",
    "                    # X = \"\".join([decode(X[i].tolist())\n",
    "                    #             for i in range(X.shape[0])])[:total_tokens] # truncate x to lower computation\n",
    "                    # X_test = \"\".join([decode(X_test[i].tolist())\n",
    "                    #             for i in range(X_test.shape[0])])[:total_tokens]\n",
    "                    # X_n = np.array(list(X[:len(X) // fixed_length * fixed_length])).reshape(\n",
    "                    #     -1, fixed_length\n",
    "                    # )\n",
    "                    # X = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "                    # X_n = np.array(list(X_test[:len(X_test) // fixed_length * fixed_length])).reshape(\n",
    "                    #     -1, fixed_length\n",
    "                    # )\n",
    "                    # X_test = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "\n",
    "                    # if rand_perm: # shuffle in input sequence\n",
    "                    #     for xidx in range(X.shape[0]):\n",
    "                    #         X[xidx] = X[xidx, torch.randperm(X[xidx].shape[0])]\n",
    "                    #     for xidx in range(X_test.shape[0]):\n",
    "                    #         X_test[xidx] = X_test[xidx, torch.randperm(X_test[xidx].shape[0])]\n",
    "                    choice_list = np.array(list('5678'))\n",
    "                    X = [encode(choice_list[np.random.randint(0, len(choice_list), (fixed_length))])\\\n",
    "                        for _ in range(total_tokens//fixed_length)]\n",
    "                    X = torch.tensor(X).long()\n",
    "                    \n",
    "                    choice_list = np.array(list('1234'))\n",
    "                    X_test = [encode(choice_list[np.random.randint(0, len(choice_list), (fixed_length))]) \\\n",
    "                            for _ in range(total_tokens//fixed_length)]\n",
    "                    X_test = torch.tensor(X_test).long()\n",
    "                    # print(X.shape, X_test.shape)\n",
    "                elif data_type=='eqx':\n",
    "                    X = [cur_model.create_equal_distancing_vecotrs(fixed_length, 384, small_component=0.1)[0] *0.05\\\n",
    "                        for _ in range(total_tokens//fixed_length)]\n",
    "                    X = torch.tensor(X).float()\n",
    "                    X_test = [cur_model.create_equal_distancing_vecotrs(fixed_length, 384, small_component=0.1)[0] *0.05\\\n",
    "                            for _ in range(total_tokens//fixed_length)]\n",
    "                    X_test = torch.tensor(X_test).float()\n",
    "\n",
    "                elif data_type=='randx':\n",
    "                    # X = [np.random.rand(fixed_length, 384) * scaler\\\n",
    "                    #     for _ in range(total_tokens//fixed_length)]\n",
    "                    X = np.random.rand(total_tokens//fixed_length, fixed_length, 384)* scaler\n",
    "                    X = torch.tensor(X).float()\n",
    "                    X_test = np.random.rand(total_tokens//fixed_length, fixed_length, 384)* scaler\n",
    "                    X_test = torch.tensor(X_test).float()\n",
    "                elif data_type == 'same':\n",
    "                    X = [np.ones((fixed_length, 384)) * scaler\\\n",
    "                        for _ in range(total_tokens//fixed_length)]\n",
    "                    X = torch.tensor(X).float()\n",
    "                    X_test = [np.ones((fixed_length, 384)) * scaler\\\n",
    "                            for _ in range(total_tokens//fixed_length)]\n",
    "                    X_test = torch.tensor(X_test).float()\n",
    "\n",
    "                \n",
    "\n",
    "                # X = X.to(device)\n",
    "                if before_after=='attention':\n",
    "                    hook_location = 'before_attn' if cm_idx==0 else 'after_attn'\n",
    "\n",
    "                # Function to create a dataset with position labels\n",
    "                layer_activations = {}\n",
    "                def create_position_dataset(model, inputs, layer, loss_type):\n",
    "                \n",
    "                    if hook_location == 'after_attn':\n",
    "                        hook = model.transformer.h[layer].attn.identity.register_forward_hook(\n",
    "                            get_activation_hook(layer_activations, f\"layer_{layer}\")\n",
    "                        )\n",
    "                    elif hook_location == 'before_attn':\n",
    "                        hook = model.transformer.h[layer].attn.pre_att_identity.register_forward_hook(\n",
    "                            get_activation_hook(layer_activations, f\"layer_{layer}\")\n",
    "                        )\n",
    "                    elif hook_location == 'mlp':\n",
    "                        hook = model.transformer.h[layer].layer_identity.register_forward_hook(\n",
    "                            get_activation_hook(layer_activations, f\"layer_{layer}\")\n",
    "                        )\n",
    "\n",
    "\n",
    "                    if hook_location != 'raw':\n",
    "                        with torch.no_grad():\n",
    "                            if data_type == 'sample':\n",
    "                                _ = model(inputs.to(device))\n",
    "                            elif data_type in ['eqx', 'randx', 'same']:\n",
    "                                _ = model(inputs.to(device), direct_input_modification=True)\n",
    "                        hook.remove()\n",
    "                    else:\n",
    "                        layer_activations[f\"layer_{layer}\"] = inputs\n",
    "\n",
    "                    \n",
    "                    # inputs.size(1) is the sequence length\n",
    "                    # if input_type == 'x':\n",
    "                    positions = torch.arange(inputs.size(1)).repeat(inputs.size(0), 1).to(inputs.device)\n",
    "                    activation_tensor = layer_activations[f\"layer_{layer}\"] # Adjust shape if necessary\n",
    "                    if input_type in ['x1*x2', 'x1-x2'] :\n",
    "                        new_positions = []\n",
    "                        new_activation_tensor = []\n",
    "                        for sidx, x1 in enumerate(activation_tensor):\n",
    "                            # pair_idx = np.random.randint(0, len(activation_tensor))\n",
    "                            # pair_odering = np.random.permutation(len(x1))\n",
    "                            # x2 = x1[pair_odering]\n",
    "                            # p1, p2 = positions[sidx], positions[sidx][pair_odering]\n",
    "\n",
    "                            x2 = x1[-1]\n",
    "                            \n",
    "                            p1, p2 = positions[sidx], positions[sidx, -1]\n",
    "                            if input_type == 'x1*x2':  # maybe normalize by their norms\n",
    "                                new_activation_tensor.append((x1*x2)[None, ...]/(x1.norm()*x2.norm()))\n",
    "                            elif input_type == 'x1-x2':\n",
    "                                new_activation_tensor.append((x1-x2).abs()[None, ...])\n",
    "                            elif input_type == '[x1, x2]':\n",
    "                                new_activation_tensor.append((np.hstack([x1,x2])).abs()[None, ...])\n",
    "                            new_positions.append((p1-p2).abs()[None, ...])\n",
    "                        # print((p1-p2).abs()[None, ...].shape, (p1-p2).abs())\n",
    "                        # print(x1.shape, x2.shape, (x1*x2).shape)\n",
    "                        positions = torch.vstack(new_positions)\n",
    "                        activation_tensor = torch.vstack(new_activation_tensor)\n",
    "                        # print('Pshape:', positions)\n",
    "                        # print('Ashape:', activation_tensor.shape)\n",
    "                        # print(': )', positions.shape, activation_tensor.shape)\n",
    "\n",
    "                    # print(inputs.shape, activation_tensor.shape, positions.shape, len(layer_activations))\n",
    "\n",
    "                    # return TensorDataset(activation_tensor, positions)\n",
    "                    if loss_type == 'Cross Entropy':\n",
    "                        positions = positions.long()\n",
    "                    elif loss_type == 'MSE':\n",
    "                        positions = positions.float()\n",
    "                    activation_tensor = activation_tensor.squeeze(0)\n",
    "                    return activation_tensor, positions\n",
    "\n",
    "                # Assuming `input_idx` and `cur_model` are defined\n",
    "                \n",
    "                train_activation_tensor, train_positions = create_position_dataset(cur_model,  X,  layer, loss_type)\n",
    "                dataset = TensorDataset(train_activation_tensor, train_positions)\n",
    "                \n",
    "\n",
    "                test_model = cur_model if model2_name is None else cur_model2\n",
    "                test_activation_tensor, test_positions =  create_position_dataset(test_model, X_test, layer, loss_type)\n",
    "                test_dataset = TensorDataset(test_activation_tensor, test_positions) \n",
    "\n",
    "                dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "                # Initialize the probe\n",
    "                \n",
    "                bs, T, probe_dim = layer_activations[f\"layer_{layer}\"].shape\n",
    "                n_outs = T if loss_type == 'Cross Entropy' else 1\n",
    "                probe = PositionalProbe(embedding_dim=probe_dim,\n",
    "                            linear_mode=linear_mode,\n",
    "                            n_outs=n_outs).to(device)\n",
    "                \n",
    "             \n",
    "\n",
    "                optimizer = torch.optim.AdamW(probe.parameters())\n",
    "\n",
    "                if loss_type == 'MSE':\n",
    "                    criterion = nn.MSELoss()  # Using Mean Squared Error Loss for regression\n",
    "                elif loss_type == 'Cross Entropy':\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                # Train the probe and evaluate Pearson correlation\n",
    "                # print(len(dataset), len(test_dataset))\n",
    "                # best_loss = float('inf')\n",
    "                # best_weights = None\n",
    "                for epoch in range(max_epochs):\n",
    "                    cur_loss = []\n",
    "                    for data, targets in dataloader:\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = probe(data.to(device))\n",
    "                        loss = criterion(outputs, targets.to(device))  # Ensure targets are float for MSE calculation\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        cur_loss.append(loss.item())\n",
    "                    cur_loss = np.mean(cur_loss)\n",
    "                    # if cur_loss < best_loss:\n",
    "                    #     best_loss = cur_loss\n",
    "                    #     best_weights = probe.state_dict()\n",
    "                    # if epoch % 15 == 0 or epoch == 0 or epoch == max_epochs-1:\n",
    "                        # print(f\"Epoch {epoch}, Loss: {cur_loss.item():.02f}\", end='\\t')\n",
    "                # print()\n",
    "\n",
    "                # probe.load_state_dict(best_weights)\n",
    "                \n",
    "                all_outputs = []\n",
    "                all_targets = []\n",
    "                all_test_loss = []\n",
    "                probe.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data, targets in test_loader:\n",
    "                        outputs = probe(data.to(device))\n",
    "                        test_loss = criterion(outputs, targets.to(device))\n",
    "                        all_test_loss.append(test_loss.item())\n",
    "                        if loss_type == 'Cross Entropy':\n",
    "                            outputs = torch.argmax(outputs, dim=-1)\n",
    "                        all_outputs.append(outputs.detach().cpu().numpy())\n",
    "                        all_targets.append(targets.detach().cpu().numpy())\n",
    "                \n",
    "                # print(f\"Test Loss: {np.mean(all_test_loss)}\", end = '\\t')\n",
    "\n",
    "                # Concatenate all collected data\n",
    "                all_outputs = np.concatenate(all_outputs).flatten()\n",
    "                all_targets = np.concatenate(all_targets).flatten()\n",
    "\n",
    "                # Compute the Pearson correlation coefficient\n",
    "                correlation, _ = pearsonr(all_outputs, all_targets)\n",
    "                # print(f'Pearson correlation coefficient: {correlation}')\n",
    "                \n",
    "                slope, intercept, r_value, p_value, std_err = linregress(all_targets, all_outputs)\n",
    "                best_fit_line = slope * np.array(all_targets) + intercept\n",
    "\n",
    "\n",
    "                data = pd.DataFrame({\n",
    "                    'Predicted Positions': all_outputs,\n",
    "                    'True Positions': all_targets.astype(int), \n",
    "                })  \n",
    "\n",
    "                data['Counts'] = data.groupby(['Predicted Positions', 'True Positions'])['Predicted Positions'].transform('count')\n",
    "                # Scatter plot on the first subplot\n",
    "                # if not before_after_training:\n",
    "\n",
    "                #     ax0loc = axs[0, lidx] if len(layer_range) > 1 else axs[0]\n",
    "                #     ax0loc.scatter(all_targets, all_outputs, alpha=0.4, s=5)\n",
    "                #     ax0loc.set_title(f'Layer {layer+1} (r={correlation:.2f}, loss={np.mean(all_test_loss):.2f})')\n",
    "                #     ax0loc.set_xlabel('True Positions')\n",
    "                #     ax0loc.set_ylabel('Predicted Positions')\n",
    "                #     ax0loc.grid(True)\n",
    "                #     ax0loc.set_ylim(min(all_outputs)-5, max(all_outputs)+5)\n",
    "                #     ax0loc.plot(all_targets, best_fit_line, 'r', label=f'y={slope:.2f}x+{intercept:.2f}')\n",
    "                #     # ax0loc.legend()\n",
    "                    \n",
    "                #     max_value = max(all_targets)\n",
    "                #     ax0loc.set_xticks(set_ticks(max_value+1))\n",
    "\n",
    "                # Violin plot on the second subplot\n",
    "\n",
    "                training_status = ''\n",
    "                if before_after=='training':\n",
    "                    training_status = 'Init ' if cm_idx==0 else 'Trained '\n",
    "\n",
    "                if plot_type in ['both', 'violin', 'scatter']:\n",
    "                    if len(layer_range) > 1 and before_after:\n",
    "                        ax1loc = axs[cm_idx*plot_height, lidx] \n",
    "                    elif len(layer_range) > 1:\n",
    "                        ax1loc = axs[lidx]\n",
    "                    else:\n",
    "                        ax1loc = axs[cm_idx*plot_height]\n",
    "                    \n",
    "                    if plot_type == 'violin':\n",
    "                        sns.violinplot(x='True Positions', y='Predicted Positions', \n",
    "                                data=data, ax=ax1loc, width=0.8, linewidth=0.5)\n",
    "                    if plot_type == 'scatter':\n",
    "                        ax1loc.scatter(all_targets, all_outputs, alpha=0.4, s=5)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    ax1loc.set_title(f'{training_status}Layer {layer+1} (r={correlation:.2f}, loss={np.mean(all_test_loss):.2f})')\n",
    "                    ax1loc.set_xlabel('True Positions')\n",
    "                    ax1loc.set_ylabel('Predicted Positions')\n",
    "                    ax1loc.grid(True)\n",
    "                    ax1loc.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                    ax1loc.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "                    # print(max(all_targets))\n",
    "                    extension = int(max(all_targets)*0.1)\n",
    "                    # ax1loc.set_ylim(min(all_targets)-extension, max(all_targets)+extension)\n",
    "                    ax1loc.set_xlim(int(min(all_targets))-extension, int(max(all_targets))+extension)\n",
    "                    ax1loc.plot(all_targets, best_fit_line, 'r', label=f'y={slope:.2f}x+{intercept:.2f}')\n",
    "                    # ax1loc.legend()\n",
    "\n",
    "                    max_value = max(all_targets)\n",
    "                    ax1loc.set_xticks(set_ticks(max_value+1).astype(int))\n",
    "                    max_value = max(all_outputs); min_value = min(all_outputs)\n",
    "                    ax1loc.set_yticks(set_ticks(max_value+1, min_value=int(min_value)).astype(int))\n",
    "                    \n",
    "\n",
    "                if plot_type in ['both', 'tsne']:\n",
    "                    add1 = 1 if plot_type == 'both' else 0\n",
    "\n",
    "                    # ax2loc = axs[cm_idx*plot_height + add1, lidx] if len(layer_range) > 1 else axs[cm_idx*plot_height]\n",
    "                    if len(layer_range) > 1 and before_after:\n",
    "                        ax2loc = axs[cm_idx*plot_height + add1, lidx] \n",
    "                    elif len(layer_range) > 1:\n",
    "                        ax2loc = axs[lidx]\n",
    "                    else:\n",
    "                        ax2loc = axs[cm_idx*plot_height + add1]\n",
    "                    embeddings = test_activation_tensor.detach().cpu().numpy()\n",
    "                    labels = test_positions.detach().cpu().numpy()\n",
    "                    seletion = np.arange(len(labels))[::2]\n",
    "                    embeddings = embeddings[seletion]\n",
    "                    labels = labels[seletion]\n",
    "\n",
    "                    print(embeddings.shape)\n",
    "                    plot_tsne_colored_by_position(embeddings, labels, ax2loc, training_status)\n",
    "\n",
    "\n",
    "                # if before_after_training:\n",
    "                #     annos = 'ab'\n",
    "                #     ax1loc.annotate(f'({annos[cm_idx]})', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=14, fontproperties=prop)\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(left=0.03, right=0.975, top=0.97, bottom=0.07, hspace=0.3)\n",
    "\n",
    "        \n",
    "        if save_all_models or save:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "            fig.savefig(f'{dir_name}/{imgname}.pdf', format='pdf')\n",
    "\n",
    "            plt.close()\n",
    "            del fig, axs, data, probe, optimizer, criterion, dataloader, test_loader\n",
    "            gc.collect()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "widgets.interact(prob_model, \n",
    "                 model_name=useful_name_list, \n",
    "                 model2_name=[None] + useful_name_list,\n",
    "                 save=False,\n",
    "                 before_after_training = False,\n",
    "                 layer_idx=(0, 5), \n",
    "                 plot_all=True, \n",
    "                 rand_perm=True, \n",
    "                 fixed_length=(4,128),\n",
    "                 total_samples=(1000, 5000),\n",
    "                 linear_mode=(0, 5),\n",
    "                 hook_location=['after_attn', 'before_attn', 'mlp', 'raw'],\n",
    "                 plot_type=['violin', 'tsne', 'scatter', 'both'],\n",
    "                 scaler=(0.001, 0.5, 0.001),)\n",
    "\n",
    "                # hook_location=['before_attn', 'after_attn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "1. It is the averaging effect of the softmax attention weights that generates the adjacency pattern.\n",
    "2. It requires the model weights to be with small variance.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the distribution of $ Vx $ when $ x $ and $ V $ are independent and identically distributed (iid) with a normal distribution $ N(\\mu, \\sigma^2) $, where $ x $ is a $ d \\times 1 $ vector and $ V $ is a $ d \\times d $ matrix, we proceed as follows:\n",
    "\n",
    "**Step 1: Understand the Components**\n",
    "- **Vector $ x $**: Each element $ x_i $ of $ x $ is distributed as $ N(\\mu, \\sigma^2) $.\n",
    "- **Matrix $ V $**: Each element $ V_{ij} $ of $ V $ is also distributed as $ N(\\mu, \\sigma^2) $.\n",
    "\n",
    "**Step 2: Multiplication $ Vx $**\n",
    "When $ V $ is multiplied by $ x $, the resulting vector $ y = Vx $ will have each element $ y_i $ given by:\n",
    "$ y_i = \\sum_{j=1}^d V_{ij} x_j $\n",
    "\n",
    "**Step 3: Distribution of Each $ y_i $**\n",
    "Since each $ V_{ij} $ and $ x_j $ are independent and normally distributed, the product $ V_{ij} x_j $ is not normally distributed. However, the sum of these products (i.e., the dot product forming each $ y_i $) could be approximated or analyzed further if $ V $ and $ x $ were linear transformations of Gaussian random variables.\n",
    "\n",
    "**Step 4: Central Limit Theorem (CLT)**\n",
    "When $ d $ (the dimension) is large, the sum of a large number of independent random variables (as is the case in the components of $ y_i $) will tend towards a normal distribution according to the Central Limit Theorem. Thus, each $ y_i $ can be approximated as normally distributed.\n",
    "\n",
    "**Step 5: Calculate Mean and Variance**\n",
    "The mean $ E[y_i] $ is:\n",
    "\n",
    "$ E[y_i] = E\\left[\\sum_{j=1}^d V_{ij} x_j\\right] = \\sum_{j=1}^d E[V_{ij}] E[x_j] = d \\mu^2 $\n",
    "\n",
    "The variance $ \\text{Var}(y_i) $ involves calculating:\n",
    "\n",
    "$ \\text{Var}(y_i) = \\sum_{j=1}^d \\text{Var}(V_{ij} x_j) $\n",
    "\n",
    "Given $ V_{ij} $ and $ x_j $ are independent, each product's variance $ \\text{Var}(V_{ij} x_j) $ will be:\n",
    "\n",
    "$ \\text{Var}(V_{ij} x_j) = E[(V_{ij} x_j)^2] - (E[V_{ij} x_j])^2 $\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$ \\text{Var}(V_{ij} x_j) = E[V_{ij}^2] E[x_j^2] - (E[V_{ij}] E[x_j])^2 $\n",
    "$ \\text{Var}(V_{ij} x_j) = (\\mu^2 + \\sigma^2)^2 - \\mu^4 $\n",
    "\n",
    "Adding these variances together (since the products are independent for different $ j $):\n",
    "\n",
    "$ \\text{Var}(y_i) = d((\\mu^2 + \\sigma^2)^2 - \\mu^4) $\n",
    "\n",
    "**Conclusion**\n",
    "If $ d $ is large enough for the Central Limit Theorem to apply, then each element of the vector $ y = Vx $ is approximately normally distributed, $ y_i \\approx N(d\\mu^2, d((\\mu^2 + \\sigma^2)^2 - \\mu^4)) $. This approach assumes that the elements of $ V $ and $ x $ are iid, and $ d $ is sufficiently large for the CLT approximation. If the matrix and vector dimensions or distributions differ, further adjustments would be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b92a1ec15a145469117edb516be7f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='seq_len', max=4096, min=10), FloatSlider(value=0.0, desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_attention(seq_len=20, mu_v=0, sig_v=1, mu_A=0, sig_A=1, shift=0, dimension=100, sim_func='cos', dist_type='normal', v_n=0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# def causal_softmax(x):\n",
    "#     \"\"\"Compute causal softmax values for the vector 'x'.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "#     return e_x / e_x.sum()\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['axes.labelsize'] = 12  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 12  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 12  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 12  # Title\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "def plot_attention(\n",
    "    seq_len = 20,   # Number of vectors\n",
    "    mu_v = 0,\n",
    "    sig_v = 1,\n",
    "    mu_A = 0,\n",
    "    sig_A = 1,\n",
    "    shift = 0,\n",
    "    dimension = 100,  # Dimensionality of each vector\n",
    "    sim_func = 'cos',\n",
    "    dist_type = 'normal',\n",
    "    v_n = 0,\n",
    "):\n",
    "    v_n = min(v_n, seq_len-1)\n",
    "    sim_func_name = 'Dot Product' if sim_func == 'dot' else 'Cosine Similarity'\n",
    "    # Parameters\n",
    "\n",
    "\n",
    "    # Initialize e vectors from index 0 to n-1\n",
    "    # v_vectors = np.array([np.random.normal(mu_v, sig_v, dimension) for _ in range(seq_len)])\n",
    "    v_vectors = np.random.normal(mu_v, sig_v, (seq_len, dimension))\n",
    "\n",
    "    # Initialize A coefficients from index 0 to n-1 and apply causal softmax\n",
    "    # A = np.zeros((n, n))\n",
    "    # A = np.random.normal(0, np.sqrt(n), (n, n))\n",
    "    if dist_type == 'normal':\n",
    "        A = np.random.normal(mu_A, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'gemma':\n",
    "        A = np.random.gamma(1, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'uniform':\n",
    "        A = np.random.uniform(mu_A, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'possion':\n",
    "        A = np.random.poisson(mu_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'chi':\n",
    "        A = np.random.chisquare()\n",
    "\n",
    "    # A = np.random.normal(10, 1, (n, n))\n",
    "    # A = np.random.gamma(0, 0.1, (n, n))\n",
    "    # A = np.random.chisquare(0.001, (n, n))\n",
    "    # A = np.random.uniform(0, 0.01, (n, n))\n",
    "\n",
    "    A = A * (1.0 / math.sqrt(v_vectors.shape[-1]))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    _A = np.tril(A, 0)\n",
    "    axes[0].hist(_A[np.where(_A!=0)].flatten(), bins=200)\n",
    "    _A_std_row_wise = _A[np.where(_A!=0)].flatten().std(axis=-1).mean()\n",
    "    axes[0].set_title(f'A distribution before softmax (std_row-wise: {_A_std_row_wise:.3f})')\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        # A[i, :i+1] = softmax(A[i, :i+1]) + 1 # amazing : )\n",
    "        A[i, :i+1] = softmax(A[i, :i+1]) + shift\n",
    "\n",
    "\n",
    "    A = np.tril(A, 0)\n",
    "\n",
    "    non_zero_A = A[np.where(A!=0)].flatten()\n",
    "    len_A = len(non_zero_A)\n",
    "    sorted_non_zero_A = sorted(non_zero_A)\n",
    "    axes[1].hist(non_zero_A, bins=200)\n",
    "    axes[1].set_title(f'A distribution after softmax (Abs_Avg = {np.abs(sorted_non_zero_A)[:len_A//4].mean():.06f})')\n",
    "\n",
    "    axes[2].plot(A[-1])\n",
    "    axes[2].set_title(f'The last row')\n",
    "\n",
    "    plt.show()\n",
    "    # Compute y vectors using matrix multiplication with a lower triangular matrix of A\n",
    "    y_vectors = A @ v_vectors\n",
    "    norms = np.linalg.norm(y_vectors, axis=1)\n",
    "    print(y_vectors.shape, norms.shape)\n",
    "\n",
    "    # plt.figure(figsize=(6, 5))\n",
    "    fig, axes = plt.subplots(1,4, figsize=(28, 5))\n",
    "    axes[0].hist(y_vectors[v_n], bins=200)\n",
    "    axes[0].set_title(f'Output Vector {v_n+1}, norm={np.linalg.norm(norms[v_n]):.3f}')\n",
    "    axes[1].hist(y_vectors[-1], bins=200)\n",
    "    axes[1].set_title(f'Output Vector {len(y_vectors)}, norm={np.linalg.norm(norms[-1]):.3f}')\n",
    "\n",
    "\n",
    "    # Compute cosine similarities using vector operations\n",
    "    # cosine_similarities = np.dot(y_vectors, y_vectors.T) / np.outer(norms, norms)\n",
    "    cosine_similarities = np.dot(y_vectors, y_vectors.T) \n",
    "    divisor = 1 if sim_func=='dot' else np.outer(norms, norms)\n",
    "    if sim_func == 'dot':\n",
    "        cosine_similarities = cosine_similarities / divisor\n",
    "    else:\n",
    "        cosine_similarities = cosine_similarity(y_vectors, y_vectors)\n",
    "\n",
    "\n",
    "    axes[2].plot(cosine_similarities[-1])\n",
    "    axes[2].set_title(f'Last Vector {sim_func_name}')\n",
    "    axes[3].plot(np.outer(norms, norms)[-1])\n",
    "    axes[3].set_title(f'Last Norm Divisor')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Extracting upper triangular part of cosine similarities matrix, excluding diagonal\n",
    "    results = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i + 1, seq_len):\n",
    "            results[i, j] = cosine_similarities[i, j]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    cm = axes[0].imshow(A, cmap='Reds')\n",
    "    axes[0].set_title('Attention Map')\n",
    "    plt.colorbar(cm)\n",
    "\n",
    "    cm = axes[1].imshow(cosine_similarities, cmap='Reds')\n",
    "    # cm = axes[1].imshow(cosine_similarities, )1````\n",
    "    axes[1].set_title(f'{sim_func_name} Matrix')\n",
    "    plt.colorbar(cm)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "interact(plot_attention, \n",
    "         seq_len=(10, 4096), \n",
    "         mu_v = (-10, 10, 0.01), \n",
    "         sig_v = (0, 2, 0.0001),\n",
    "         mu_A = (-20, 20, 0.01), \n",
    "         sig_A = (0, 10, 0.0001),  \n",
    "         shift=(-2, 2, 0.001), \n",
    "         dimension=(4, 384),\n",
    "         sim_func = ['cos', 'dot'],\n",
    "         dist_type = ['normal', 'gemma', 'uniform', 'possion'],\n",
    "         v_n = (0, 2048)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'order(13597,79351)=43120\\norder(20695,26905)=02314\\norder(45126,45216)=01324\\norder(74916,91746)=23014\\norder(2765,5276)=3012\\norder('"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('./data/order/test_order_10000.txt', 'r').read()[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41b9f9c81e4428dbf9054be605d6ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_name', options=('acc=0_wherex9_nc_original_sd240_T2408310224â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_att(model_name, layer_idx=0, plot_all=0, save_map=False, find_best=True, search_all_heads=False, metric_id=4, bestk=4, clean_plot=False, input_name=['eqx', 'randx', 'sample_1', 'sample_2', 'sample_3'], amp_head=2)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the difference in projection for the same vector\n",
    "# x = torch.rand(1, 1, 384).to(device)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from ast import literal_eval\n",
    "import ipywidgets as widgets\n",
    "\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['axes.labelsize'] = 12  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 12  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 12  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 12  # Title\n",
    "\n",
    "n_embd = 384\n",
    "sz = 64\n",
    "# x = torch.zeros(1, sz, n_embd).to(device)\n",
    "# sc = torch.rand(1, sz, n_embd).to(device)\n",
    "\n",
    "# # idx = np.random.randint(0, n_embd)\n",
    "# idx = np.random.randint(0, n_embd, size=(sz))\n",
    "\n",
    "# x[0, :, idx] = 1\n",
    "# x += sc\n",
    "x = np.random.normal(0, 0.01, (1, sz, n_embd))\n",
    "x = torch.tensor(x).float().to(device)\n",
    "\n",
    "# y = model_list[17].transformer.h[1].attn.c_attn(x)\n",
    "# input_idx = torch.tensor(encode('12modp(123)=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('$123+45=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('$333+444=777$\\n$123+54=')).to(device)[None,...]\n",
    "# input_idx2 = torch.tensor(encode('$33333333333333')).to(device)[None,...]\n",
    "# input_idx3 = torch.tensor(encode('$992+299=')).to(device)[None,...]\n",
    "input_idx = torch.tensor(encode('wherex(0917328,7)=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('rev(0917328938453)=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode(open('./data/rev/test_rev16_10000.txt', 'r').read()[:128])).to(device)[None,...]\n",
    "\n",
    "\n",
    "\n",
    "# input_idx2 = torch.tensor(encode('rev(54321)=')).to(device)[None,...]\n",
    "# input_idx2 = torch.tensor(encode('\\norder(56310,03651)=42103\\norder(94213,31492)=')).to(device)[None,...]\n",
    "\n",
    "input_idx2 = torch.tensor(encode(open('./data/order/test_order_10000.txt', 'r').read()[:128])).to(device)[None,...]\n",
    "\n",
    "# input_idx3 = torch.tensor(encode('\\norder(94871,87194)=23401\\norder(65023,35206)=')).to(device)[None,...]\n",
    "\n",
    "# input_idx3 = torch.tensor(encode('$992+299=')).to(device)[None,...]\n",
    "rand_nums =  list(np.random.randint(0, 10, (sz)))\n",
    "rand_nums = ''.join(map(str, rand_nums))\n",
    "\n",
    "input_idx3 = torch.tensor(encode(f'rev({rand_nums})=')).to(device)[None,...]\n",
    "\n",
    "\n",
    "eqx = torch.tensor(model.create_equal_distancing_vecotrs(sz, n_embd, small_component=0.01)[0]).to(device).to(torch.float32)[None,...] * 0.01\n",
    "\n",
    "inputs_dict = {\n",
    "    'sample_1': input_idx,\n",
    "    'sample_2': input_idx2,\n",
    "    'sample_3': input_idx3,\n",
    "    'eqx': eqx,\n",
    "    'randx': x,\n",
    "} \n",
    "\n",
    "# Specify the path to your Times New Roman font file\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# def levenshteinDistance(s1, s2):\n",
    "#     if len(s1) > len(s2):\n",
    "#         s1, s2 = s2, s1\n",
    "\n",
    "#     distances = range(len(s1) + 1)\n",
    "#     for i2, c2 in enumerate(s2):\n",
    "#         distances_ = [i2+1]\n",
    "#         for i1, c1 in enumerate(s1):\n",
    "#             if c1 == c2:\n",
    "#                 distances_.append(distances[i1])\n",
    "#             else:\n",
    "#                 distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "#         distances = distances_\n",
    "#     return distances[-1]\n",
    "\n",
    "# def get_PE_tendency(mat):\n",
    "#     r = 0\n",
    "#     r2 = 0\n",
    "#     r3 = 0\n",
    "#     r4 = 0\n",
    "#     r5 = 0\n",
    "#     r5_counts = 0\n",
    "#     r6 = 0\n",
    "#     r6_count = 0\n",
    "#     mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "#     for lidx, layer in enumerate(mat[1:]):\n",
    "#         r += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "#         r3 += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "#         r3 -= (np.diff(layer[:lidx+1], axis=0) <= 0).sum()\n",
    "\n",
    "#         c2p = sum([((layer[j+1:lidx+1]-layer[j]) > 0).sum() for j in range(lidx+1)])\n",
    "#         c2m = sum([((layer[j+1:lidx+1]-layer[j]) <= 0).sum() for j in range(lidx+1)])\n",
    "#         r2 += c2p\n",
    "#         r4 += c2p - c2m\n",
    "\n",
    "#         valid_layer = layer[:lidx+1]\n",
    "#         if len(valid_layer)<3: \n",
    "#             continue\n",
    "\n",
    "#         # cur_order = valid_layer\n",
    "#         # original_order = valid_layer[np.argsort(valid_layer)]\n",
    "#         # corr = spearmanr(original_order, cur_order).correlation\n",
    "#         # if np.isnan(corr):\n",
    "#             # continue\n",
    "#         # r5 += spearmanr(original_order, cur_order).correlation \n",
    "\n",
    "\n",
    "#         right_order = np.argsort(valid_layer-np.arange(len(valid_layer))*1e-5)\n",
    "#         right_right_order = np.argsort(right_order)\n",
    "#         original_order = np.arange(len(valid_layer))\n",
    "#         edit_distance = levenshteinDistance(right_order, original_order)\n",
    "#         r5 += edit_distance\n",
    "#         r5_counts += len(valid_layer)\n",
    "#         r_value = pearsonr(original_order, right_right_order)[0]\n",
    "#         if np.isnan(r_value): continue\n",
    "#         r6 += pearsonr(original_order, right_right_order)[0]\n",
    "#         r6_count += 1\n",
    "#         # r5 += pearsonr(original_order, cur_order)[0]\n",
    "\n",
    "#     max_r = np.arange(mat.shape[0]-1).sum()\n",
    "#     t1 = round(r/max_r, 2)\n",
    "\n",
    "#     max_r2 = sum([ np.arange(n+1).sum() for n in np.arange(mat.shape[0]-1)])\n",
    "#     t2 = round(r2/max_r2, 2)\n",
    "\n",
    "#     t3 = round(r3/max_r, 2)\n",
    "\n",
    "#     t4 = round(r4/max_r2, 2)\n",
    "    \n",
    "#     t5 =  round((r5_counts-r5) / r5_counts, 2)\n",
    "\n",
    "#     t6 = round(r6/r6_count, 2)\n",
    "#     return f'({t1},{t2},{t3},{t4},{t5},{t6})'\n",
    "\n",
    "\n",
    "# def generate_tendency_map(mat):\n",
    "#     mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "#     empty_mat = np.zeros_like(mat)\n",
    "#     for lidx, layer in enumerate(mat):\n",
    "#         if lidx == 0:\n",
    "#             continue\n",
    "#         counter = 0\n",
    "#         for j in range(1, lidx+1):\n",
    "#             if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "#                 counter = 0\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "#             empty_mat[lidx, j] = counter\n",
    "    \n",
    "#     return empty_mat\n",
    "\n",
    "# def generate_tendency_map(mat):\n",
    "#     mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "#     empty_mat = np.zeros_like(mat)\n",
    "#     for lidx, layer in enumerate(mat):\n",
    "      \n",
    "#         counter = 0\n",
    "#         for j in range(0, lidx+1):\n",
    "#             if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "#                 counter = 0\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "#             empty_mat[lidx, j] = counter\n",
    "#             # empty_mat[lidx, j] = np.log(mat[lidx, j])\n",
    "    \n",
    "#     return empty_mat\n",
    "\n",
    "    \n",
    "\n",
    "def plot_att(model_name, \n",
    "             layer_idx=0, \n",
    "             plot_all=0, \n",
    "             save_map=False, \n",
    "             find_best=True, \n",
    "             search_all_heads=False,\n",
    "             metric_id = 4,\n",
    "             bestk = 4,\n",
    "             clean_plot = False,\n",
    "             input_name = sorted(list(inputs_dict.keys())), \n",
    "             amp_head=2,):\n",
    "    total_std = []\n",
    "    total_scores = []\n",
    "    next_att_scores = []\n",
    "\n",
    "    acc = model_name.split('_')[0]\n",
    "    rest = '_'.join(model_name.split('_')[1:])\n",
    "    imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + '_' + acc\n",
    "\n",
    "    # cur_model = model_list[12]\n",
    "    # cur_model = model_list[2]\n",
    "    model_idx = useful_name_list.index(model_name)\n",
    "    cur_model = model_list[model_idx]\n",
    "\n",
    "    \n",
    "    layer_range = range(layer_idx, layer_idx+1) if plot_all==0 else range(plot_all)\n",
    "    \n",
    "    if len(layer_range) > len(cur_model.transformer.h):\n",
    "        layer_range = range(len(cur_model.transformer.h))\n",
    "\n",
    "    for level in layer_range:\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        h1 = cur_model.transformer.h[level].attn.c_attn.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}\")\n",
    "        )\n",
    "        \n",
    "        h1q = cur_model.transformer.h[level].attn.iq.register_forward_hook(\n",
    "            getActivation(f\"q{level}\")\n",
    "        ) \n",
    "        h1k = cur_model.transformer.h[level].attn.ik.register_forward_hook(\n",
    "            getActivation(f\"k{level}\")\n",
    "        ) \n",
    "        h1v = cur_model.transformer.h[level].attn.iv.register_forward_hook(\n",
    "            getActivation(f\"v{level}\")\n",
    "        ) \n",
    "\n",
    "        h2 = cur_model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}_iden\")\n",
    "        )\n",
    "\n",
    "        mlp_out = cur_model.transformer.h[level].layer_identity.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}_mlp\")\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_values = inputs_dict[input_name].cuda()\n",
    "            cur_model = cur_model.cuda()\n",
    "            if 'sample' in input_name:\n",
    "                out = cur_model(input_values)\n",
    "                y = decode([out[0].detach().cpu().numpy().argmax()])\n",
    "                print(y)\n",
    "            else: # eqx or randx\n",
    "                _ = cur_model(input_values, direct_input_modification=True)\n",
    "            # cur_model = cur_model.cpu()\n",
    "      \n",
    "\n",
    "        h1.remove()\n",
    "        h1q.remove()\n",
    "        h1k.remove()\n",
    "        h1v.remove()\n",
    "        h2.remove()\n",
    "        mlp_out.remove()\n",
    "\n",
    "\n",
    "        '''Plot the attention maps'''\n",
    "        xw = activation[f\"layer_{level}\"]\n",
    "        q, k, v  = xw.split(n_embd, dim=2)\n",
    "        q_reshape = activation[f\"q{level}\"]\n",
    "        k_reshape = activation[f\"k{level}\"]\n",
    "        v_reshape = activation[f\"v{level}\"]\n",
    "        mlp_out = activation[f\"layer_{level}_mlp\"]\n",
    "\n",
    "        # plt.figure(figsize=(12, 3))\n",
    "        k_numpy = k_reshape.detach().cpu().numpy()\n",
    "        k_numpy = k_numpy.transpose(0, 2, 1, 3)\n",
    "        bs, T, n_head, head_dim = k_numpy.shape\n",
    "        k_numpy = k_numpy.reshape(bs, T, -1)\n",
    "        # print(k_numpy.shape)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "        axes[0].plot(k_numpy[:, 0].flatten())\n",
    "        axes[0].set_title('key 0')\n",
    "        axes[1].hist(k_numpy[:, 0].flatten(), bins=300)\n",
    "        axes[1].set_title('key 0 distribution')\n",
    "        # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        # plt.show()\n",
    "\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "        k_last = k_numpy[:, -1].flatten()\n",
    "        axes[2].plot(k_last)\n",
    "        axes[2].set_title(f'key vector {T}')\n",
    "        axes[3].hist(k_last, bins=300)\n",
    "        axes[3].set_title(f'key vector {T} distribution')\n",
    "        # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        '''value vectors'''\n",
    "        # v_numpy = v_reshape.detach().cpu().numpy()\n",
    "        # v_numpy = v_numpy.transpose(0, 2, 1, 3)\n",
    "        # bs, T, n_head, head_dim = v_numpy.shape\n",
    "        # v_numpy = v_numpy.reshape(bs, T, -1)\n",
    "\n",
    "        # fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "        # axes[0].plot(v_numpy[:, 0].flatten())\n",
    "        # axes[0].set_title('value 0')\n",
    "        # axes[1].hist(v_numpy[:, 0].flatten(), bins=300)\n",
    "        # axes[1].set_title('value 0 distribution')\n",
    "        # # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        # # plt.show()\n",
    "        \n",
    "        for tidx, x_reshape in enumerate([q_reshape, k_reshape, v_reshape]):\n",
    "            x_numpy = x_reshape.detach().cpu().numpy()\n",
    "            x_numpy = x_numpy.transpose(0, 2, 1, 3)\n",
    "            bs, T, n_head, head_dim = x_numpy.shape\n",
    "            v_norm = np.linalg.norm(x_numpy, axis=-1)\n",
    "            # v_norm = np.std(x_numpy, axis=-1)\n",
    "\n",
    "            fig, axes = plt.subplots(1, n_head, figsize=(16, 3))\n",
    "            for hidx in range(n_head):\n",
    "                axes[hidx].plot(v_norm[0, :, hidx].flatten()) # bs =1 anyway\n",
    "                axes[hidx].set_title(f'norm of {\"QKV\"[tidx]} vectors')\n",
    "            plt.show()\n",
    "            # fig, axes = plt.subplots(1, n_head, figsize=(16, 3))\n",
    "            # for hidx in range(n_head):\n",
    "            #     axes[hidx].hist(v_norm[0, :, hidx].flatten()) # bs =1 anyway\n",
    "            #     axes[hidx].set_title(f'norm of {\"QKV\"[tidx]} vectors')\n",
    "            # plt.show()\n",
    "\n",
    "        mlp_out = mlp_out.detach().cpu().numpy()\n",
    "        mlp_out_norm = np.linalg.norm(mlp_out, axis=-1)\n",
    "        # fig = plt.figure(figsize=(16, 3))\n",
    "        # plt.plot(mlp_out_norm.flatten())\n",
    "        # plt.title('norm of mlp output')\n",
    "        # plt.show()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 3))\n",
    "        axes[0].plot(mlp_out_norm.flatten())\n",
    "        axes[0].set_title('norm of mlp output')\n",
    "        axes[1].plot(mlp_out.flatten()) \n",
    "        axes[1].set_title('mlp output')\n",
    "\n",
    "\n",
    "\n",
    "        from itertools import combinations\n",
    "        head_dim = n_embd//cur_model.config.n_head\n",
    "        idx_comb = [np.arange(head_dim)] + [[i] for i in range(head_dim)]\n",
    "\n",
    "        n_arrays = cur_model.config.n_head\n",
    "        all_records = [[] for _ in range(n_arrays)] \n",
    "        all_maps = [[] for _ in range(n_arrays)] \n",
    "        all_idxsets = [[] for _ in range(n_arrays)] \n",
    "        original_score = []\n",
    "        head2 = []\n",
    "        s_head2 = []\n",
    "        sim_head2 = []\n",
    "        y_out_head2 = []\n",
    "\n",
    "        run_search = True\n",
    "        # for cur_idxset in tqdm(idx_comb):\n",
    "        for cur_idxset in idx_comb[:1]:\n",
    "\n",
    "            if not run_search:\n",
    "                break\n",
    "            if not find_best:\n",
    "                run_search = False\n",
    "                cur_idxset = range(head_dim)\n",
    "\n",
    "            if not run_search:\n",
    "                fig, ax = plt.subplots(4, cur_model.config.n_head, figsize=(18, 10))\n",
    "\n",
    "            for hidx in range(cur_model.config.n_head):\n",
    "                \n",
    "                attmap = q_reshape[0, hidx, :, cur_idxset].cuda()@k_reshape[0, hidx, :, cur_idxset].cuda().transpose(0,1)\n",
    "                attmap = attmap / math.sqrt(q_reshape.shape[-1])\n",
    "                mask = np.triu(np.ones_like(attmap.detach().cpu().numpy(), dtype=bool), 1)\n",
    "                # mask = np.triu(np.ones_like(attmap.detach().cpu().numpy(), dtype=bool), )\n",
    "                mask = torch.tensor(mask).to(device)\n",
    "\n",
    "                # digit = attmap.flatten()[0].detach().cpu().clone()\n",
    "                attmap[mask] = -np.inf \n",
    "                # T = attmap.shape[-1]\n",
    "                # attmap[np.arange(T), np.arange(T)] = digit\n",
    "\n",
    "                attmap_np = attmap.detach().cpu().numpy()\n",
    "                attmap_tend = get_PE_tendency(attmap_np)\n",
    "\n",
    " \n",
    "                \n",
    "                s_attmap = torch.nn.Softmax(dim=-1)(attmap)\n",
    "                # calc = torch.ones_like(calc) * calc.mean()\n",
    "                s_attmap_np = s_attmap.detach().cpu().numpy()\n",
    "                tendency_map = generate_tendency_map(s_attmap_np)\n",
    "                mat_tend = get_PE_tendency(s_attmap_np)\n",
    "\n",
    "                y_out = s_attmap.cpu() @ v_reshape[0, hidx, :, cur_idxset].cpu()\n",
    "                # print(y_out.shape)\n",
    "                y_out = activation[f\"layer_{level}_iden\"].detach().cpu().numpy()\n",
    "                B, T, hd = y_out.shape\n",
    "                y_out = y_out.reshape(B, T, cur_model.config.n_head, -1)\n",
    "                y_out = y_out[..., hidx, :]\n",
    "\n",
    "                # print(y_out.shape)\n",
    "                y_out = y_out.sum(axis=0)\n",
    "                # print(y_out.shape)\n",
    "\n",
    "                sim_map = cosine_similarity(y_out, y_out)\n",
    "\n",
    "\n",
    "                if hidx == amp_head or (search_all_heads and find_best):\n",
    "                    if find_best:\n",
    "                        adj_score = literal_eval(attmap_tend)[metric_id]\n",
    "\n",
    "                        \n",
    "                        if len(cur_idxset) == head_dim:\n",
    "                            original_score.append(adj_score)\n",
    "                            head2.append(attmap_np)\n",
    "                            s_head2.append(s_attmap_np)\n",
    "                            sim_head2.append(sim_map)\n",
    "                            y_out_head2.append(y_out)\n",
    "                        else:\n",
    "                            all_records[hidx].append(adj_score)\n",
    "                            all_maps[hidx].append(attmap_np)\n",
    "                            all_idxsets[hidx].append(cur_idxset)\n",
    "                    else:\n",
    "                        head2.append(attmap_np) \n",
    "                        s_head2.append(s_attmap_np)\n",
    "                        sim_head2.append(sim_map)\n",
    "                        y_out_head2.append(y_out)\n",
    "\n",
    "\n",
    "\n",
    "                if not run_search: \n",
    "                    mat = ax[0, hidx].imshow(attmap_np, cmap='Reds', interpolation='nearest')\n",
    "                    ax[0, hidx].set_title(f'head {hidx}={attmap_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[0, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[1, hidx].imshow(s_attmap_np, cmap='Reds', interpolation='nearest')\n",
    "                    ax[1, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[1, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[2, hidx].imshow(tendency_map, cmap='Reds', interpolation='nearest')\n",
    "                    ax[2, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[2, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[3, hidx].imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "                    sim_tend = get_PE_tendency(sim_map)\n",
    "\n",
    "                    ax[3, hidx].set_title(f'head {hidx}={sim_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[3, hidx], orientation='vertical', fraction=0.06,)\n",
    "\n",
    "            if not run_search: \n",
    "                plt.subplots_adjust(hspace=.4)\n",
    "                fig.suptitle(f'{input_name}_{imgname}_layer={level+1}', fontsize=16, y=0.96)\n",
    "                if save_map:\n",
    "                    os.makedirs(f'./saved_heads_plots/', exist_ok=True)\n",
    "                    fig.savefig(f'./saved_heads_plots/{input_name}_{imgname}_layer={level+1}.svg')\n",
    "                plt.show()\n",
    "\n",
    "        if find_best:\n",
    "            n_plots = 9\n",
    "            fig, ax = plt.subplots(n_plots, len(head2), figsize=(3.5*len(head2), n_plots*27/8))\n",
    "            for amidx, attnmap_np in enumerate(head2):\n",
    "                ax_loc_0 = ax[0, amidx] if len(head2)>1 else ax[0]\n",
    "                ax_loc_1 = ax[1, amidx] if len(head2)>1 else ax[1]\n",
    "                ax_loc_2 = ax[2, amidx] if len(head2)>1 else ax[2]\n",
    "                ax_loc_3 = ax[3, amidx] if len(head2)>1 else ax[3]\n",
    "                ax_loc_4 = ax[4, amidx] if len(head2)>1 else ax[4]\n",
    "                ax_loc_5 = ax[5, amidx] if len(head2)>1 else ax[5]\n",
    "                ax_loc_6 = ax[6, amidx] if len(head2)>1 else ax[6]\n",
    "                ax_loc_7 = ax[7, amidx] if len(head2)>1 else ax[7]\n",
    "                ax_loc_8 = ax[8, amidx] if len(head2)>1 else ax[8]\n",
    "\n",
    "\n",
    "                mat = ax_loc_0.imshow(attnmap_np, cmap='Reds', interpolation='nearest')\n",
    "                head2_PE_scores = get_PE_tendency(attnmap_np)\n",
    "                ax_loc_0.set_title(f'head {amidx}={head2_PE_scores}')\n",
    "                plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                tendmap_head2 = generate_tendency_map(attnmap_np)\n",
    "                mat = ax_loc_1.imshow(tendmap_head2, cmap='Reds', interpolation='nearest')\n",
    "                ax_loc_1.set_title(f'head {amidx}={original_score[amidx]}')\n",
    "                plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "                next_att_scores.append(original_score[amidx])\n",
    "\n",
    "\n",
    "                lower_tril_attnmap_np = attnmap_np[np.tril_indices_from(attnmap_np)]\n",
    "                std_val = lower_tril_attnmap_np.std()\n",
    "                std_val_last_row = attnmap_np[-1].std()\n",
    "                print(attnmap_np.shape)\n",
    "\n",
    "                total_std.append(std_val)\n",
    "\n",
    "                n_zeros = (lower_tril_attnmap_np==0).sum()\n",
    "                ax_loc_2.hist(lower_tril_attnmap_np.flatten(), bins=100)\n",
    "                ax_loc_2.set_title(f'h-{amp_head} A_mat distr ({std_val:.3f} vs {std_val_last_row:.3f})')\n",
    "                # ax_loc_2.set_xlim(-0.01, 0.01)\n",
    "                \n",
    "                ax_loc_3.plot(attnmap_np[-1])\n",
    "                ax_loc_3.set_title(f'head {amidx} last row')\n",
    "\n",
    "                s_attmap_np = s_head2[amidx]\n",
    "                                    \n",
    "                lower_tril_s_attnmap_np = s_attmap_np[np.tril_indices_from(s_attmap_np)]\n",
    "                std_val_sa = lower_tril_s_attnmap_np.std()\n",
    "\n",
    "                n_zeros = (lower_tril_s_attnmap_np==0).sum()\n",
    "                ax_loc_4.hist(lower_tril_s_attnmap_np, bins=100)\n",
    "                # zero_map = np.zeros_like(s_attmap_np)\n",
    "                # zero_map[np.where(s_attmap_np==0)] = 1\n",
    "                # cm = ax_loc_4.imshow(zero_map)\n",
    "                # plt.colorbar(cm, ax=ax_loc_4, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "\n",
    "                ax_loc_4.set_title(f'head {amidx} SA_mat distribution ({std_val_sa:.3f})')\n",
    "\n",
    "                \n",
    "                \n",
    "                ax_loc_5.plot(s_attmap_np[-1])\n",
    "                ax_loc_5.set_title(f'head {amidx} last row')\n",
    "\n",
    "\n",
    "                ax_loc_6.hist(s_attmap_np[-1], bins=32)\n",
    "                ax_loc_6.set_title(f'head {amidx} last row dist')\n",
    "\n",
    "\n",
    "                sim_map = sim_head2[amidx]\n",
    "                sim_map_tend = literal_eval(get_PE_tendency(sim_map))[4]\n",
    "                mat = ax_loc_7.imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "                ax_loc_7.set_title(f'cosine_similarity ({sim_map_tend})')\n",
    "                \n",
    "                total_scores.append(sim_map_tend)\n",
    "\n",
    "                plt.colorbar(mat, ax=ax_loc_7, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                y_out = y_out_head2[amidx]\n",
    "                embedding_norm = np.linalg.norm(y_out, axis=-1)\n",
    "                # embedding_norm = np.std(y_out, axis=-1)\n",
    "\n",
    "                ax_loc_8.plot(embedding_norm)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "            # fig, ax = plt.subplots(2, len(head2), figsize=(3.5*len(head2), 6))\n",
    "\n",
    "            # for hridx, head_rec in enumerate(all_records):\n",
    "\n",
    "            #     if len(head_rec) == 0: \n",
    "            #         continue\n",
    "\n",
    "            #     topk_records_idx = np.argsort(head_rec)[-bestk:]\n",
    "\n",
    "            #     best_maps = np.array([all_maps[hridx][i] for i in topk_records_idx])\n",
    "            #     best_map = best_maps.mean(axis=0)\n",
    "            #     best_score = np.mean([head_rec[i] for i in topk_records_idx])\n",
    "            #     best_idxcomb = [all_idxsets[hridx][i] for i in topk_records_idx]\n",
    "\n",
    "\n",
    "            #     ax_loc_0 = ax[0, hridx] if len(head2)>1 else ax[0]\n",
    "            #     ax_loc_1 = ax[1, hridx] if len(head2)>1 else ax[1]\n",
    "\n",
    "            #     mat = ax_loc_0.imshow(best_map, cmap='Reds', interpolation='nearest')\n",
    "            #     # best_PE_scores = get_PE_tendency(best_map)\n",
    "            #     ax_loc_0.set_title(f'best avg={best_score:.02f}')\n",
    "            #     plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "            #     tendmap_best = generate_tendency_map(best_map)\n",
    "            #     mat = ax_loc_1.imshow(tendmap_best, cmap='Reds', interpolation='nearest')\n",
    "            #     ax_loc_1.set_title(f'best ={best_idxcomb}')\n",
    "            #     plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "            \n",
    "            plot_name = f'bestk={bestk}_{input_name}_{imgname}_layer={level+1}'\n",
    "            # fig.suptitle(plot_name, fontsize=16, y=0.96)\n",
    "            if save_map:\n",
    "                os.makedirs(f'./saved_dist_heads_plots/', exist_ok=True)\n",
    "                fig.savefig(f'./saved_dist_heads_plots/{plot_name}.svg')\n",
    "            plt.show()\n",
    "            \n",
    "    print(np.mean(total_std))\n",
    "    if len(total_std) > 1:\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.scatter(total_std, total_scores)\n",
    "        r, p = pearsonr(total_std, total_scores, )\n",
    "        plt.title(f'Pearson Correlation={r:.3f}, p={p:.3f}')\n",
    "        plt.xlabel('attmap_std')\n",
    "        plt.ylabel('cos_adj_score')\n",
    "        plt.show()\n",
    "    \n",
    "    if len(total_scores)>6:\n",
    "\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.scatter(total_scores[:-6], next_att_scores[6:])\n",
    "        r, p = pearsonr(total_scores[:-6], next_att_scores[6:], )\n",
    "        plt.title(f'Pearson Correlation={r:.3f}, p={p:.3f}')\n",
    "        plt.xlabel('cos_adj_score')\n",
    "        plt.ylabel('att_adj_score')\n",
    "        plt.show()\n",
    "\n",
    "widgets.interact(plot_att, \n",
    "                 model_name=useful_name_list, \n",
    "                 layer_idx=(0, 11), \n",
    "                 plot_all=(0, 11), \n",
    "                 amp_head=(0, 5),\n",
    "                 metric_id = (0, 5),\n",
    "                 bestk=(1, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of $y_n$ and the distribution of the dot product $y_n \\cdot y_i$ for a sequence of iid normal vectors $x_0, x_1, \\ldots, x_n$ can be derived through the properties of normal distributions and linear transformations of these distributions.\n",
    "\n",
    "**Distribution of $y_n$**\n",
    "\n",
    "1. **Distribution of Each $x_i$**:\n",
    "   Assume each $x_i$ is a vector of independent and identically distributed (iid) standard normal random variables, say $x_i \\sim N(0, I)$, where $I$ is the identity matrix. This implies that each component of $x_i$ is $N(0,1)$.\n",
    "\n",
    "2. **Sum of Normal Vectors**:\n",
    "   The sum $S_k = x_0 + x_1 + \\ldots + x_k$ of iid normal vectors is also normally distributed. The mean of $S_k$ will be the sum of the means of each $x_i$, which is $0$, and the covariance matrix will be $(k+1)I$ because the covariance matrix of each $x_i$ is $I$.\n",
    "\n",
    "3. **Mean of Normal Vectors**:\n",
    "   $y_n = \\frac{1}{n+1} S_n = \\frac{1}{n+1}(x_0 + x_1 + \\ldots + x_n)$ is also normally distributed as a linear transformation of normal vectors. The mean of $y_n$ remains $0$, and the covariance matrix is scaled by $\\frac{1}{(n+1)^2}$ of $S_n$'s covariance, resulting in $\\frac{1}{n+1} I$.\n",
    "\n",
    "   Hence, $y_n \\sim N(0, \\frac{1}{n+1} I)$.\n",
    "\n",
    "**Distribution of $y_n \\cdot y_i$**\n",
    "\n",
    "1. **Vectors Involved**:\n",
    "   We know $y_i = \\frac{1}{i+1} S_i$ for $i \\leq n$. Thus, $y_i$ and $y_n$ are both linear transformations of sums of the normal vectors $x_0, \\ldots, x_i$ and $x_0, \\ldots, x_n$ respectively.\n",
    "\n",
    "2. **Dot Product**:\n",
    "   The dot product $y_n \\cdot y_i$ is a bilinear form of the Gaussian vectors. This product $y_n \\cdot y_i$ is a scalar random variable.\n",
    "\n",
    "   The expected value of $y_n \\cdot y_i$ can be computed considering $E[y_n \\cdot y_i] = \\frac{1}{(n+1)(i+1)} E[S_n \\cdot S_i]$. Since $S_i$ is a part of $S_n$ when $i \\leq n$, we have $E[S_n \\cdot S_i] = (i+1)$, hence $E[y_n \\cdot y_i] = \\frac{i+1}{(n+1)(i+1)} = \\frac{1}{n+1}$.\n",
    "\n",
    "3. **Distribution**:\n",
    "   Being a linear combination of Gaussian variables, $y_n \\cdot y_i$ is itself Gaussian. Its variance needs further calculation, usually involving expanding $(y_n \\cdot y_i)^2$ and using independence and variances of components of $y_n$ and $y_i$.\n",
    "\n",
    "In summary, $y_n$ is normally distributed with mean zero and covariance matrix $\\frac{1}{n+1} I$, and $y_n \\cdot y_i$ is a normally distributed scalar with mean $\\frac{1}{n+1}$ and a variance that needs further calculation involving their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae766867f0b4ec4b78fb647f8e0d35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfMerger \n",
    "import cairosvg\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def convert_svg_to_pdf(svg_files, output_pdf):\n",
    "    temp_pdfs = []\n",
    "\n",
    "    # Convert each SVG to a temporary PDF\n",
    "    for svg_file in svg_files:\n",
    "        pdf_file = f\"{svg_file}.pdf\"\n",
    "        cairosvg.svg2pdf(url=svg_file, write_to=pdf_file)\n",
    "        temp_pdfs.append(pdf_file)\n",
    "\n",
    "    # Merge all temporary PDFs into a single PDF\n",
    "    merger = PdfMerger ()\n",
    "    for pdf in temp_pdfs:\n",
    "        merger.append(pdf)\n",
    "\n",
    "    # Write out the final merged PDF\n",
    "    merger.write(output_pdf)\n",
    "    merger.close()\n",
    "\n",
    "    # Clean up temporary PDF files\n",
    "    for pdf in temp_pdfs:\n",
    "        os.remove(pdf)\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "root_dir = './saved_dist_heads_plots'\n",
    "svg_files = sorted(glob.glob(root_dir + '/*.svg'))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "f_dict = {}\n",
    "for fpath in svg_files:\n",
    "    fname = fpath.split('/')[-1]\n",
    "    model_name = '_'.join(fname.split('_')[:-1])\n",
    "    if model_name not in f_dict:\n",
    "        f_dict[model_name] = []\n",
    "    f_dict[model_name].append(fpath)\n",
    "\n",
    "for model_name in tqdm(f_dict):\n",
    "    sorted_files = sorted(f_dict[model_name])\n",
    "    os.makedirs(f'{root_dir}_pdf', exist_ok=True)\n",
    "    output_pdf = f'{root_dir}_pdf/{model_name}.pdf'\n",
    "    convert_svg_to_pdf(sorted_files, output_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate matrix for different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cac3e7d617a4380b27d5cb338bff281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='seq_len', max=4096, min=10), FloatSlider(value=0.0, desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_attention(seq_len=20, mu_v=0, sig_v=1, mu_A=0, sig_A=1, shift=0, dimension=100, sim_func='cos', dist_type='normal', v_n=0)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# def causal_softmax(x):\n",
    "#     \"\"\"Compute causal softmax values for the vector 'x'.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "#     return e_x / e_x.sum()\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 28})\n",
    "plt.rcParams['axes.labelsize'] = 28  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 28  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 28  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 28  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 28  # Title\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "def plot_attention(\n",
    "    seq_len = 20,   # Number of vectors\n",
    "    mu_v = 0,\n",
    "    sig_v = 1,\n",
    "    mu_A = 0,\n",
    "    sig_A = 1,\n",
    "    shift = 0,\n",
    "    dimension = 100,  # Dimensionality of each vector\n",
    "    sim_func = 'cos',\n",
    "    dist_type = 'normal',\n",
    "    v_n = 0,\n",
    "):\n",
    "    v_n = min(v_n, seq_len-1)\n",
    "    sim_func_name = 'Dot Product' if sim_func == 'dot' else 'Cosine Similarity'\n",
    "    # Parameters\n",
    "\n",
    "\n",
    "    # Initialize e vectors from index 0 to n-1\n",
    "    # v_vectors = np.array([np.random.normal(mu_v, sig_v, dimension) for _ in range(seq_len)])\n",
    "    v_vectors = np.random.normal(mu_v, sig_v, (seq_len, dimension))\n",
    "\n",
    "    # Initialize A coefficients from index 0 to n-1 and apply causal softmax\n",
    "    # A = np.zeros((n, n))\n",
    "    # A = np.random.normal(0, np.sqrt(n), (n, n))\n",
    "\n",
    "\n",
    "    if dist_type == 'normal':\n",
    "        A = np.random.normal(mu_A, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'gemma':\n",
    "        A = np.random.gamma(1, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'uniform':\n",
    "        A = np.random.uniform(mu_A, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'possion':\n",
    "        A = np.random.poisson(mu_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'chi':\n",
    "        A = np.random.chisquare()\n",
    "\n",
    "    # A = np.random.normal(10, 1, (n, n))\n",
    "    # A = np.random.gamma(0, 0.1, (n, n))\n",
    "    # A = np.random.chisquare(0.001, (n, n))\n",
    "    # A = np.random.uniform(0, 0.01, (n, n))\n",
    "\n",
    "    A = A * (1.0 / math.sqrt(v_vectors.shape[-1]))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    _A = np.tril(A, 0)\n",
    "    axes[0].hist(_A[np.where(_A!=0)].flatten(), bins=200)\n",
    "    _A_std_row_wise = _A[np.where(_A!=0)].flatten().std(axis=-1).mean()\n",
    "    axes[0].set_title(f'A distribution before softmax (std_row-wise: {_A_std_row_wise:.3f})')\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        # A[i, :i+1] = softmax(A[i, :i+1]) + 1 # amazing : )\n",
    "        A[i, :i+1] = softmax(A[i, :i+1]) + shift\n",
    "\n",
    "\n",
    "    A = np.tril(A, 0)\n",
    "\n",
    "    non_zero_A = A[np.where(A!=0)].flatten()\n",
    "    len_A = len(non_zero_A)\n",
    "    sorted_non_zero_A = sorted(non_zero_A)\n",
    "    axes[1].hist(non_zero_A, bins=200)\n",
    "    axes[1].set_title(f'A distribution after softmax (Abs_Avg = {np.abs(sorted_non_zero_A)[:len_A//4].mean():.06f})')\n",
    "\n",
    "    axes[2].plot(A[-1])\n",
    "    axes[2].set_title(f'The last row')\n",
    "\n",
    "    plt.show()\n",
    "    # Compute y vectors using matrix multiplication with a lower triangular matrix of A\n",
    "    y_vectors = A @ v_vectors\n",
    "    norms = np.linalg.norm(y_vectors, axis=1)\n",
    "    print(y_vectors.shape, norms.shape)\n",
    "\n",
    "    # plt.figure(figsize=(6, 5))\n",
    "    fig, axes = plt.subplots(1,4, figsize=(28, 5))\n",
    "    axes[0].hist(y_vectors[v_n], bins=200)\n",
    "    axes[0].set_title(f'Output Vector {v_n+1}, norm={np.linalg.norm(norms[v_n]):.3f}')\n",
    "    axes[1].hist(y_vectors[-1], bins=200)\n",
    "    axes[1].set_title(f'Output Vector {len(y_vectors)}, norm={np.linalg.norm(norms[-1]):.3f}')\n",
    "\n",
    "\n",
    "    # Compute cosine similarities using vector operations\n",
    "    # cosine_similarities = np.dot(y_vectors, y_vectors.T) / np.outer(norms, norms)\n",
    "    cosine_similarities = np.dot(y_vectors, y_vectors.T) \n",
    "    divisor = 1 if sim_func=='dot' else np.outer(norms, norms)\n",
    "    if sim_func == 'dot':\n",
    "        cosine_similarities = cosine_similarities / divisor\n",
    "    else:\n",
    "        cosine_similarities = cosine_similarity(y_vectors, y_vectors)\n",
    "\n",
    "\n",
    "    axes[2].plot(cosine_similarities[-1])\n",
    "    axes[2].set_title(f'Last Vector {sim_func_name}')\n",
    "    axes[3].plot(np.outer(norms, norms)[-1])\n",
    "    axes[3].set_title(f'Last Norm Divisor')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Extracting upper triangular part of cosine similarities matrix, excluding diagonal\n",
    "    results = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i + 1, seq_len):\n",
    "            results[i, j] = cosine_similarities[i, j]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(5.*8, 4))\n",
    "    v_vectors = np.random.normal(mu_v, sig_v, (seq_len, dimension))\n",
    "\n",
    "    prob = [1, 0.83, 0.72, 0.59, 0.40, 0.25, 0.1, 0.000]\n",
    "\n",
    "\n",
    "    A = np.random.normal(mu_A, 0.5, (seq_len, seq_len))\n",
    "    A = A * (1.0 / math.sqrt(v_vectors.shape[-1]))\n",
    "    for j in range(seq_len):\n",
    "        # A[i, :i+1] = softmax(A[i, :i+1]) + 1 # amazing : )\n",
    "        A[j, :j+1] = softmax(A[j, :j+1]) + shift\n",
    "    A = np.tril(A, 0)\n",
    "    y_vectors = A @ v_vectors\n",
    "    original_cosine_similarities = cosine_similarity(y_vectors, y_vectors)\n",
    "    for i in range(8):\n",
    "        \n",
    "\n",
    "        if i < 7:\n",
    "            cosine_similarities = original_cosine_similarities.copy()\n",
    "            \n",
    "            for r in range(seq_len):\n",
    "                idx_to_shuffle = sorted(np.random.permutation(np.arange(r+1))[:int((1-prob[i]) * (r+1))])\n",
    "                # shuffled_idx = np.random.permutation(idx_to_shuffle)\n",
    "                shuffled_idx = idx_to_shuffle[::-1]\n",
    "                cosine_similarities[r, idx_to_shuffle] = cosine_similarities[r, shuffled_idx]\n",
    "                cosine_similarities[idx_to_shuffle, r] = cosine_similarities[shuffled_idx, r]\n",
    "        # # copy the matrix symmectrically over the diagonal\n",
    "        # for r in range(seq_len):\n",
    "        #     for c in range(r, seq_len):\n",
    "        #         cosine_similarities[r, c] = cosine_similarities[c, r]\n",
    "        else:\n",
    "            cosine_similarities = np.random.rand(seq_len,seq_len)\n",
    "\n",
    "\n",
    "        tend = get_PE_tendency(cosine_similarities, as_list=True)[6]\n",
    "\n",
    "        cm = axes[i].imshow(cosine_similarities, cmap='Reds')\n",
    "        # cm = axes[1].imshow(cosine_similarities, )1````\n",
    "        axes[i].set_title(f\"Score={tend}\")\n",
    "        plt.colorbar(cm, ax=axes[i], orientation='vertical', fraction=0.06, )\n",
    "    # save to pdf\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(left=0.03, right=0.975, top=0.9, bottom=0.1, hspace=0.25)\n",
    "    fig.savefig(f'spectrum.pdf', format='pdf') \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "interact(plot_attention, \n",
    "         seq_len=(10, 4096), \n",
    "         mu_v = (-10, 10, 0.01), \n",
    "         sig_v = (0, 2, 0.0001),\n",
    "         mu_A = (-20, 20, 0.01), \n",
    "         sig_A = (0, 10, 0.0001),  \n",
    "         shift=(-2, 2, 0.001), \n",
    "         dimension=(4, 384),\n",
    "         sim_func = ['cos', 'dot'],\n",
    "         dist_type = ['normal', 'gemma', 'uniform', 'possion'],\n",
    "         v_n = (0, 2048)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1], [1, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = 2\n",
    "\n",
    "idx_to_shuffle = sorted(np.random.permutation(np.arange(r))[:int((1-0.00) * (r+1))])\n",
    "idx_to_shuffle, idx_to_shuffle[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, bidirectional=True, num_buckets=32, max_distance=128, n_heads=2):\n",
    "        super(RelativePositionBias, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.n_heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        \"\"\"\n",
    "        Adapted from Mesh Tensorflow:\n",
    "        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "        Translate relative position to a bucket number for relative attention.\n",
    "        The relative position is defined as memory_position - query_position, i.e.\n",
    "        the distance in tokens from the attending position to the attended-to\n",
    "        position.  If bidirectional=False, then positive relative positions are\n",
    "        invalid.\n",
    "        We use smaller buckets for small absolute relative_position and larger buckets\n",
    "        for larger absolute relative_positions.  All relative positions >=max_distance\n",
    "        map to the same bucket.  All relative positions <=-max_distance map to the\n",
    "        same bucket.  This should allow for more graceful generalization to longer\n",
    "        sequences than the model has been trained on.\n",
    "        Args:\n",
    "            relative_position: an int32 Tensor\n",
    "            bidirectional: a boolean - whether the attention is bidirectional\n",
    "            num_buckets: an integer\n",
    "            max_distance: an integer\n",
    "        Returns:\n",
    "            a Tensor with the same shape as relative_position, containing int32\n",
    "            values in the range [0, num_buckets)\n",
    "        \"\"\"\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            ret += (n < 0).to(torch.long) * num_buckets  # mtf.to_int32(mtf.less(n, 0)) * num_buckets\n",
    "            n = torch.abs(n)\n",
    "        else:\n",
    "            n = torch.max(n, torch.zeros_like(n))\n",
    "        # now n is in the range [0, inf)\n",
    "\n",
    "        # half of the buckets are for exact increments in positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def compute_bias(self, qlen, klen):\n",
    "        \"\"\" Compute binned relative position bias \"\"\"\n",
    "        context_position = torch.arange(qlen, dtype=torch.long,\n",
    "                                        device=self.relative_attention_bias.weight.device)[:, None]\n",
    "        memory_position = torch.arange(klen, dtype=torch.long,\n",
    "                                       device=self.relative_attention_bias.weight.device)[None, :]\n",
    "        relative_position = memory_position - context_position  # shape (qlen, klen)\n",
    "        \"\"\"\n",
    "                   k\n",
    "             0   1   2   3\n",
    "        q   -1   0   1   2\n",
    "            -2  -1   0   1\n",
    "            -3  -2  -1   0\n",
    "        \"\"\"\n",
    "        rp_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (qlen, klen)\n",
    "            bidirectional=self.bidirectional,\n",
    "            num_buckets=self.num_buckets,\n",
    "        )\n",
    "        rp_bucket = rp_bucket.to(self.relative_attention_bias.weight.device)\n",
    "        values = self.relative_attention_bias(rp_bucket)  # shape (qlen, klen, num_heads)\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, qlen, klen)\n",
    "        return values\n",
    "\n",
    "    def forward(self, qlen, klen):\n",
    "        return self.compute_bias(qlen, klen)  # shape (1, num_heads, qlen, klen)\n",
    "\n",
    "\n",
    "rb = RelativePositionBias(False, num_buckets=32, max_distance=20, n_heads=1)\n",
    "print(rb(256,256).shape)\n",
    "plt.imshow(rb(256,256)[0,0].detach())\n",
    "plt.colorbar( orientation='vertical', fraction=0.06, pad=0.04,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, bidirectional=True, num_buckets=32, max_distance=128, n_heads=2):\n",
    "        super(RelativePositionBias, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.n_heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        \"\"\"\n",
    "        Adapted from Mesh Tensorflow:\n",
    "        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "\n",
    "        Translate relative position to a bucket number for relative attention. The relative position is defined as\n",
    "        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n",
    "        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n",
    "        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n",
    "        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n",
    "        This should allow for more graceful generalization to longer sequences than the model has been trained on\n",
    "\n",
    "        Args:\n",
    "            relative_position: an int32 Tensor\n",
    "            bidirectional: a boolean - whether the attention is bidirectional\n",
    "            num_buckets: an integer\n",
    "            max_distance: an integer\n",
    "\n",
    "        Returns:\n",
    "            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n",
    "        \"\"\"\n",
    "        relative_buckets = 0\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n",
    "            relative_position = torch.abs(relative_position)\n",
    "        else:\n",
    "            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))\n",
    "        # now relative_position is in the range [0, inf)\n",
    "\n",
    "        # half of the buckets are for exact increments in positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = relative_position < max_exact\n",
    "\n",
    "        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "        relative_postion_if_large = max_exact + (\n",
    "            torch.log(relative_position.float() / max_exact)\n",
    "            / math.log(max_distance / max_exact)\n",
    "            * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        relative_postion_if_large = torch.min(\n",
    "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
    "        )\n",
    "\n",
    "        relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)\n",
    "        return relative_buckets\n",
    "\n",
    "    def compute_bias(self, query_length, key_length):\n",
    "        \"\"\"Compute binned relative position bias\"\"\"\n",
    "        context_position = torch.arange(\n",
    "            query_length, dtype=torch.long, device=self.relative_attention_bias.weight.device\n",
    "        )[:, None]\n",
    "        memory_position = torch.arange(\n",
    "            key_length, dtype=torch.long, device=self.relative_attention_bias.weight.device\n",
    "        )[None, :]\n",
    "        relative_position = memory_position - context_position  # shape (query_length, key_length)\n",
    "        relative_position_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (query_length, key_length)\n",
    "            bidirectional=self.bidirectional,\n",
    "            num_buckets=self.num_buckets,\n",
    "        )\n",
    "        values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n",
    "        return values\n",
    "\n",
    "    def forward(self, qlen, klen):\n",
    "        return self.compute_bias(qlen, klen)  # shape (1, num_heads, qlen, klen)\n",
    "    \n",
    "rb = RelativePositionBias(False, num_buckets=32, max_distance=20, n_heads=1)\n",
    "print(rb(256,256).shape)\n",
    "plt.imshow(rb(256,256)[0,0].detach())\n",
    "plt.colorbar( orientation='vertical', fraction=0.06, pad=0.04,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "T = 10\n",
    "\n",
    "attn_mask = torch.ones(1, 1, T, T, dtype=torch.bool)\n",
    "# attn_mask = rb(T,T).detach().cpu()\n",
    "temp_mask = torch.ones(1, 1, T, T, dtype=torch.bool).tril(diagonal=0)\n",
    "diag_mask = torch.diag_embed(torch.ones(T, dtype=torch.bool)).unsqueeze(0).unsqueeze(0)\n",
    "plt.imshow(temp_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(diag_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(attn_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# attn_mask = attn_mask.masked_fill(temp_mask == 0, float('-inf'))\n",
    "attn_mask = attn_mask & temp_mask\n",
    "\n",
    "plt.imshow(attn_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "attn_mask = attn_mask | diag_mask\n",
    "\n",
    "plt.imshow(attn_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight distributuion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kqv_weights = model.transformer.h[0].attn.c_attn.weight.detach().cpu().numpy()\n",
    "q_weights = kqv_weights[:kqv_weights.shape[0]//3]\n",
    "k_weights = kqv_weights[kqv_weights.shape[0]//3:2*kqv_weights.shape[0]//3]\n",
    "v_weights = kqv_weights[2*kqv_weights.shape[0]//3:]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(k_weights.flatten(), bins=100)\n",
    "plt.show()\n",
    "plt.hist(v_weights.flatten(), bins=100)\n",
    "plt.show()\n",
    "plt.hist(q_weights.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, dists = GPT_nope.create_equal_distancing_vecotrs(12, 384,)\n",
    "dists[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvecs = vecs@k_weights\n",
    "kdists = kvecs@kvecs.T\n",
    "vvecs = vecs@v_weights\n",
    "vdists = vvecs@vvecs.T\n",
    "qvecs = vecs@q_weights\n",
    "qdists = qvecs@qvecs.T\n",
    "# subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].imshow(kdists, cmap='coolwarm', interpolation='nearest')\n",
    "axs[0].set_title('K')\n",
    "axs[1].imshow(vdists, cmap='coolwarm', interpolation='nearest')\n",
    "axs[1].set_title('V')\n",
    "axs[2].imshow(qdists, cmap='coolwarm', interpolation='nearest')\n",
    "axs[2].set_title('Q')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = kdists.shape[0]\n",
    "bias = torch.tril(torch.ones(T, T)).numpy()\n",
    "kvmap = qvecs@kvecs.T\n",
    "kvmap_causal = kvmap.copy()\n",
    "kvmap_causal[bias==0] = 0\n",
    "\n",
    "plt.imshow(kvmap_causal, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvmap_softmax = kvmap.copy()\n",
    "kvmap_softmax[bias==0] = -np.inf\n",
    "kvmap_softmax = np.exp(kvmap_softmax)\n",
    "kvmap_softmax /= kvmap_softmax.sum(axis=1, keepdims=True)\n",
    "plt.imshow(kvmap_softmax, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "for hidx in range(kvmap_softmax.shape[0]):\n",
    "    for j in range(kvmap_softmax.shape[1]):\n",
    "        plt.text(j, hidx, f'{kvmap_softmax[hidx, j]:.02f}', ha='center', va='center', color='black', fontsize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_out = kvmap_softmax@vvecs\n",
    "self_dot_prod = att_out@att_out.T\n",
    "plt.imshow(self_dot_prod, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THEORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, simplify\n",
    "\n",
    "# Define symbols\n",
    "m, seq_len = symbols('m n')\n",
    "\n",
    "# Given values for dot products\n",
    "v1_v1 = v2_v2 = v3_v3 = seq_len\n",
    "v1_v2 = v1_v3 = v2_v3 = m\n",
    "\n",
    "# z.x\n",
    "zx = (1/4)*v1_v1 + (1/4)*v1_v2 + (1/2)*v1_v3\n",
    "# z.y\n",
    "zy = (1/4)*(1/3)*v1_v1 + (1/4)*(2/3)*v1_v2 + (1/4)*(1/3)*v1_v2 + (1/4)*(2/3)*v2_v2 + (1/2)*(1/3)*v1_v3 + (1/2)*(2/3)*v2_v3\n",
    "# z.zz\n",
    "zz = (1/4)**2*v1_v1 + 2*(1/4)**2*v1_v2 + (1/2)**2*v1_v3 + (1/4)**2*v2_v2 + 2*(1/4)*(1/2)*v2_v3 + (1/2)**2*v3_v3\n",
    "\n",
    "# Simplify expressions\n",
    "zx_simplified = simplify(zx)\n",
    "zy_simplified = simplify(zy)\n",
    "zz_simplified = simplify(zz)\n",
    "\n",
    "zx_simplified, zy_simplified, zz_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import exp, simplify\n",
    "\n",
    "# Define e as the base of the natural logarithm\n",
    "# e = exp(1)\n",
    "e = 2.718281828459045\n",
    "\n",
    "# Calculate a1, a2, b1, b2, b3\n",
    "a1 = e**(1/2) / (e**(1/2) + e**1)\n",
    "a2 = 1 - a1\n",
    "b1 = b2 = e**(1/2) / (e**(1/2) + e**(1/2) + e**1)\n",
    "b3 = 1 - b1 - b2\n",
    "\n",
    "# Define symbols\n",
    "m, seq_len = symbols('m n')\n",
    "\n",
    "# Given values for dot products\n",
    "v1_v1 = v2_v2 = v3_v3 = seq_len\n",
    "v1_v2 = v1_v3 = v2_v3 = m\n",
    "\n",
    "# z.x\n",
    "zx = b1*v1_v1 + b2*v1_v2 + b3*v1_v3\n",
    "# z.y\n",
    "zy = (b1*a1 + b2*a2)*v1_v1 + (b1*a2)*v1_v2 + (b2*a1)*v1_v2 + (b2*a2)*v2_v2 + b3*a1*v1_v3 + b3*a2*v2_v3\n",
    "# z.z\n",
    "zz = (b1**2 + b2**2)*v1_v1 + 2*(b1*b2)*v1_v2 + 2*(b1*b3 + b2*b3)*v1_v3 + b3**2*v3_v3 + (b2**2)*v2_v2 + 2*(b2*b3)*v2_v3\n",
    "\n",
    "# Simplify expressions\n",
    "a1_val, a2_val, b1_val, b2_val, b3_val = [simplify(val) for val in [a1, a2, b1, b2, b3]]\n",
    "zx_simplified = simplify(zx.subs({v1_v1: seq_len, v1_v2: m, v1_v3: m}))\n",
    "zy_simplified = simplify(zy.subs({v1_v1: seq_len, v1_v2: m, v1_v3: m, v2_v2: seq_len, v2_v3: m}))\n",
    "zz_simplified = simplify(zz.subs({v1_v1: seq_len, v1_v2: m, v1_v3: m, v2_v2: seq_len, v2_v3: m, v3_v3: seq_len}))\n",
    "\n",
    "a1_val, a2_val, b1_val, b2_val, b3_val, zx_simplified, zy_simplified, zz_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_value = 1\n",
    "n_value = 2  # Example values where n > m\n",
    "\n",
    "# Evaluate zx, zy, zz with substituted values of m and n\n",
    "zx_value = zx_simplified.subs({m: m_value, seq_len: n_value,})\n",
    "zy_value = zy_simplified.subs({m: m_value, seq_len: n_value,})\n",
    "zz_value = zz_simplified.subs({m: m_value, seq_len: n_value,})\n",
    "\n",
    "zx_value, zy_value, zz_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA visualizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "from IPython.utils import io\n",
    "\n",
    "\n",
    "input_act1_list = []\n",
    "# for config_dir, model_config_fold in exp_list:\n",
    "#   with open(f'{config_dir}/{model_config_fold}/config.yaml') as f:\n",
    "#     config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "#   ckpt = f\"{config_dir}/ckpt_10000_final.pt\"\n",
    "#   model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda')\n",
    "\n",
    "for config_dir, model_config_fold in exp_list:\n",
    "    glob_dir = config_dir.replace('[', '*').replace(']', '*')\n",
    "    yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "    config_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "    with open(yaml_path) as f:\n",
    "        config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    # ckpt = glob.glob(f\"{config_dir}/ckpt_**_acc.pt\", recursive=True)[0]\n",
    "    # ckpt = glob.glob(f'{glob_dir}/ckpt_**_acc.pt')[0]\n",
    "    ckpt = glob.glob(f'{glob_dir}/ckpt_**.pt')[0]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        # model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "        model, gptconfig = load_checkpoint(\n",
    "            ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "        # gptconfig.use_pe = 'sin'\n",
    "        # model = GPT_nope(gptconfig)\n",
    "\n",
    "    cur_input_act1_list = []\n",
    "    # for i in range(0, 3):\n",
    "    # model.transformer.h[0].permute = False\n",
    "    # prompts = [\n",
    "    #   f\"${823}\" + '+' + f\"{8}\"*3 + '=',\n",
    "    #   f\"${238}\" + '+' + f\"{8}\"*3 + '='\n",
    "    # ]\n",
    "    zs = np.zeros(12).astype(np.int64)\n",
    "    n_1s = 3\n",
    "    zs[np.random.permutation(12)[:n_1s]] = 1\n",
    "    str_zs = ''.join(map(str, zs))\n",
    "\n",
    "    prompts = [\n",
    "        # f'\\nparity({str_zs})=',\n",
    "        # '\\nparity(100101010000)='\n",
    "        # '\\nparity(010101000101)='\n",
    "        # 'parity(000010101001)=',\n",
    "        # '    $331+=',\n",
    "        'paridy(110001101010)=',\n",
    "        # '123+456'\n",
    "    ]\n",
    "    # prompts = [\n",
    "    #   f\"${623}\" + '+' + f\"{5}\"*3 + '=',\n",
    "    #   f\"${632}\" + '+' + f\"{5}\"*3 + '='\n",
    "    # ]\n",
    "    for hidx in range(0, 1):  # try 10 batches\n",
    "\n",
    "        # prompt = \"$\" + f\"{i}\"*3 + f\"{i}\"*6\n",
    "        # prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "        # prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "        prompt = prompts[hidx]\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "            return hook\n",
    "        # register forward hooks on the layers of choice\n",
    "        h1 = model.transformer.h[1].register_forward_hook(\n",
    "            getActivation('layer_1'))\n",
    "        h2 = model.transformer.ln_f.register_forward_hook(\n",
    "            getActivation('x_out'))\n",
    "        # out_text = generate_output(model, prompt, max_new_tokens=5)\n",
    "        out_text = generate_output(model, prompt, max_new_tokens=4,)\n",
    "\n",
    "        h1.remove()\n",
    "        h2.remove()\n",
    "        model_name = config_dir.split('/')[-1]\n",
    "        print(model_name)\n",
    "        PCA_analysis(prompt, activation['x_out'][0], out_text, config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paridy(x):\n",
    "    x_trunc = str(x)\n",
    "    start_1 = x_trunc.find('1')\n",
    "    end_1 = x_trunc.rfind('1')\n",
    "    y_trunc = x_trunc[start_1:end_1+1].count('0') % 2\n",
    "    return y_trunc\n",
    "\n",
    "\n",
    "paridy(110001101010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import main_thread\n",
    "import main_utils\n",
    "# start_train = None\n",
    "reverse_ab = False\n",
    "reverse_c = True\n",
    "zero_pad = False\n",
    "algo_reason = False\n",
    "add_space = False\n",
    "config['causal_training'] = True\n",
    "\n",
    "\n",
    "\n",
    "config['start'] = start\n",
    "\n",
    "model, gptconfig = load_checkpoint(\n",
    "                \"./outputs_permute/add3_remove_8_nope_residual_exp/add3_remove_8_sd240_T2405280721_nope_lwpTrue_pmremove00000/ckpt_10000_acc_5000.pt\",\n",
    "                GPTConfig_nope,\n",
    "                GPT_nope,\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                return_config=True,\n",
    "                init=False,\n",
    "                init_additional_config={},\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "main_utils.evaluate_addition_batch(config, model, ctx, encode, decode, verbose=True, num_digit=num_digit, zero_pad=zero_pad,\n",
    "                                   reverse_ab=reverse_ab, reverse_c=reverse_c, algo_reason=algo_reason,\n",
    "                                   binary=binary, data_type=data_type, operator=operator, data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interesting: for a causal model with pe, with or without \"\\n\" makes a 180 degree difference in outcome !!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original_model_pe_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.transformer.wpe.weight.cpu().detach().numpy()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "x = model.transformer.wpe.weight.cpu().detach().numpy()\n",
    "# select sample maybe from test set\n",
    "# but if, different digits seems to be encoded the same way, than it has a patter\n",
    "pca = PCA(n_components=2)\n",
    "new_x = pca.fit_transform(x)\n",
    "new_x = new_x[::16]\n",
    "for text, pt in zip(range(len(new_x)), new_x, ):\n",
    "    plt.scatter(pt[0], pt[1], label=text)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(new_x)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import U\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_measure = lambda u1, u2: np.dot(u1, u2.T)\n",
    "# sim_measure = cosine_similarity\n",
    "\n",
    "\n",
    "def standardize_rows(matrix):\n",
    "    \"\"\"Standardize each row of the matrix.\"\"\"\n",
    "    mean = matrix.mean(axis=1, keepdims=True)\n",
    "    std = matrix.std(axis=1, keepdims=True)\n",
    "    return (matrix - mean) / std\n",
    "\n",
    "\n",
    "def plot_corr_mat(corr_mat, vec_dim, is_corr=True, absval=True):\n",
    "\n",
    "    show_text = False if not is_corr else True\n",
    "\n",
    "    # Create a heatmap of corr_mat\n",
    "    plt.figure(figsize=(6, 5), dpi=120)\n",
    "    # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "    plt.imshow(corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "\n",
    "    extra_text = \"Absolute\" if is_corr else \"\"\n",
    "    plot_type = f\"{extra_text} Correlation Coefficient\" if is_corr else \"Dot Product\"\n",
    "\n",
    "    plt.colorbar(\n",
    "        label=plot_type,\n",
    "        # fraction=0.01,\n",
    "        # pad=0.04,\n",
    "    )\n",
    "    if show_text:\n",
    "        for i in range(0, len(corr_mat), vec_dim):\n",
    "            for j in range(0, len(corr_mat), vec_dim):\n",
    "                plt.text(\n",
    "                    j + vec_dim // 2,\n",
    "                    i + vec_dim // 2,\n",
    "                    f\"{corr_mat[i:i+vec_dim, j:j+vec_dim].sum():.02f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"black\",\n",
    "                )\n",
    "\n",
    "\n",
    "level_corr_mat_accum = []\n",
    "corr_mat = None\n",
    "u1, u2, u1_name, u2_name = None, None, None, None\n",
    "\n",
    "task_name = \"mod_\"\n",
    "folder_name = f\"corr_{task_name}\" if not equal_distancing_exp else f\"dot_{task_name}\"\n",
    "folder_name += \"_trained\" if not model_init else \"_init\"\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "    idx1=(0, len(input_act1_list) - 1), idx2=(0, len(input_act1_list) - 1), level=(0, 5)\n",
    ")\n",
    "def get_corr(\n",
    "    idx1,\n",
    "    level=0,\n",
    "    save=False,\n",
    "    absval=True,\n",
    "    save_all=False,\n",
    "    accumulate_all=False,\n",
    "    is_rand_init=False,\n",
    "    subplot_layers=False,\n",
    "):\n",
    "    global corr_mat\n",
    "    global u1, u2, u1_name, u2_name\n",
    "    rand_state = \"init_\" if is_rand_init else \"\"\n",
    "    if not (save_all or accumulate_all):\n",
    "        if not subplot_layers:\n",
    "            idx2 = idx1\n",
    "            input_act1_list = all_level_input_act_list[level]\n",
    "            u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "\n",
    "            u1 = u1.T\n",
    "            u2 = u2.T\n",
    "            print(exp_list[idx1][0].split(\"sd\")[-1], exp_list[idx2][0].split(\"sd\")[-1])\n",
    "            print(u2.shape)\n",
    "\n",
    "            is_corr = False\n",
    "            if u2.shape[1] != 1:\n",
    "                # Standardize each row of u1 and u2\n",
    "                u1_standardized = standardize_rows(u1)\n",
    "                u2_standardized = standardize_rows(u2)\n",
    "                # u1_standardized = u1\n",
    "                # u2_standardized = u2\n",
    "\n",
    "                # Compute the correlation matrix\n",
    "                corr_mat = np.dot(u1_standardized, u2_standardized.T) / (u1.shape[1])\n",
    "            else:\n",
    "                u1 = u1.reshape(-1, 384)\n",
    "                u2 = u2.reshape(-1, 384)\n",
    "                # corr_mat = np.dot(u1, u2.T)\n",
    "                corr_mat = sim_measure(u1, u2)\n",
    "\n",
    "            if absval:\n",
    "                corr_mat = np.abs(corr_mat)\n",
    "            else:\n",
    "                pass\n",
    "            vec_dim = corr_mat.shape[0] // 8\n",
    "            total_sum = np.abs(corr_mat).sum()\n",
    "            block_sum = 0\n",
    "            for i in range(0, len(corr_mat), vec_dim):\n",
    "                block_sum += np.abs(corr_mat[i : i + vec_dim, i : i + vec_dim]).sum()\n",
    "            ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "            plot_corr_mat(corr_mat, vec_dim, is_corr=is_corr, absval=absval)\n",
    "\n",
    "            nope2 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "            u2_name = \"_\".join(exp_list[idx2][1].split(\"_\")[2:])\n",
    "            u2_name = (\n",
    "                nope2 + u2_name.split(\"_\")[-1] + \"_\" + \"_\".join(u2_name.split(\"_\")[:-1])\n",
    "            )\n",
    "\n",
    "            if save:\n",
    "                os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    f\"./saved_plots_{folder_name}/{task_name+folder_name}_{rand_state}_{u2_name}_layer{level}_{ratio:.03f}_{abs}.svg\"\n",
    "                )\n",
    "\n",
    "            # close img\n",
    "            # plt.close()\n",
    "            plt.show()\n",
    "        else:\n",
    "            fig, axs = plt.subplots(\n",
    "                1, 6, figsize=(36, 5)\n",
    "            )  # 6 subplots in a row, adjust size as needed\n",
    "\n",
    "            global_min, global_max = float(\"inf\"), float(\"-inf\")\n",
    "            corr_mat_list = []\n",
    "            for level in range(6):\n",
    "                idx2 = idx1\n",
    "                input_act1_list = all_level_input_act_list[level]\n",
    "                u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "\n",
    "                u1 = u1.T\n",
    "                u2 = u2.T\n",
    "\n",
    "                if u2.shape[1] != 1:\n",
    "                    u1_standardized = standardize_rows(u1)\n",
    "                    u2_standardized = standardize_rows(u2)\n",
    "                    corr_mat = np.dot(u1_standardized, u2_standardized.T) / (\n",
    "                        u1.shape[1]\n",
    "                    )\n",
    "                else:\n",
    "                    u1 = u1.reshape(-1, 384)\n",
    "                    u2 = u2.reshape(-1, 384)\n",
    "                    # corr_mat = np.dot(u1, u2.T)\n",
    "                    corr_mat = sim_measure(u1, u2)\n",
    "\n",
    "                if absval:\n",
    "                    corr_mat = np.abs(corr_mat)\n",
    "                corr_mat_list.append(corr_mat)\n",
    "\n",
    "                global_min = min(global_min, corr_mat.min())\n",
    "                global_max = max(global_max, corr_mat.max())\n",
    "\n",
    "            for level in range(6):\n",
    "\n",
    "                corr_mat = corr_mat_list[level]\n",
    "\n",
    "                # vec_dim = 384\n",
    "                vec_dim = corr_mat.shape[0] // fixed_length\n",
    "\n",
    "                total_sum = np.abs(corr_mat).sum()\n",
    "                block_sum = 0\n",
    "                for i in range(0, len(corr_mat), vec_dim):\n",
    "                    block_sum += np.abs(\n",
    "                        corr_mat[i : i + vec_dim, i : i + vec_dim]\n",
    "                    ).sum()\n",
    "                ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "                cm = axs[level].imshow(\n",
    "                    corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\"\n",
    "                )\n",
    "                #   , vmin=global_min, vmax=global_max)\n",
    "                axs[level].set_title(f\"Layer {level}\")\n",
    "                plt.colorbar(\n",
    "                    cm,\n",
    "                    ax=axs[level],\n",
    "                    orientation=\"vertical\",\n",
    "                    fraction=0.06,\n",
    "                    pad=0.04,\n",
    "                )\n",
    "\n",
    "            extra_text = \"Absolute \" if absval else \"\"\n",
    "            # fig.colorbar(cbar_ax, ax=axs, orientation='vertical', label=f'{extra_text}Correlation Coefficient')\n",
    "\n",
    "            # Set axis labels and title\n",
    "            # plt.xlabel('U2 Entries')\n",
    "            # plt.ylabel('U1 Entries')\n",
    "\n",
    "            # Show the plot\n",
    "            nope1 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "            u1_name = \"_\".join(exp_list[idx1][1].split(\"_\")[2:])\n",
    "            u1_name = (\n",
    "                nope1 + u1_name.split(\"_\")[-1] + \"_\" + \"_\".join(u1_name.split(\"_\")[:-1])\n",
    "            )\n",
    "            nope2 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "            u2_name = \"_\".join(exp_list[idx2][1].split(\"_\")[2:])\n",
    "            u2_name = (\n",
    "                nope2 + u2_name.split(\"_\")[-1] + \"_\" + \"_\".join(u2_name.split(\"_\")[:-1])\n",
    "            )\n",
    "\n",
    "            extra_self = \"Self\" if u1_name == u2_name else \"\"\n",
    "            # plt.title(f'{extra_self} Correlation Matrix for  {ratio:.2f}')\n",
    "            print(u2_name)\n",
    "            if save:\n",
    "                os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    f\"./saved_plots_{folder_name}/{task_name+folder_name}_{len(all_level_input_act_list)}layers_{rand_state}_{u2_name}_{ratio:.03f}_{abs}.svg\"\n",
    "                )\n",
    "\n",
    "            # close img\n",
    "            # plt.close()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        corr_mat_list = []\n",
    "        for level in range(len(all_level_input_act_list)):\n",
    "            input_act1_list = all_level_input_act_list[level]\n",
    "            global level_corr_mat_accum\n",
    "            for idx1 in tqdm(range(len(input_act1_list))):\n",
    "                idx2 = idx1\n",
    "                u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "                u1 = u1.T\n",
    "                u2 = u2.T\n",
    "\n",
    "                if u2.shape[1] != 1: # calculate correlation coefficient\n",
    "                    u1_standardized = standardize_rows(u1)\n",
    "                    u2_standardized = standardize_rows(u2)\n",
    "                    # u1_standardized = u1\n",
    "                    # u2_standardized = u2\n",
    "\n",
    "                    # Compute the correlation matrix\n",
    "                    corr_mat = np.dot(u1_standardized, u2_standardized.T) / (\n",
    "                        u1.shape[1]\n",
    "                    )\n",
    "                else:\n",
    "                    u1 = u1.reshape(-1, 384)\n",
    "                    u2 = u2.reshape(-1, 384)\n",
    "                    # corr_mat = np.dot(u1, u2.T)\n",
    "                    corr_mat = sim_measure(u1, u2)\n",
    "\n",
    "                if absval:\n",
    "                    corr_mat = np.abs(corr_mat)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if accumulate_all:\n",
    "                    level_corr_mat_accum.append(corr_mat[None, ...])\n",
    "                if not save_all:\n",
    "                    continue\n",
    "\n",
    "                if not subplot_layers:\n",
    "                    vec_dim = corr_mat.shape[0] // 8\n",
    "                    total_sum = np.abs(corr_mat).sum()\n",
    "                    block_sum = 0\n",
    "                    for i in range(0, len(corr_mat), vec_dim):\n",
    "                        block_sum += np.abs(\n",
    "                            corr_mat[i : i + vec_dim, i : i + vec_dim]\n",
    "                        ).sum()\n",
    "                    ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "                    # Assuming you have calculated 'corr_mat' as described in the previous answer\n",
    "\n",
    "                    # Create a heatmap of corr_mat\n",
    "                    plt.figure(figsize=(6, 5), dpi=120)\n",
    "                    # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "                    plt.imshow(corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "\n",
    "                    extra_text = \"Absolute \" if absval else \"\"\n",
    "                    plt.colorbar(label=f\"{extra_text}Correlation Coefficient\")\n",
    "\n",
    "                    # Set axis labels and title\n",
    "                    # plt.xlabel('U2 Entries')\n",
    "                    # plt.ylabel('U1 Entries')\n",
    "\n",
    "                    # Show the plot\n",
    "                    nope1 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "                    u1_name = \"_\".join(exp_list[idx1][1].split(\"_\")[2:])\n",
    "                    u1_name = (\n",
    "                        nope1\n",
    "                        + u1_name.split(\"_\")[-1]\n",
    "                        + \"_\"\n",
    "                        + \"_\".join(u1_name.split(\"_\")[:-1])\n",
    "                    )\n",
    "                    nope2 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "                    u2_name = \"_\".join(exp_list[idx2][1].split(\"_\")[2:])\n",
    "                    u2_name = (\n",
    "                        nope2\n",
    "                        + u2_name.split(\"_\")[-1]\n",
    "                        + \"_\"\n",
    "                        + \"_\".join(u2_name.split(\"_\")[:-1])\n",
    "                    )\n",
    "\n",
    "                    extra_self = \"Self\" if u1_name == u2_name else \"\"\n",
    "                    # plt.title(f'{extra_self} Correlation Matrix for  {ratio:.2f}')\n",
    "                    # print(ratio)\n",
    "                    # print(u1_name, u2_name)\n",
    "                    os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                    plt.savefig(\n",
    "                        f\"./saved_plots_{folder_name}/{task_name+folder_name}_{rand_state}_{u2_name}_layer{level}_{ratio:.03f}_{abs}.svg\"\n",
    "                    )\n",
    "\n",
    "                    # close img\n",
    "                    plt.close()\n",
    "                # plt.show()\n",
    "\n",
    "            if accumulate_all:\n",
    "                level_corr_mat_accum = np.vstack(level_corr_mat_accum)\n",
    "                print(level_corr_mat_accum.shape)\n",
    "                level_corr_mat_accum = level_corr_mat_accum.mean(axis=0)\n",
    "                # Create a heatmap of corr_mat\n",
    "                if not subplot_layers:\n",
    "                    plt.figure(figsize=(6, 5), dpi=120)\n",
    "                    # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "                    plt.imshow(\n",
    "                        level_corr_mat_accum, cmap=\"coolwarm\", interpolation=\"nearest\"\n",
    "                    )\n",
    "\n",
    "                    extra_text = \"Absolute \" if absval else \"\"\n",
    "                    plt.colorbar(label=f\"{extra_text}Correlation Coefficient\")\n",
    "                    plt.show()\n",
    "                corr_mat = level_corr_mat_accum\n",
    "                level_corr_mat_accum = []\n",
    "            corr_mat_list.append(corr_mat)\n",
    "        if subplot_layers:\n",
    "            fig, axs = plt.subplots(\n",
    "                1, 6, figsize=(36, 5)\n",
    "            )  # 6 subplots in a row, adjust size as needed\n",
    "            for level in range(len(all_level_input_act_list)):\n",
    "\n",
    "                corr_mat = corr_mat_list[level]\n",
    "\n",
    "                # vec_dim = 384\n",
    "                vec_dim = corr_mat.shape[0] // 8\n",
    "\n",
    "                total_sum = np.abs(corr_mat).sum()\n",
    "                block_sum = 0\n",
    "                for i in range(0, len(corr_mat), vec_dim):\n",
    "                    block_sum += np.abs(\n",
    "                        corr_mat[i : i + vec_dim, i : i + vec_dim]\n",
    "                    ).sum()\n",
    "                ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "                cm = axs[level].imshow(\n",
    "                    corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\"\n",
    "                )\n",
    "                #   , vmin=global_min, vmax=global_max)\n",
    "                axs[level].set_title(f\"Layer {level}\")\n",
    "                plt.colorbar(\n",
    "                    cm,\n",
    "                    ax=axs[level],\n",
    "                    orientation=\"vertical\",\n",
    "                    fraction=0.06,\n",
    "                    pad=0.04,\n",
    "                )\n",
    "\n",
    "            print(u2_name)\n",
    "            if save:\n",
    "                os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    f\"./saved_plots_{folder_name}/{task_name+folder_name}_avg_{abs}.svg\"\n",
    "                )\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "fixed_length = 9\n",
    "all_level_input_act_list = []\n",
    "sample_num = 1024\n",
    "\n",
    "# model_list = []\n",
    "# for idx, (config_dir, model_config_fold) in enumerate(exp_list):\n",
    "#     glob_dir = config_dir.replace('[', '[[]')\n",
    "#     yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "#     revised_glob_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "#     exp_list[idx][0] = revised_glob_dir\n",
    "#     exp_list[idx][1] = revised_glob_dir.split('/')[-1]\n",
    "\n",
    "#     with open(yaml_path) as f:\n",
    "#         config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "#     ckpt = f\"{revised_glob_dir}/ckpt_10000_acc.pt\"\n",
    "#     model, gptconfig = load_checkpoint(\n",
    "#         ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "\n",
    "#     # gptconfig.not_causal = [1]*6\n",
    "#     # model = GPT_nope(GPTConfig_nope())\n",
    "#     # model = GPT_nope(gptconfig)\n",
    "\n",
    "#     # for i in range(len(model.transformer.h)):\n",
    "#     #   model.transformer.h[i].attn._reset_parameters()\n",
    "#     #   model.transformer.h[i].mlp._reset_parameters()\n",
    "#     # model.apply(model._init_weights)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "#     model_list.append(model)\n",
    "\n",
    "\n",
    "for level in range(0, 8):\n",
    "    level = level - 1\n",
    "    input_act1_list = [list() for _ in range(len(model_list))]\n",
    "\n",
    "    for hidx in range(0, 1):  # try 5 batches\n",
    "        X, Y = get_batch('train')\n",
    "        X = decode(X[0].tolist())\n",
    "        X_8 = list(map(lambda x: x[:fixed_length], filter(lambda x: len(\n",
    "            x) >= fixed_length and x[fixed_length-1] == '=', X.split('\\n'))))\n",
    "        X_8 = [''.join(list(np.array(list(x))[np.random.permutation(len(x))]))\n",
    "               for x in X_8]\n",
    "        X = torch.tensor(list(map(lambda x: encode(x), X_8)))\n",
    "        # X = X.reshape(-1, fixed_length)\n",
    "        # X = X[:sample_num]\n",
    "        X = X.to(device)\n",
    "\n",
    "        for midx, model in enumerate(model_list):\n",
    "            activation = {}\n",
    "\n",
    "            def getActivation(name):\n",
    "                # the hook signature\n",
    "                def hook(model, input, output):\n",
    "                    activation[name] = output.detach()\n",
    "                return hook\n",
    "            # register forward hooks on the layers of choice\n",
    "\n",
    "            if level == 6:\n",
    "                h1 = model.transformer.ln_f.register_forward_hook(\n",
    "                    getActivation(f'layer_{level}'))\n",
    "            elif level == -1:\n",
    "                h1 = model.transformer.wte.register_forward_hook(\n",
    "                    getActivation(f'layer_{level}'))\n",
    "            else:\n",
    "                h1 = model.transformer.h[level].ln_1.register_forward_hook(\n",
    "                    getActivation(f'layer_{level}'))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _ = model(X)\n",
    "\n",
    "            h1.remove()\n",
    "\n",
    "            acts = activation[f'layer_{level}'].detach().cpu().numpy()\n",
    "            input_act1_list[midx].append(acts)\n",
    "            # outs = activation['x_out'].detach().cpu().numpy()\n",
    "            # input_act1_list[midx].append(outs)\n",
    "\n",
    "    for hidx in range(len(input_act1_list)):\n",
    "        print(len(input_act1_list[hidx]))\n",
    "        cur_input_act1 = np.concatenate(input_act1_list[hidx], axis=0)\n",
    "        bs, l, dim = cur_input_act1.shape\n",
    "        targets = np.zeros((bs, l)) + np.arange(l)[None, ...]\n",
    "        print(cur_input_act1.shape)\n",
    "        input_act1_list[hidx] = (cur_input_act1, targets)\n",
    "\n",
    "    all_level_input_act_list.append(input_act1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "len(all_level_input_act_list[0][1][1])\n",
    "\n",
    "# do a scklearn linear regression\n",
    "\n",
    "\n",
    "def crossing(X, y):\n",
    "    X_cross = []\n",
    "    y_cross = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            if i != j:\n",
    "                X_cross.append(X[i] * X[j])\n",
    "                y_cross.append(np.abs(y[i] - y[j]))\n",
    "    X = np.array(X_cross)\n",
    "    y = np.array(y_cross)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "@widgets.interact(layer=(-1, 6), model_idx=(0, len(all_level_input_act_list[0])-1))\n",
    "def probe_layer(layer=-1, model_idx=0):\n",
    "    layer = layer + 1\n",
    "    print(exp_list[model_idx][0])\n",
    "    X = all_level_input_act_list[layer][model_idx][0]\n",
    "    y = all_level_input_act_list[layer][model_idx][1]\n",
    "\n",
    "    # X = np.random.rand(*y.shape,10)\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    y = y.reshape(-1)\n",
    "    X_train, X_test = X[:int(len(X)*0.8)], X[int(len(X)*0.8):]\n",
    "    y_train, y_test = y[:int(len(X)*0.8)], y[int(len(X)*0.8):]\n",
    "\n",
    "    # X_train, y_train = crossing(X, y)\n",
    "    # X_test, y_test = crossing(X, y)\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    # X_test = X_train\n",
    "    # y_test = y_train\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(mean_squared_error(y_test, y_pred))\n",
    "    print(reg.score(X_test, y_test))\n",
    "    print(y_test[:10])\n",
    "    print(y_pred[:10])\n",
    "\n",
    "    # print('coef:', reg.coef_)\n",
    "    plt.plot(reg.coef_)\n",
    "    plt.show()\n",
    "\n",
    "    mav = []\n",
    "    for i in range(10):\n",
    "        plt.plot(X_test[i])\n",
    "        mav.append(np.abs(X_test[i]).mean())\n",
    "    plt.show()\n",
    "\n",
    "    print(list(zip(y_test[:10], mav)))\n",
    "\n",
    "    mav = []\n",
    "    for i in range(len(y_test)):\n",
    "        mav.append(np.abs(X_test[i]).mean())\n",
    "\n",
    "    # plt.scatter(y_test, mav, s=20, alpha=0.2)\n",
    "    # plt.show()\n",
    "    print(y_test.shape)\n",
    "\n",
    "    plt.scatter(y_test, y_pred, s=20, alpha=0.2)\n",
    "\n",
    "# normalize the input and do again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Nope and original behaves differently\n",
    "2. When organized and unorganized, the activation output from the trained model is different, meaning that the model somehow also semantically managed the position of numbers and symbos \n",
    "3. For original pe, the regression model must be memorizing the absvalvalolute position initialized randomly at the start of the model. But that didn't explain why NOPE can also get positions right? Then there must be something permanent inside the model that indicates the positions, emm, such as a fixed bias?\n",
    "4. If looking closely at the activations from layers without skip \n",
    "\n",
    "\n",
    "Maybe check why noncausal still doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check random initialization problem\n",
    "\n",
    "* Want to actually check that empirically Qx \\cdot Ky is maximized\n",
    "(usually) when x\\approx y\n",
    "    - could be Wk making W_ky more similar to W_kx\n",
    "    - \\sum a_i b_i (W_Q Z_i) (W_k Z_i) where Z_i can be the pca basis / or any other spectral decomposition\n",
    "\n",
    "... an evidence that residual connection is preserving the locality of r.vec. x\n",
    "\n",
    "... Hypothesis: signs tend to be the same for z1=z2 and different otherwise\n",
    "\n",
    "* Want to compute the rank of\n",
    "    - with\n",
    "        - (1) Transformers with random initialization\n",
    "        - (2) Transformers at convergence\n",
    "\n",
    "    - for\n",
    "       - (a) full residual connections\n",
    "       - (b) no residual connections\n",
    "       - (c) some residual connections\n",
    "\n",
    "check rank degeneration: PCA -> compute the ratio \n",
    "- i.e. a1^2 / sum(ai^2) \n",
    "- plz see how the paper measures the rank degeneration \n",
    "\n",
    "* Want to check what happens when the residual connections we ablate\n",
    "are not consecutive (both non-consecutive layers, and when things\n",
    "taken out are pre-MLP/post-MLP)\n",
    "\n",
    "\n",
    "* Fix the description of the correlation img\n",
    "    - Collect the ratio from the graph and put into a table\n",
    "    - generate the image for all experiments we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- emperically validate qk, if vectors points in the same direction, othen kx dot qy should be high (for trained k,q)\n",
    "    - Plot the correlation on this: QxdotKx vs xdoty \n",
    "- description for correlation matrix\n",
    "\n",
    "or x being actual inputs $x \\in R^d$, project x using K and Q (for a lot of x) would be generally a projection into to the same subspace, namely $Kx \\cdot Qx$ be high\n",
    "    - PCA on K{x}, project on the first 100 components\n",
    "    - project on the first 100 components of Q{x}\n",
    "\n",
    "$K{x} : subspace {Kx| x \\in R^d}$\n",
    "compare projecting K{x} on the principal components of Q{x} to projecting K{x} on the standard basis\n",
    "(n.b., projecting on the first 100 components of the standard basis is just taking the first 100 coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 24\n",
    "plt.plot(X_train[idx])\n",
    "print(y_train[:10])\n",
    "print(reg.predict(X_train[:10][:]))\n",
    "print(X_train[:10].sum(axis=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
