{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "\n",
    "# from model import GPTConfig, GPT\n",
    "from pe_info.model_nope import GPTConfig as GPTConfig_nope, GPT as GPT_nope\n",
    "from main_utils import *\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "resume_dir = None\n",
    "resume_iter = False  # if True, resume from saved iter_num, otherwise resume from iter_num 0\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False  # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_entity = 'ssdd'\n",
    "wandb_log = True  # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2'  # 'run' + str(time.time())\n",
    "exp_name = 'default_exp_name'\n",
    "# data\n",
    "train_data_path = 'train_3digit_10000.txt'\n",
    "gradient_accumulation_steps = 1  # used to simulate larger batch sizes\n",
    "test_batch_size = 128\n",
    "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "val_data_path = 'val.bin'\n",
    "multi_digit = False\n",
    "num_digit = 5\n",
    "binary = False\n",
    "# using two data - data1 = text / data2 = addition\n",
    "# use seperate text/add data for train/val (get_batch uses this to sample from two differernt datasets)\n",
    "train_both = False\n",
    "data_ratio = 0.2  # ratio of data_path2 compared with data_path1\n",
    "train_data_path2 = 'train_addition.bin'  # only used when train_both = True\n",
    "val_data_path2 = 'val_addition.bin'\n",
    "# evaluation\n",
    "eval_text = False  # if True get perplexity using eval_text_data_path\n",
    "# directory to text data (.bin file) - ex. 'data/shakespeare_add_ar_mixed/val_text.bin'\n",
    "eval_text_data_path = None\n",
    "eval_addition = False  # if True compute test accuracy of \"a+b=\"\n",
    "start = \"FILE:data/bal/test_10000.txt\"\n",
    "eval_addition_ar = False\n",
    "start_ar = None\n",
    "# use this to evaluate other operations (ex. train on operator '-' but evaluate on other_operator '+')\n",
    "eval_other = False\n",
    "start_other = None\n",
    "other_operator = '+'\n",
    "eval_addition_train = False\n",
    "start_train = None\n",
    "reverse_ab = False\n",
    "reverse_c = False\n",
    "zero_pad = False\n",
    "algo_reason = False\n",
    "add_space = False\n",
    "# model\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "ckpt_path_name = 'ckpt.pt'\n",
    "save_final = True\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4  # max learning rate\n",
    "max_iters = 600000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 2000  # how many steps to warm up for\n",
    "lr_decay_iters = 600000  # should be ~= max_iters per Chinchilla\n",
    "min_lr = None  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl'  # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "# 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "try:\n",
    "    dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16'\n",
    "except:\n",
    "    dtype = 'float16'\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "use_flash = True\n",
    "data_type = 'text'\n",
    "dataset = 'bal'\n",
    "operator = '+'  # can be '+', '-', '*', 'sin', 'sqrt'\n",
    "data_shuffle = True\n",
    "# data_format = 'algo_reasoning' # 'plain' or 'reverse' or 'algo_reasoning'\n",
    "data_format = 'plain'  # 'plain' or 'reverse' or 'algo_reasoning'\n",
    "\n",
    "# can be 'all_ascii_chars' or 'numbers_only' or 'custom_input_data'\n",
    "vocabulary = 'all_ascii_chars'\n",
    "meta_path_specified = False  # use saved meta_file (False if data_type='text')\n",
    "eps = 0\n",
    "tokenizer = 'char'  # by default, use char level tokenizer. but for pretrained models, use openai tokenizer eg: 'gpt2'\n",
    "\n",
    "simple = False\n",
    "random_A = False\n",
    "random_C = False\n",
    "\n",
    "use_lora = False  # use lora (from minLoRA)\n",
    "print_interval = 2  # if we're using gpt-2 model, I want to see it prompted on text\n",
    "\n",
    "general_seed = 1337\n",
    "# general_seed = 1227\n",
    "resume_metric_from_best = True\n",
    "use_pe = 'original'\n",
    "use_residual = True\n",
    "no_att_residual = False\n",
    "no_mlp_residual = False\n",
    "layerwise_pe = False\n",
    "permute = False\n",
    "not_causal = False\n",
    "\n",
    "causal_training = True\n",
    "\n",
    "\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith(\n",
    "    '_') and isinstance(v, (int, float, bool, str, type(None)))]\n",
    "# exec(open('./pe_info/config2_pe/modp/jason_train_addition_bal.py').read()) # overrides from command line or config file\n",
    "# exec(open('./pe_info/config2_pe/paridy/jason_train_addition_bal.py').read()) # overrides from command line or config file\n",
    "\n",
    "# overrides from command line or config file\n",
    "exec(open('./pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py').read())\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating meta file for all reasonable characters...\n",
      "all the unique characters: \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "vocab size: 96\n",
      "data has 139,890 tokens\n",
      "data has 14,272 tokens\n"
     ]
    }
   ],
   "source": [
    "# for later use in torch.autocast\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32,\n",
    "           'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "    device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "if data_type == 'binary':\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data = np.memmap(os.path.join(\n",
    "        data_dir, train_data_path), dtype=np.uint16, mode='r')\n",
    "    val_data = np.memmap(os.path.join(\n",
    "        data_dir, val_data_path), dtype=np.uint16, mode='r')\n",
    "    if train_both:\n",
    "        train_data2 = np.memmap(os.path.join(\n",
    "            data_dir, train_data_path2), dtype=np.uint16, mode='r')\n",
    "        val_data2 = np.memmap(os.path.join(\n",
    "            data_dir, val_data_path2), dtype=np.uint16, mode='r')\n",
    "    if eval_text:\n",
    "        if eval_text_data_path is None:\n",
    "            print(\n",
    "                'eval_text_data_path is None!!! No binary file to evaluate perplexity on.')\n",
    "        eval_text_data = np.memmap(\n",
    "            eval_text_data_path, dtype=np.uint16, mode='r')\n",
    "    # test_data_str = None # test_data for addition testing will be handled with \"start\"\n",
    "    meta_path = None\n",
    "else:\n",
    "    # check for data_format\n",
    "    if data_type == 'text':\n",
    "        if ('reverse' in data_format and not reverse_c) or (reverse_c and 'reverse' not in data_format):\n",
    "            raise ValueError(\n",
    "                'reverse_c must be True for data_format == \"reverse\"')\n",
    "        elif (data_format == 'algo_reasoning' and not algo_reason) or (algo_reason and data_format != 'algo_reasoning'):\n",
    "            raise ValueError(\n",
    "                'algo_reason must be True for data_format == \"algo_reasoning\"')\n",
    "    meta_path_specified = False\n",
    "\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data_path = os.path.join(data_dir, train_data_path)\n",
    "    # val_data = os.path.join(data_dir, val_data_path)\n",
    "    train_data_list = get_data_list(\n",
    "        train_data_path, operator=operator)  # a list of (x, y, op)\n",
    "    # get_data_list(val_data, operator='+')\n",
    "    val_data_list = get_data_list(filename=None, operator=operator)\n",
    "    train_data_str = generate_data_str(train_data_list, operator=operator, format=data_format, train=True,\n",
    "                                       shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    val_data_str = generate_data_str(val_data_list, operator=operator, format=data_format, train=True,\n",
    "                                     shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    meta, meta_path, data_encoder, data_decoder = create_meta_file(\n",
    "        vocabulary=vocabulary, input_data_str=train_data_str, tokenizer=tokenizer)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    train_data = data_encoder(train_data_str)\n",
    "    val_data = data_encoder(val_data_str)\n",
    "    if eval_addition_train and start_train is None:\n",
    "        # specify the start_train to be our train data file\n",
    "        start_train = f\"FILE:{train_data_path}\"\n",
    "\n",
    "    if train_both:\n",
    "        # This is for the case where we use two different datasets for training\n",
    "        # we sample from both with a specified ratio - data_ratio\n",
    "        # TODO: let's leave this here for now.\n",
    "        train_data2 = np.memmap(os.path.join(\n",
    "            data_dir, train_data_path2), dtype=np.uint16, mode='r')\n",
    "        val_data2 = np.memmap(os.path.join(\n",
    "            data_dir, val_data_path2), dtype=np.uint16, mode='r')\n",
    "\n",
    "    if eval_text:\n",
    "        # eval_text_data = np.memmap(eval_text_data_path, dtype=np.uint16, mode='r')\n",
    "        text_data_list = get_data_list(eval_text_data_path, operator='text')\n",
    "        text_data_str = generate_data_str(\n",
    "            text_data_list, operator='text', format=data_format, train=False, shuffle=False)\n",
    "        eval_text_data = data_encoder(text_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1 tokens\n",
      "data has 1 tokens\n",
      "data has 1 tokens\n",
      "data has 1 tokens\n"
     ]
    }
   ],
   "source": [
    "from numpy import block\n",
    "\n",
    "\n",
    "space_token = data_encoder(' ')[0]\n",
    "switch_line_token = data_encoder('\\n')[0]\n",
    "equal_token = data_encoder('=')[0]\n",
    "dollar_token = data_encoder('$')[0]\n",
    "# non_causal_fix_length = 15\n",
    "# non_causal_fix_length = 27\n",
    "answer_length = 1\n",
    "# def get_batch(split):\n",
    "#         attn_mask = None\n",
    "#         data = train_data if split == 'train' else val_data\n",
    "#         if train_both:\n",
    "#             data2 = train_data2 if split == 'train' else val_data2\n",
    "#             batch_size2 = int(batch_size*data_ratio)\n",
    "#             ix = torch.randint(len(data) - block_size, (batch_size-batch_size2,))\n",
    "#             ix2 = torch.randint(len(data2) - block_size, (batch_size2,))\n",
    "#         else:\n",
    "#             if causal_training:\n",
    "#                 ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#             else:\n",
    "#                 split_points = np.where(data==(switch_line_token))[0]\n",
    "#                 answer_split_points = np.where(data==(equal_token))[0]\n",
    "#                 answer_length_list = split_points - answer_split_points - 1\n",
    "#                 split_points = split_points + 1 # i should have had this\n",
    "#                 split_points = np.hstack([np.array([0]), split_points.flatten()])\n",
    "\n",
    "#                 sample_length_list = np.diff(split_points)\n",
    "#                 start_points = split_points[:-1]\n",
    "\n",
    "#                 randidx = np.random.permutation(len(start_points))[:batch_size]\n",
    "#                 ix = start_points[randidx]\n",
    "#                 sample_length_list = sample_length_list[randidx]\n",
    "#                 answer_length_list = answer_length_list[randidx]\n",
    "\n",
    "#         if causal_training:\n",
    "#             x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "#             y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "#         else:\n",
    "#             remove_dollar_count = 1 if dollar_token in data else 0\n",
    "#             # cur_answer_length_list = np.random.randint(1+remove_dollar_count, answer_length_list+1) + 1\n",
    "#             x = []\n",
    "#             y = []\n",
    "#             picked_i = []\n",
    "#             picked_len = []\n",
    "#             for i in range(len(ix)):\n",
    "#                 for j in range(1+remove_dollar_count+1, answer_length_list[i]+2):\n",
    "#                     x.append(data[ix[i]:ix[i]+sample_length_list[i]-j].astype(np.int64))\n",
    "#                     y.append(data[ix[i]+sample_length_list[i]-j:ix[i]+1+sample_length_list[i]-j].astype(np.int64))\n",
    "#                     picked_i.append(i)\n",
    "#                     picked_len.append(j)\n",
    "#                     if len(x) == batch_size:\n",
    "#                         break\n",
    "\n",
    "#                 if len(x) == batch_size:\n",
    "#                     break\n",
    "#             # x = [data[ix[i]:ix[i]+sample_length_list[i]-cur_answer_length_list[i]].astype(np.int64) for i in range(len(ix))]\n",
    "#             x_len = [len(x[i]) for i in range(len(x))]\n",
    "#             pad_to_length = max(x_len)\n",
    "#             min_length = min(x_len)\n",
    "#             # only do padding when the length is not equal\n",
    "#             if pad_to_length > min_length:\n",
    "#                 x = np.vstack([np.pad(x[i], (pad_to_length-len(x[i]), 0), mode='constant', constant_values=space_token) for i in range(len(x))])\n",
    "#                 attn_mask = np.ones_like(x)\n",
    "#                 # mask out the paddings\n",
    "#                 attn_mask[x==space_token] = 0\n",
    "#                 attn_mask = attn_mask[..., None]\n",
    "#                 attn_mask = attn_mask @ attn_mask.transpose(0, 2, 1)\n",
    "#                 attn_mask = attn_mask.astype(bool)\n",
    "#                 if (attn_mask==1).all():\n",
    "#                     attn_mask = None\n",
    "#                 else:\n",
    "#                     attn_mask = torch.from_numpy(attn_mask)\n",
    "#             x = torch.from_numpy(x)\n",
    "#             # predict the next digit\n",
    "#             # y = torch.stack([torch.from_numpy((data[ix[i]+sample_length_list[i]-cur_answer_length_list[i]:ix[i]+1+sample_length_list[i]-cur_answer_length_list[i]]).astype(np.int64)) for i in range(len(ix))])\n",
    "#             y = torch.from_numpy(np.array(y))\n",
    "#         if train_both:\n",
    "#             x2 = torch.stack([torch.from_numpy((data2[i:i+block_size]).astype(np.int64)) for i in ix2])\n",
    "#             y2 = torch.stack([torch.from_numpy((data2[i+1:i+1+block_size]).astype(np.int64)) for i in ix2])\n",
    "#             x = torch.cat([x,x2])\n",
    "#             y = torch.cat([y,y2])\n",
    "\n",
    "#         if device_type == 'cuda':\n",
    "#             # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "#             x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "#             if attn_mask is not None:\n",
    "#                 attn_mask = attn_mask.pin_memory().to(device, non_blocking=True)\n",
    "#         else:\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "#             if attn_mask is not None:\n",
    "#                 attn_mask = attn_mask.to(device)\n",
    "\n",
    "#         # attn_mask = None\n",
    "#         return x, y, attn_mask\n",
    "\n",
    "# split= \"train\"\n",
    "\n",
    "# def get_batch(split, autoregressive_training=False):\n",
    "#         attn_mask = None\n",
    "#         w = None\n",
    "#         data = train_data if split == 'train' else val_data\n",
    "#         if train_both:\n",
    "#             data2 = train_data2 if split == 'train' else val_data2\n",
    "#             batch_size2 = int(batch_size*data_ratio)\n",
    "#             ix = torch.randint(len(data) - block_size, (batch_size-batch_size2,))\n",
    "#             ix2 = torch.randint(len(data2) - block_size, (batch_size2,))\n",
    "#         else:\n",
    "#             if causal_training:\n",
    "#                 ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#             else:\n",
    "#                 split_points = np.where(data==(switch_line_token))[0]\n",
    "#                 answer_split_points = np.where(data==(equal_token))[0]\n",
    "#                 answer_length_list = split_points - answer_split_points - 1\n",
    "#                 split_points = split_points + 1 # i should have had this\n",
    "#                 split_points = np.hstack([np.array([0]), split_points.flatten()])\n",
    "\n",
    "#                 sample_length_list = np.diff(split_points)\n",
    "#                 start_points = split_points[:-1]\n",
    "\n",
    "#                 # valid_choices = np.where(start_points>block_size)[0]\n",
    "#                 # start_points = start_points[valid_choices]\n",
    "#                 # sample_length_list = sample_length_list[valid_choices]\n",
    "\n",
    "#                 randidx = np.random.permutation(len(start_points))[:batch_size]\n",
    "#                 ix = start_points[randidx]\n",
    "#                 sample_length_list = sample_length_list[randidx]\n",
    "#                 answer_length_list = answer_length_list[randidx]\n",
    "\n",
    "#         if causal_training:\n",
    "#             x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "#             y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "#         else:\n",
    "#             # remove_dollar_count = 1 if dollar_token in data else 0\n",
    "#             # if not autoregressive_training:\n",
    "#             #     cur_answer_length_list = np.random.randint(1+remove_dollar_count, answer_length_list+1) + 1\n",
    "#             # else:\n",
    "#             #     cur_answer_length_list = answer_length_list + 1\n",
    "#             # x = [data[ix[i]:ix[i]+sample_length_list[i]-cur_answer_length_list[i]].astype(np.int64) for i in range(len(ix))]\n",
    "#             # x_len = [len(x[i]) for i in range(len(x))]\n",
    "#             # pad_to_length = max(x_len)\n",
    "#             # min_length = min(x_len)\n",
    "#             # # only do padding when the length is not equal\n",
    "#             # if pad_to_length > min_length:\n",
    "#             #     x = np.vstack([np.pad(x[i], (pad_to_length-len(x[i]), 0), mode='constant', constant_values=space_token) for i in range(len(x))])\n",
    "#             #     attn_mask = np.ones_like(x)\n",
    "#             #     # mask out the paddings\n",
    "#             #     attn_mask[x==space_token] = 0\n",
    "#             #     attn_mask = attn_mask[..., None]\n",
    "#             #     attn_mask = attn_mask @ attn_mask.transpose(0, 2, 1)\n",
    "#             #     attn_mask = attn_mask.astype(bool)\n",
    "#             #     if (attn_mask==1).all():\n",
    "#             #         attn_mask = None\n",
    "#             #     else:\n",
    "#             #         attn_mask = torch.from_numpy(attn_mask)\n",
    "#             # else:\n",
    "#             #     x = np.vstack(x)\n",
    "#             #     attn_mask = None\n",
    "\n",
    "#             # x = torch.from_numpy(x)\n",
    "#             # # predict the next digit\n",
    "#             # if not autoregressive_training:\n",
    "#             #     y = torch.stack([torch.from_numpy((data[ix[i]+sample_length_list[i]-cur_answer_length_list[i]:ix[i]+1+sample_length_list[i]-cur_answer_length_list[i]]).astype(np.int64)) for i in range(len(ix))])\n",
    "#             # else:\n",
    "#             #     y = [torch.from_numpy((data[ix[i]+sample_length_list[i]-cur_answer_length_list[i]:ix[i]-1+sample_length_list[i]]).astype(np.int64)) for i in range(len(ix))]\n",
    "#             #     max_len_y = max([len(y[i]) for i in range(len(y))])\n",
    "#             #     y = np.vstack([np.pad(y[i], (0, max_len_y-len(y[i])+pad_to_length-x_len[i]), mode='constant', constant_values=space_token) for i in range(len(y))])\n",
    "#             #     y = torch.from_numpy(y)\n",
    "#             #     w = torch.ones_like(y)\n",
    "#             #     w[y==space_token] = 0\n",
    "#             x = torch.stack([torch.from_numpy((data[i:i+block_size//4]).astype(np.int64)) for i in ix])\n",
    "#             y = torch.stack([torch.from_numpy((data[i+block_size//4:i+5+block_size//4]).astype(np.int64)) for i in ix])\n",
    "#             w = torch.ones_like(y)\n",
    "\n",
    "#         if train_both:\n",
    "#             x2 = torch.stack([torch.from_numpy((data2[i:i+block_size]).astype(np.int64)) for i in ix2])\n",
    "#             y2 = torch.stack([torch.from_numpy((data2[i+1:i+1+block_size]).astype(np.int64)) for i in ix2])\n",
    "#             x = torch.cat([x,x2])\n",
    "#             y = torch.cat([y,y2])\n",
    "\n",
    "#         if device_type == 'cuda':\n",
    "#             # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "#             x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "#             if w is not None:\n",
    "#                 w = w.pin_memory().to(device, non_blocking=True)\n",
    "#             if attn_mask is not None:\n",
    "#                 attn_mask = attn_mask.pin_memory().to(device, non_blocking=True)\n",
    "#         else:\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "\n",
    "#             if attn_mask is not None:\n",
    "#                 attn_mask = attn_mask.to(device)\n",
    "\n",
    "#         # attn_mask = None\n",
    "#         return x, y, attn_mask, w\n",
    "\n",
    "\n",
    "def get_batch(split, autoregressive_training=False, batch_size=batch_size):\n",
    "    global causal_training\n",
    "    attn_mask = None\n",
    "    w = None\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    if train_both:\n",
    "        data2 = train_data2 if split == 'train' else val_data2\n",
    "        batch_size2 = int(batch_size*data_ratio)\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size-batch_size2,))\n",
    "        ix2 = torch.randint(len(data2) - block_size, (batch_size2,))\n",
    "    else:\n",
    "        if causal_training:\n",
    "            ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        else:\n",
    "            split_points = np.where(data == (switch_line_token))[0]\n",
    "            answer_split_points = np.where(data == (equal_token))[0]\n",
    "            answer_length_list = split_points - answer_split_points - 1\n",
    "            split_points = split_points + 1  # i should have had this\n",
    "            split_points = np.hstack([np.array([0]), split_points.flatten()])\n",
    "\n",
    "            sample_length_list = np.diff(split_points)\n",
    "            start_points = split_points[:-1]\n",
    "\n",
    "            randidx = np.random.permutation(len(start_points))[:batch_size]\n",
    "            ix = start_points[randidx]\n",
    "            sample_length_list = sample_length_list[randidx]\n",
    "            answer_length_list = answer_length_list[randidx]\n",
    "\n",
    "    if causal_training:\n",
    "        x = torch.stack(\n",
    "            [torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack(\n",
    "            [torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    else:\n",
    "        remove_dollar_count = 1 if dollar_token in data else 0\n",
    "        if not autoregressive_training:\n",
    "            cur_answer_length_list = np.random.randint(\n",
    "                1+remove_dollar_count, answer_length_list+1) + 1\n",
    "        else:\n",
    "            cur_answer_length_list = answer_length_list + 1 + 4\n",
    "        x = [data[ix[i]:ix[i]+sample_length_list[i]-cur_answer_length_list[i]\n",
    "                  ].astype(np.int64) for i in range(len(ix))]\n",
    "        x_len = [len(x[i]) for i in range(len(x))]\n",
    "        pad_to_length = max(x_len)\n",
    "        min_length = min(x_len)\n",
    "        # only do padding when the length is not equal\n",
    "        if pad_to_length > min_length:\n",
    "            x = np.vstack([np.pad(x[i], (pad_to_length-len(x[i]), 0), mode='constant',\n",
    "                          constant_values=space_token) for i in range(len(x))])\n",
    "            attn_mask = np.ones_like(x)\n",
    "            # mask out the paddings\n",
    "            attn_mask[x == space_token] = 0\n",
    "            attn_mask = attn_mask[..., None]\n",
    "            attn_mask = attn_mask @ attn_mask.transpose(0, 2, 1)\n",
    "            attn_mask = attn_mask.astype(bool)\n",
    "            if (attn_mask == 1).all():\n",
    "                attn_mask = None\n",
    "            else:\n",
    "                attn_mask = torch.from_numpy(attn_mask)\n",
    "        else:\n",
    "            x = np.vstack(x)\n",
    "            attn_mask = None\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        # predict the next digit\n",
    "        if not autoregressive_training:\n",
    "            y = torch.stack([torch.from_numpy((data[ix[i]+sample_length_list[i]-cur_answer_length_list[i]:ix[i] +\n",
    "                            1+sample_length_list[i]-cur_answer_length_list[i]]).astype(np.int64)) for i in range(len(ix))])\n",
    "        else:\n",
    "            y = [torch.from_numpy((data[ix[i]+sample_length_list[i]-cur_answer_length_list[i]:ix[i]-1+sample_length_list[i]]).astype(np.int64)) for i in range(len(ix))]\n",
    "            max_len_y = max([len(y[i]) for i in range(len(y))])\n",
    "            y = np.vstack([np.pad(y[i], (0, max_len_y-len(y[i])), mode='constant',\n",
    "                          constant_values=space_token) for i in range(len(y))])\n",
    "            y = torch.from_numpy(y)\n",
    "            w = torch.ones_like(y)\n",
    "            w[y == space_token] = 0\n",
    "\n",
    "    if train_both:\n",
    "        x2 = torch.stack(\n",
    "            [torch.from_numpy((data2[i:i+block_size]).astype(np.int64)) for i in ix2])\n",
    "        y2 = torch.stack(\n",
    "            [torch.from_numpy((data2[i+1:i+1+block_size]).astype(np.int64)) for i in ix2])\n",
    "        x = torch.cat([x, x2])\n",
    "        y = torch.cat([y, y2])\n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(\n",
    "            device, non_blocking=True)\n",
    "        if autoregressive_training:\n",
    "            w = w.pin_memory().to(device, non_blocking=True)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "    # attn_mask = None\n",
    "    return x, y, attn_mask, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 12]) torch.Size([256, 1])\n",
      "torch.Size([256, 12, 12])\n",
      " $815+762=77 5\n",
      " ,$,8,1,5,+,7,6,2,=,7,7,\n",
      "$677+639=613 1\n",
      "$,6,7,7,+,6,3,9,=,6,1,3,\n",
      "   $269+583= 2\n",
      " , , ,$,2,6,9,+,5,8,3,=,\n",
      " $442+437=97 8\n",
      " ,$,4,4,2,+,4,3,7,=,9,7,\n",
      "   $310+409= 9\n",
      " , , ,$,3,1,0,+,4,0,9,=,\n",
      "  $310+995=5 0\n",
      " , ,$,3,1,0,+,9,9,5,=,5,\n",
      "   $152+203= 5\n",
      " , , ,$,1,5,2,+,2,0,3,=,\n",
      "$975+968=349 1\n",
      "$,9,7,5,+,9,6,8,=,3,4,9,\n",
      "    $16+14=0 3\n",
      " , , , ,$,1,6,+,1,4,=,0,\n",
      " $615+345=06 9\n",
      " ,$,6,1,5,+,3,4,5,=,0,6,\n"
     ]
    }
   ],
   "source": [
    "causal_training = False\n",
    "x, y, z, w = get_batch('train', autoregressive_training=False)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "if z is not None:\n",
    "    print(z.shape)\n",
    "\n",
    "for hidx in range(min(10, len(x))):\n",
    "    print(data_decoder(x[hidx].cpu().numpy()), data_decoder(y[hidx].cpu().numpy()))\n",
    "    for xi in x[hidx].cpu().numpy():\n",
    "        print(data_decoder(xi[..., None]), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "Loading meta from meta_all_ascii_chars.pkl...\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "best_perplexity = 1e9  # on text data\n",
    "best_accuracy = -1  # on addition data\n",
    "\n",
    "if meta_path_specified:\n",
    "    # attempt to derive vocab_size from the dataset\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "    else:\n",
    "        meta_path = None\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout)  # start with model_args from command line\n",
    "\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "# determine the vocab size we'll use for from-scratch training\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "# gptconf = GPTConfig(**model_args)\n",
    "# model = GPT(gptconf)\n",
    "\n",
    "encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Flash Attention\n",
      "Block 0: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 1: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 2: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 3: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 4: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "Using Flash Attention\n",
      "Block 5: 1.0 | att_res True | perm 0.0 | mlp_res True | layerwise_pe 0.0 | casual True\n",
      "PE in use: original\n",
      "number of parameters: 10.66M\n",
      "test_run\n",
      "0 \n",
      "1 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(96, 384)\n",
      "    (drop): Dropout(p=0.2, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (pre_att_identity): Identity()\n",
      "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
      "          (identity): Identity()\n",
      "          (iq): Identity()\n",
      "          (ik): Identity()\n",
      "          (iv): Identity()\n",
      "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_identity): Identity()\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "    (wpe): Embedding(256, 384)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=384, out_features=96, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, use_flash=use_flash,\n",
    "                  use_residual=use_residual, use_pe=use_pe,\n",
    "                  no_att_residual=no_att_residual,\n",
    "                  no_mlp_residual=no_mlp_residual,\n",
    "                  layerwise_pe=layerwise_pe,\n",
    "                  permute=permute,\n",
    "                  not_causal=not_causal\n",
    "                  )  # jason's change\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "# if use_pe=='original':\n",
    "#     gptconf = GPTConfig(**model_args)\n",
    "#     model = GPT(gptconf)\n",
    "# elif use_pe == 'nope':\n",
    "gptconf = GPTConfig_nope(**model_args)\n",
    "model = GPT_nope(gptconf)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2170, device='cuda:0')\n",
      "$955+$445+ $77+$532+$493+$212+$382+$298+$996+$536+\n",
      "tensor([[19, 19, 18, 30, 23, 24, 18, 18,  5],\n",
      "        [20, 18, 19, 30, 24, 22, 24,  5,  1],\n",
      "        [21, 25, 17, 30, 24, 22, 22,  5,  1],\n",
      "        [21, 18, 22, 30, 24, 21, 26,  5,  1],\n",
      "        [22, 20, 20, 30, 23, 19, 17, 18,  5],\n",
      "        [25, 21, 24, 30, 26, 22, 17, 18,  5],\n",
      "        [20, 21, 17, 30, 19, 19, 24,  5,  1],\n",
      "        [22, 24, 22, 30, 20, 24, 25,  5,  1],\n",
      "        [20, 24, 19, 30, 25, 23, 20, 18,  5],\n",
      "        [21, 22, 24, 30, 20, 26, 26,  5,  1]], device='cuda:0')\n",
      "221=6711$312=757$ 480=755$ 415=749$ 533=6201$847=9501$340=227$ 575=378$ 372=8631$457=399$ \n",
      "cPgNPNy!x`!`6s'c.C!!]]`@*!x+iYy]`Wb.cPU==x6pt!4!2cc#i!`!!?cG~W!T`^s!TK:8!)`@K)`7x+`P)9TY/~\n",
      "tensor(4.5930, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z, w = get_batch('valid', autoregressive_training=True)\n",
    "print(w.sum())\n",
    "with torch.no_grad():\n",
    "    # model.autoregressive_training(x, y, max_new_tokens=y.shape, attn_mask=z)\n",
    "    model.train()\n",
    "    # outs, loss = model(x, y, attn_mask=z, causal_training=causal_training)\n",
    "    outs, loss = model.autoregressive_training(\n",
    "        x, y, w,  max_new_tokens=y.shape[-1], attn_mask=z)\n",
    "\n",
    "    print(decode(x[:10].flatten().detach().cpu().numpy()))\n",
    "    print(y[:10])\n",
    "    print(decode(y[:10].flatten().detach().cpu().numpy()))\n",
    "    print(decode(outs[:10].argmax(-1).flatten().detach().cpu().numpy()))\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_path, \n",
    "                    model_config, \n",
    "                    model_type, \n",
    "                    device='cuda', \n",
    "                    return_config=False, \n",
    "                    init=False,\n",
    "                    init_additional_config={}):\n",
    "    # load ckpt into model\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    model_args = checkpoint['model_args']\n",
    "    # for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    # model_args[k] = checkpoint_model_args[k]\n",
    "    # for k in checkpoint_model_args:\n",
    "    #         model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    original_gptconf = model_config(**model_args)\n",
    "    gptconf = model_config(**model_args)\n",
    "    if not init:\n",
    "        model = model_type(original_gptconf)\n",
    "        \n",
    "        state_dict = checkpoint['model']\n",
    "        # fix the keys of the state dictionary :(\n",
    "        # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k, v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        # override with keys\n",
    "        for k in init_additional_config:\n",
    "            original_gptconf.__dict__[k] = init_additional_config[k]\n",
    "        model = model_type(original_gptconf)\n",
    "   \n",
    "    if return_config:\n",
    "        return model, gptconf\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def generate_output(model, prompt, max_new_tokens=5, attn_mask=None, top_k=None):\n",
    "    # temperature = 0.8\n",
    "    # top_k = 200\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        model = torch.compile(model)  # requires PyTorch 2.0 (optional)\n",
    "    # run generation\n",
    "\n",
    "    start_ids = encode(prompt)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_samples = 1\n",
    "        for k in range(num_samples):\n",
    "\n",
    "            attn_mask = None\n",
    "\n",
    "            y = model.generate(x, max_new_tokens,\n",
    "                               attn_mask=attn_mask, top_k=top_k)\n",
    "\n",
    "    return decode(y[0].tolist())\n",
    "\n",
    "\n",
    "def PCA_analysis(prompt, embs, out_text, config_dir):\n",
    "    pca = PCA(n_components=2)\n",
    "    new_x = pca.fit_transform(embs.cpu().numpy())\n",
    "    data = []\n",
    "    for i, (text, pt) in enumerate(zip(prompt, new_x)):\n",
    "        trace = go.Scatter(\n",
    "            x=[pt[0]],\n",
    "            y=[pt[1]],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=10),  # Adjust the size of the points\n",
    "            text=[str(i+1)],\n",
    "            textposition='middle center',  # Center the text within the marker\n",
    "            name=text,\n",
    "            textfont=dict(\n",
    "                family='Times New Rotman',  # Specify the font family\n",
    "                size=18,  # Adjust the font size\n",
    "                color='black',  # Adjust the font color\n",
    "            ),\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        xaxis=dict(title='Principal Component 1'),\n",
    "        yaxis=dict(title='Principal Component 2'),\n",
    "        title=f'PCA visualization for {prompt}'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n",
    "#   out_num = out_text.split('=')[-1][:-1]\n",
    "#   eqn = out_text.split('=')[0]\n",
    "#   out_text = eqn+'='+out_num[::-1]+out_text[-1]\n",
    "    print(out_text)\n",
    "    # print(new_x)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    import plotly.io as pio\n",
    "    pio.write_html(fig, f'./{config_dir}/{prompt}.html')\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # to make sure GPU runs are deterministic even if they are slower set this to True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    # warning: this causes the code to vary across runs\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Seeded everything: {}\".format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# config_dir = \"out2/addition_reverse/\"\n",
    "\n",
    "# ckpt = f\"{config_dir}/ckpt_10000_final.pt\"\n",
    "# import yaml\n",
    "# with open(f'{config_dir}/addition_reverse/config.yaml') as f:\n",
    "#   config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# gptconf = GPTConfig_nope(**model_args)\n",
    "# model = GPT_nope(gptconf)\n",
    "# model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope)\n",
    "# model = load_checkpoint(ckpt, GPTConfig, GPT)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import glob\n",
    "import yaml\n",
    "\n",
    "\n",
    "exp_list = []\n",
    "# exp_list\n",
    "# exp_list = glob.glob('./outputs/residual_exp/*') + glob.glob('./outputs/nope_residual_exp/*')\n",
    "# exp_list = glob.glob('./outputs/modp_nope_residual_exp/*')\n",
    "# exp_list = glob.glob('./outputs/paridy_nope_residual_exp/*')\n",
    "\n",
    "# exp_list = glob.glob('./outputs/residual_exp/*')\n",
    "# exp_list = glob.glob('./outputs_permute/add3_nope*/*') \\\n",
    "#     + glob.glob('./outputs_permute/add3_residual*/*') \\\n",
    "#     + glob.glob('./outputs_permute/add3_shuffle_6_*/*') \n",
    "# exp_list = glob.glob('./outputs_permute/add3_shuffle_6_*/*')\n",
    "\n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_16_*/*') \\\n",
    "#     + glob.glob('./outputs_permute/add3_remove_16_lwp_residual*/*') \n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_8_*/*')\n",
    "\n",
    "\n",
    "# ===== removing residual connections\n",
    "# exp_list = glob.glob('./outputs/nope_residual_exp/*')\n",
    "# exp_list = [p for p in exp_list if 'sd222' in p]\n",
    "\n",
    "# exp_list += [p for p in glob.glob('./outputs_permute/add3_remove_8_rotary_rotary/*') if 'lwpTrue' not in p]\n",
    "# exp_list += glob.glob('./outputs_permute/add3_remove_8_T5_T5/*')\n",
    "# exp_list += [p for p in glob.glob('./outputs_permute/add3_remove_8_sin_residual_exp/*') if 'lwpTrue' not in p]\n",
    "# exp_list += glob.glob('./outputs_permute/add3_remove_8_residual_exp/*')\n",
    "\n",
    "\n",
    "# ===== 6-12 layers\n",
    "exp_list = glob.glob('./outputs_permute/add3_ref*/*')\n",
    "\n",
    "# exp_list = glob.glob('./outputs_permute/add3_lwp_residual_exp/*')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_8_sin_sin/*')\n",
    "# exp_list = glob.glob('./outputs_permute/add3_remove_8_*/*')\n",
    "# exp_list = exp_list[len(exp_list)//2:]\n",
    "\n",
    "\n",
    "\n",
    "# exp_list = ['./outputs/nope_residual_exp/addition_reverse_sd111_T2401311659_nope_res=[1, 2, 3, 4, 5]']\n",
    "\n",
    "# exp_list = [p for p in exp_list if '[0' not in p]\n",
    "# exp_list  = [p for p in exp_list if 'res=[0, 1, 2, 3, 4, 5]' in p or 'res=[0, 1, 2]' in p]\n",
    "\n",
    "\n",
    "exp_list = [[x, x.split('/')[-1]] for x in exp_list] \n",
    "print(len(exp_list))\n",
    "# calc ratio\n",
    "# increase contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615c476bec234b879bedc13debf013ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded everything: 61\n",
      "Seeded everything: 99\n",
      "Seeded everything: 129\n",
      "Seeded everything: 95\n",
      "Seeded everything: 150\n",
      "Seeded everything: 186\n",
      "Seeded everything: 173\n",
      "Seeded everything: 28\n",
      "Seeded everything: 1\n",
      "Seeded everything: 37\n",
      "Seeded everything: 143\n",
      "Seeded everything: 78\n",
      "Seeded everything: 197\n",
      "Seeded everything: 17\n",
      "Seeded everything: 111\n",
      "Seeded everything: 84\n",
      "Seeded everything: 170\n",
      "Seeded everything: 154\n",
      "Seeded everything: 73\n",
      "Seeded everything: 150\n",
      "causal_training is True\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "from operator import eq\n",
    "from IPython.utils import io\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# fixed_length = 21\n",
    "fixed_length = 9\n",
    "\n",
    "# fixed_length = 10\n",
    "\n",
    "# total_tokens = 2048\n",
    "total_samples = 256\n",
    "sample_num = 1024\n",
    "all_level_input_act_list = []\n",
    "\n",
    "rand_perm = False\n",
    "if rand_perm:\n",
    "    print(\"permutation added\")\n",
    "equal_distancing_exp = False\n",
    "use_1_sample = False\n",
    "model_init = False\n",
    "if model_init:\n",
    "    print(\"model init\")\n",
    "    init_additional_config = {\n",
    "        'n_head':1,\n",
    "    }\n",
    "    # put additional configs that needs to be initialized here\n",
    "else:\n",
    "    init_additional_config = {}\n",
    "\n",
    "small_dim = False\n",
    "if small_dim:\n",
    "    print(\"small dim\")\n",
    "\n",
    "\n",
    "ablation_config = {\n",
    "    \"V_out\": False,\n",
    "    \"no_K\": False,\n",
    "    \"no_V\": False,\n",
    "    \"no_Q\": False,\n",
    "    \"shrink_x\": False,\n",
    "    # \"c_proj\": True,\n",
    "    # only one at a time\n",
    "    \"func_config\": {\n",
    "        0: None,\n",
    "        1:\"nodiv\", \n",
    "        2:\"softmax\", \n",
    "        3:\"abs\", \n",
    "        4:\"relu\", \n",
    "        5:\"divsum\", \n",
    "        6:\"gelu\",\n",
    "        7:\"gelu_divabs\",\n",
    "        8:\"sigmoid\",\n",
    "        9:\"same\"}[0],\n",
    "\n",
    "\n",
    "}\n",
    "for k in ablation_config:\n",
    "    if ablation_config[k]:\n",
    "        if k == \"func_config\":\n",
    "            print(f\"{k}: {ablation_config[k]}\")\n",
    "        else:\n",
    "            print(f\"{k} is on\")\n",
    "\n",
    "model_list = []\n",
    "useful_name_list = []\n",
    "for idx, (config_dir, model_config_fold) in enumerate(tqdm(exp_list)):\n",
    "    glob_dir = config_dir.replace(\"[\", \"*\").replace(\"]\", \"*\")\n",
    "    try:\n",
    "        yaml_path = glob.glob(f\"{glob_dir}/**/config.yaml\")[0]\n",
    "        csv_path = glob.glob(f\"{glob_dir}/**/result.csv\")[0]\n",
    "        revised_glob_dir = \"/\".join(yaml_path.split(\"/\")[:-2])\n",
    "        exp_list[idx][0] = revised_glob_dir\n",
    "        exp_list[idx][1] = revised_glob_dir.split(\"/\")[-1]\n",
    "\n",
    "        config_dir = \"/\".join(yaml_path.split(\"/\")[:-2])\n",
    "        with open(yaml_path) as f:\n",
    "            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "\n",
    "        # ckpt = f\"{config_dir}/ckpt_10000_acc.pt\"\n",
    "        glob_dir = config_dir.replace(\"[\", \"*\").replace(\"]\", \"*\")\n",
    "        try:\n",
    "            all_ckpts = sorted(glob.glob(f\"{glob_dir}/ckpt_*.pt\"), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        except:\n",
    "            all_ckpts = [gdir for gdir in glob.glob(f\"{glob_dir}/ckpt_*.pt\") if '_acc' in gdir]\n",
    "        # ckpt = f\"{config_dir}/ckpt_2000_acc.pt\"\n",
    "\n",
    "        # add the initialized model\n",
    "        for init_scheme in ['default', '(\"normal\", 0, 1)', ]:\n",
    "            set_seed(np.random.randint(0,200))\n",
    "            ckpt = all_ckpts[0]\n",
    "            with io.capture_output() as captured:\n",
    "                model, gptconfig = load_checkpoint(\n",
    "                    ckpt,\n",
    "                    GPTConfig_nope,\n",
    "                    GPT_nope,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    return_config=True,\n",
    "                    init=True,\n",
    "                    init_additional_config={\n",
    "                        'init_scheme': init_scheme,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            model_list.append(model)\n",
    "            try:\n",
    "                iter_num = int(ckpt.split('_')[-1].split('.')[0])\n",
    "                convergence = 0\n",
    "                useful_name_list.append('acc='+str(int(convergence))+ '_' + '/'.join(ckpt.split('/')[3:]).replace(str(iter_num), '0')+f'_{init_scheme}')\n",
    "            except:\n",
    "                useful_name_list.append('acc=0'+ '_' + '/'.join(ckpt.split('/')[3:])+'_untrained')\n",
    "            \n",
    "        # for ckpt in all_ckpts[len(all_ckpts)//2:len(all_ckpts)//2+1] + all_ckpts[-1:]:\n",
    "        for ckpt in all_ckpts[-1:]:\n",
    "\n",
    "        # for ckpt in all_ckpts:\n",
    "\n",
    "            with io.capture_output() as captured:\n",
    "                # model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda')\n",
    "                # model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "                model, gptconfig = load_checkpoint(\n",
    "                    ckpt,\n",
    "                    GPTConfig_nope,\n",
    "                    GPT_nope,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                    return_config=True,\n",
    "                    init=model_init,\n",
    "                    init_additional_config=init_additional_config,\n",
    "                )\n",
    "                if small_dim:\n",
    "                    gptconfig.n_head = 1\n",
    "                    # gptconfig.not_causal = True\n",
    "                    gptconfig.n_embd = 16\n",
    "                    model = GPT_nope(gptconfig)\n",
    "                                \n",
    "                model.eval()\n",
    "                model.to(device)\n",
    "                model_list.append(model)\n",
    "                try:\n",
    "                    iter_num = int(ckpt.split('_')[-1].split('.')[0])\n",
    "                    convergence = df.loc[df['iter']==iter_num, 'test_acc'].values[0]\n",
    "                    useful_name_list.append('acc='+str(int(convergence))+ '_' + '/'.join(ckpt.split('/')[3:]))\n",
    "                except:\n",
    "                    convergence = df.loc[:, 'test_acc'].values.max()\n",
    "                    useful_name_list.append('acc='+str(int(convergence))+ '_'+ '/'.join(ckpt.split('/')[3:])+'_trained')\n",
    "\n",
    "\n",
    "                # change all parameters to 0.05\n",
    "                # for name, param in model.named_parameters():\n",
    "                #     if 'att' in name:\n",
    "                #         # param.data.fill_(0.5)\n",
    "                #         torch.nn.init.xavier_uniform_(param)\n",
    "                    # else:\n",
    "                        # param.data.fill_(0)\n",
    "\n",
    "    except ValueError:\n",
    "        print(f\"no model {glob_dir}\")\n",
    "        continue\n",
    "\n",
    "n_embd = model_list[0].config.n_embd\n",
    "equation_num = 1\n",
    "causal_training = True # for add3\n",
    "total_tokens = total_samples * 2 * fixed_length * equation_num\n",
    "diviser = 256 if causal_training else fixed_length\n",
    "samples_to_generate = total_tokens//fixed_length + total_tokens%fixed_length\n",
    "fixed_length = equation_num * fixed_length\n",
    "print('causal_training is', causal_training)\n",
    "\n",
    "\n",
    "X = get_batch(\"valid\", batch_size=samples_to_generate)[0]\n",
    "X = \"\\n\".join([decode(X[i].tolist()) for i in range(X.shape[0])])[:total_tokens] # truncate x to lower computation\n",
    "equations = X.split('\\n')\n",
    "X_n = []\n",
    "for eidx in range(0, len(equations), equation_num):\n",
    "    full_eqn = '\\n'.join(equations[eidx:eidx+equation_num])\n",
    "    if causal_training and not full_eqn.startswith('$'): continue\n",
    "    if len(full_eqn)<fixed_length:\n",
    "        if len(full_eqn) < fixed_length//2: continue\n",
    "        full_eqn = '$' * (fixed_length-len(full_eqn))  + full_eqn\n",
    "    full_eqn = full_eqn[:fixed_length]\n",
    "    X_n.append(full_eqn)\n",
    "X_n = X_n[:total_samples]\n",
    "X = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "\n",
    "\n",
    "# X_n = np.array(list(X[:len(X) // fixed_length * fixed_length])).reshape(-1, fixed_length)\n",
    "\n",
    "if rand_perm: # shuffle in input sequence\n",
    "    for xidx in range(X.shape[0]):\n",
    "        X[xidx] = X[xidx, torch.randperm(X[xidx].shape[0])]\n",
    "\n",
    "X = X.to(device)\n",
    "if use_1_sample:\n",
    "    X = X[:1]\n",
    "\n",
    "for level in range(1, 13):\n",
    "    level = level - 1\n",
    "    input_act1_list = [list() for _ in range(len(model_list))]\n",
    "\n",
    "\n",
    "    for midx, model in enumerate(model_list):\n",
    "        if len(model.transformer.h)<=level:\n",
    "            continue\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "    \n",
    "\n",
    "        # register forward hooks on the layers of choice\n",
    "        if level >= 0:\n",
    "            # h1 = model.transformer.h[level].register_forward_hook(\n",
    "            #     getActivation(f\"layer_{level}\")\n",
    "            # )\n",
    "            # if hook_location == 'pre_attn':\n",
    "            #     h1 = model.transformer.h[level].attn.pre_attn_identity.register_forward_hook(\n",
    "            #         getActivation(f\"layer_{level}\")\n",
    "            #     )\n",
    "            # elif hook_location == 'post_attn':\n",
    "            h1 = model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "                getActivation(f\"layer_{level}\")\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # if 'nope' in exp_list[midx][0]:\n",
    "            # h1 = model.transformer.drop.register_forward_hook(\n",
    "            #     getActivation(f\"layer_{level}\")\n",
    "            # )\n",
    "            h1 = model.transformer.drop.register_forward_hook(\n",
    "                getActivation(f\"layer_{level}\")\n",
    "            )\n",
    "            # else:\n",
    "            #     h1 = model.transformer.wte.register_forward_hook(\n",
    "            #         getActivation(f\"layer_{level}\")\n",
    "            #     )\n",
    "\n",
    "\n",
    "        h2 = model.transformer.ln_f.register_forward_hook(\n",
    "            getActivation(\"x_out\")\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model(\n",
    "                X, equal_distancing_exp=equal_distancing_exp, ablation_config=ablation_config,\n",
    "            )\n",
    " \n",
    "        h1.remove()\n",
    "        h2.remove()\n",
    "\n",
    "        acts = activation[f\"layer_{level}\"].detach().cpu().numpy()\n",
    "        \n",
    "        input_act1_list[midx].append(acts)\n",
    "\n",
    "    for hidx in range(len(input_act1_list)):\n",
    "        if len(input_act1_list[hidx]) == 0:\n",
    "            input_act1_list[hidx] \n",
    "        else:\n",
    "            cur_input_act1 = np.concatenate(input_act1_list[hidx], axis=0)\n",
    "            bs, l, dim = cur_input_act1.shape\n",
    "            # print(cur_input_act1.shape)\n",
    "            input_act1_list[hidx] = cur_input_act1.reshape(bs, l, dim)\n",
    "\n",
    "    all_level_input_act_list.append(input_act1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using function cosine_similarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace9900ec6824c3cac85a93d5a508d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='sample_idx', max=255), IntSlider(value=0, description='l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.get_corr(sample_idx=0, level=0, drop_down=[], drop_down2=None, drop_down3=None, plot_type=['dot', 'corr'], individual=True, before_after=['training', False, 'attention'], save=False, abs=True, save_all=False, accumulate_all=False, subplot_layers=True)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from re import T\n",
    "import gc\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size':36})\n",
    "plt.rcParams['axes.labelsize'] =36  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] =36  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] =36  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] =36  # Legend\n",
    "plt.rcParams['axes.titlesize'] =36  # Title\n",
    "\n",
    "\n",
    "sim_func = cosine_similarity\n",
    "# sim_func = lambda x, y: np.dot(x, y.T)\n",
    "# sim_func = lambda x, y: (x[None, ...] - y[:, None, ...]).sum(axis=-1)\n",
    "show_vals = False\n",
    "print(f\"using function {repr(sim_func).split(' ')[1]}\")\n",
    "\n",
    "def standardize_rows(matrix):\n",
    "    \"\"\"Standardize each row of the matrix.\"\"\"\n",
    "    mean = matrix.mean(axis=1, keepdims=True)\n",
    "    std = matrix.std(axis=1, keepdims=True)\n",
    "    return (matrix - mean) / std\n",
    "\n",
    "level_corr_mat_accum = []\n",
    "corr_mat = None\n",
    "u1, u2, u1_name, u2_name = None, None, None, None\n",
    "\n",
    "task_name = 'add_'\n",
    "folder_name = f'corr_{task_name}' if not equal_distancing_exp else f'dot_{task_name}'\n",
    "folder_name += '_trained' if not model_init else '_init'\n",
    "\n",
    "for k in ablation_config:\n",
    "    if ablation_config[k]:\n",
    "        if k == 'func_config':\n",
    "            folder_name += f'_{ablation_config[k]}'\n",
    "        else: \n",
    "            folder_name += f'_{k}' \n",
    "\n",
    "def get_corr(sample_idx=0, \n",
    "             level=0, \n",
    "             drop_down=[], \n",
    "             drop_down2=None, \n",
    "             drop_down3=None, \n",
    "             plot_type = ['dot', 'corr'],\n",
    "             individual = True,\n",
    "             before_after = ['training', False,  'attention'],\n",
    "             save=False,  \n",
    "             abs=True, \n",
    "             save_all=False, \n",
    "             accumulate_all=False, \n",
    "             subplot_layers=True):\n",
    "    global corr_mat, input_act1_list\n",
    "    global u1, u2, u1_name, u2_name\n",
    "    rand_state = 'init_' if model_init else ''\n",
    "\n",
    "    idx1 = useful_name_list.index(drop_down)\n",
    "    idx2 = useful_name_list.index(drop_down2) if drop_down2 is not None else None\n",
    "    idx3 = useful_name_list.index(drop_down3) if drop_down3 is not None else None\n",
    "\n",
    "    model_layers = [l[idx1] for l in all_level_input_act_list if len(l[idx1])!=0]\n",
    "    level = min(level, len(model_layers)-1)\n",
    "    print(level, idx1, useful_name_list[idx1])\n",
    "\n",
    "    for l in all_level_input_act_list:\n",
    "        if l == []:\n",
    "            return\n",
    "\n",
    "    # individual = individual if not save_all else False\n",
    "    if individual :\n",
    "        if not equal_distancing_exp:\n",
    "            print(X_n[sample_idx:sample_idx+1])\n",
    "        else:\n",
    "            print('eqx')\n",
    "    save = True if save_all else save\n",
    "    model_range = range(idx1, idx1+1) if not save_all else range(len(input_act1_list))\n",
    "    for idx1 in tqdm(model_range):\n",
    "        eqn_text = X_n[sample_idx] if not equal_distancing_exp else 'eqx'\n",
    "        \n",
    "        models_corr_mat_list = []\n",
    "        \n",
    "        model_name = useful_name_list[idx1]\n",
    "        acc = model_name.split('_')[0]\n",
    "        rest = '_'.join(model_name.split('_')[1:])\n",
    "        try:\n",
    "            iteration = int(re.search(r'acc_(\\d+).pt', model_name).group(1))\n",
    "        except:\n",
    "            if 'untrained' in model_name:\n",
    "                iteration = 0\n",
    "            else:\n",
    "                iteration = 5000\n",
    "        imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + ' ' + eqn_text\n",
    "\n",
    "        cur_model_act_list1 = [l[idx1] for l in all_level_input_act_list if len(l[idx1])!=0]\n",
    "        models_to_plot = [cur_model_act_list1]\n",
    "        \n",
    "        has_model = True\n",
    "        if before_after=='training':\n",
    "            rest = '_'.join(rest.split('.')[0].split('_')[:-1])\n",
    "            if acc.split('=')[-1]!='0' or iteration!=0:\n",
    "                print(acc, iteration)\n",
    "                has_model = False\n",
    "                continue\n",
    "            trained_model_name = None\n",
    "            for m_idx, m_name in enumerate(useful_name_list):\n",
    "                # if rest in m_name and not (m_name==model_name):\n",
    "                if rest in m_name and m_name.endswith('.pt'):\n",
    "                    trained_model_name = m_name\n",
    "                    # trained_cur_model = model_list[m_idx]\n",
    "                    m_idx = useful_name_list.index(m_name)\n",
    "                    cur_model_act_list2 = [l[m_idx] for l in all_level_input_act_list if len(l[m_idx])!=0]\n",
    "                    break\n",
    "            assert trained_model_name is not None, 'no trained model found'\n",
    "            models_to_plot.append(cur_model_act_list2)\n",
    "\n",
    "        elif before_after=='attention':\n",
    "            models_to_plot.append(cur_model_act_list1)\n",
    "\n",
    "        if drop_down2 is not None:\n",
    "            cur_model_act_list2 = [l[idx2] for l in all_level_input_act_list if len(l[idx2])!=0]\n",
    "            models_to_plot.append(cur_model_act_list2)\n",
    "        if drop_down3 is not None:\n",
    "            cur_model_act_list3 = [l[idx3] for l in all_level_input_act_list if len(l[idx3])!=0]\n",
    "            models_to_plot.append(cur_model_act_list3)\n",
    "        \n",
    "\n",
    "        for cur_model_act_list in models_to_plot:\n",
    "            corr_mat_list = []\n",
    "            \n",
    "            for level in range(len(cur_model_act_list)):\n",
    "                input_act1_list = cur_model_act_list[level]\n",
    "                global level_corr_mat_accum\n",
    "\n",
    "                u1, u2 = cur_model_act_list[level], cur_model_act_list[level]\n",
    "                # u1 = u1.T\n",
    "                # u2 = u2.T\n",
    "                if individual:\n",
    "                    u1, u2 = u1[sample_idx:sample_idx+1], u2[sample_idx:sample_idx+1]\n",
    "                \n",
    "                if plot_type=='corr':\n",
    "                    corr_mat = []\n",
    "                    for b1, b2 in zip(u1, u2):\n",
    "                        u1_standardized = standardize_rows(u1)\n",
    "                        u2_standardized = standardize_rows(u2)\n",
    "                        corr_mat.append(np.dot(u1_standardized, u2_standardized.T) / (u1.shape[1]))\n",
    "                elif  plot_type=='dot':\n",
    "                    corr_mat = []\n",
    "                    for b1, b2 in zip(u1, u2):\n",
    "                    # corr_mat = np.dot(u1, u2.T)\n",
    "                        corr_mat.append(sim_func(b1, b2))\n",
    "                    corr_mat = np.array(corr_mat)\n",
    "                    corr_mat = corr_mat.mean(axis=0) \n",
    "\n",
    "                if abs:\n",
    "                    corr_mat = np.abs(corr_mat)\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "                if accumulate_all:\n",
    "                    level_corr_mat_accum.append(corr_mat[None, ...])\n",
    "                # if not save_all:\n",
    "                #     continue \n",
    "                \n",
    "                corr_mat_list.append(corr_mat)\n",
    "\n",
    "                if not subplot_layers:\n",
    "                    vec_dim = corr_mat.shape[0]//fixed_length\n",
    "                    total_sum = np.abs(corr_mat).sum()\n",
    "                    block_sum = 0\n",
    "                    for i in range(0, len(corr_mat), vec_dim):\n",
    "                        block_sum += np.abs(corr_mat[i:i+vec_dim, i:i+vec_dim]).sum()\n",
    "                    ratio = block_sum/(total_sum-block_sum)\n",
    "\n",
    "                    plt.figure(figsize=(6, 5), dpi=120)\n",
    "                    plt.imshow(corr_mat, cmap='Reds') #, interpolation='nearest')\n",
    "\n",
    "                    extra_text = 'Absolute ' if abs else ''\n",
    "                    plt.colorbar(label=f'{extra_text}Correlation Coefficient')\n",
    "\n",
    "                    os.makedirs(f'./saved_plots_{folder_name}/', exist_ok=True)\n",
    "                    plt.savefig(f'./saved_plots_{folder_name}/{useful_name_list[idx1]}_avg_{abs}.pdf')\n",
    "                    plt.close()\n",
    "            models_corr_mat_list.append(corr_mat_list)\n",
    "        # corr_mat_list = models_corr_mat_list\n",
    "        \n",
    "        if subplot_layers and has_model:\n",
    "            handel=f'{before_after}_indi={individual}/'\n",
    "            folder_dir = f'./saved_plots_sim/{handel}'\n",
    "            # if save:\n",
    "            #     if f'{folder_dir}/{imgname}.pdf' in glob.glob(f'{folder_dir}/{imgname}.pdf'):\n",
    "            #         continue\n",
    "            fig, axs = plt.subplots(len(models_corr_mat_list), len(models_corr_mat_list[0]), figsize=(40, 5.6*len(models_corr_mat_list)))  # 6 subplots in a row, adjust size as needed\n",
    "            for level in range(len(models_corr_mat_list[0])):\n",
    "                \n",
    "                for cm_idx, corr_mat_list in enumerate(models_corr_mat_list):\n",
    "                    try:\n",
    "                        corr_mat = corr_mat_list[level]\n",
    "                    except:\n",
    "                        print(corr_mat_list)\n",
    "                        raise ValueError\n",
    "\n",
    "                    # vec_dim = n_embd\n",
    "                    vec_dim = corr_mat.shape[0]//fixed_length\n",
    "\n",
    "                    total_sum = np.abs(corr_mat).sum()\n",
    "                    block_sum = 0\n",
    "                    for i in range(0, len(corr_mat), vec_dim):\n",
    "                        block_sum += np.abs(corr_mat[i:i+vec_dim, i:i+vec_dim]).sum()\n",
    "                    ratio = block_sum/(total_sum-block_sum)\n",
    "\n",
    "                    axloc = axs[level] if len(models_corr_mat_list)==1 else axs[cm_idx, level]\n",
    "\n",
    "                    cm = axloc.imshow(corr_mat, cmap='Reds') #, interpolation='nearest')\n",
    "                    # show the number on each block\n",
    "                    if show_vals:\n",
    "                        for i in range(0, len(corr_mat), vec_dim):\n",
    "                            for j in range(0, len(corr_mat), vec_dim):\n",
    "                                text = axloc.text(j, i, f'{corr_mat[i:i+vec_dim, j:j+vec_dim].sum():.02f}',\n",
    "                                                    ha=\"center\", va=\"center\", color=\"black\", fontsize=6)\n",
    "\n",
    "                    training_status = ''\n",
    "                    if before_after=='training':\n",
    "                        training_status = 'Trained ' if cm_idx==1 else 'Init '\n",
    "                    title_text = f'{training_status}Layer {level+1}' if level+1 > 0 else 'Token Embeddings'                \n",
    "                    axloc.set_title(title_text)\n",
    "                    cbar = plt.colorbar(cm, ax=axloc, orientation='vertical', fraction=0.05, )\n",
    "                    # Get the scalar formatter from the colorbar\n",
    "                    from matplotlib.ticker import FormatStrFormatter, MaxNLocator\n",
    "                    formatter = FormatStrFormatter('%.1f')\n",
    "                    axloc.yaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "                    axloc.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "\n",
    "                    axloc.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "\n",
    "                    # Set the desired format (e.g., rounding to 2 decimal places)\n",
    "                    # formatter.set_useOffset(False)  # Disable offset if necessary\n",
    "\n",
    "                    # Apply the formatter to the colorbar\n",
    "                    cbar.formatter = formatter\n",
    "                    cbar.update_ticks()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            fig.subplots_adjust(left=0.03, right=0.975, top=0.95, bottom=0.06, hspace=0.25)\n",
    "            if save:\n",
    "                os.makedirs(f'{folder_dir}', exist_ok=True)\n",
    "\n",
    "                plt.savefig(f'{folder_dir}/{imgname}.pdf', format='pdf')\n",
    "                plt.close()\n",
    "                gc.collect()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "        # if idx1 >=10:\n",
    "        #     raise ValueError\n",
    "        if accumulate_all:\n",
    "            level_corr_mat_accum = np.vstack(level_corr_mat_accum)\n",
    "            print(level_corr_mat_accum.shape)\n",
    "            level_corr_mat_accum = level_corr_mat_accum.mean(axis=0)\n",
    "            # Create a heatmap of corr_mat\n",
    "            if not subplot_layers:\n",
    "                plt.figure(figsize=(6, 5), dpi=120)\n",
    "                # plt.imshow(corr_mat, cmap='seismic') #, interpolation='nearest')\n",
    "                plt.imshow(level_corr_mat_accum, cmap='Reds') #, interpolation='nearest')\n",
    "\n",
    "                extra_text = 'Absolute ' if abs else ''\n",
    "                plt.colorbar(label=f'{extra_text}Correlation Coefficient')\n",
    "                plt.show()\n",
    "            corr_mat  =  level_corr_mat_accum\n",
    "            level_corr_mat_accum = []\n",
    "        \n",
    "           \n",
    "\n",
    "\n",
    "# all_drop_down = [e[1] for e in useful_name_list]\n",
    "\n",
    "widgets.interact(get_corr, \n",
    "                 sample_idx=(0, len(X_n)-1), \n",
    "                 drop_down=useful_name_list, \n",
    "                 drop_down2=[None] + useful_name_list, \n",
    "                 drop_down3=[None] + useful_name_list, \n",
    "                 level=(0,12))\n",
    "\n",
    "\n",
    "# idxes = [\n",
    "# # [0,0], [0,1], [0,2], [0,3], [0,4],\n",
    "# # [1,1], [2,2], [3,3], [4,4],\n",
    "# # [4,1], [4,2], [4,3], \n",
    "# [1,2], [1,3],c\n",
    "# ]\n",
    "# from tqdm.auto import tqdm\n",
    "# for idx1, idx2 in tqdm(idxes):\n",
    "#     get_corr(idx1, idx2, save=True)\n",
    "    \n",
    "# all residual vs picking ones that are illustrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae61cfd950e14ed8bbe46514ebf91240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_name', options=('acc=0_add3_ref_original_sd240_T2406121801/c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_att(model_name, layer_idx=0, plot_all=False, save_map=False)>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the difference in projection for the same vector\n",
    "# x = torch.rand(1, 1, 384).to(device)\n",
    "\n",
    "\n",
    "n_embd = 384\n",
    "x = torch.zeros(1, 1, n_embd).to(device)\n",
    "sc = torch.rand(1, 1, n_embd).to(device)\n",
    "\n",
    "idx = np.random.randint(0, n_embd)\n",
    "\n",
    "x[0, 0, idx] = 1\n",
    "x += sc\n",
    "# y = model_list[17].transformer.h[1].attn.c_attn(x)\n",
    "# input_idx = torch.tensor(encode('12modp(123)=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('$123+456=')).to(device)[None,...]\n",
    "input_idx = torch.tensor(encode('$123+456=')).to(device)[None,...]\n",
    "\n",
    "\n",
    "# eqx = torch.tensor(model.create_equal_distancing_vecotrs(8, n_embd, small_component=0.1)[0]).to(device).to(torch.float32)[None,...] * 0.1\n",
    "\n",
    "eqx = torch.tensor(model.create_equal_distancing_vecotrs(9, n_embd, small_component=0.0)[0]).to(device).to(torch.float32)[None,...] * 0.1\n",
    "\n",
    "# Specify the path to your Times New Roman font file\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "\n",
    "def plot_att(model_name, layer_idx=0, plot_all=False, save_map=False):\n",
    "\n",
    "    acc = model_name.split('_')[0]\n",
    "    rest = '_'.join(model_name.split('_')[1:])\n",
    "    imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + '_' + acc\n",
    "\n",
    "    # cur_model = model_list[12]\n",
    "    # cur_model = model_list[2]\n",
    "    model_idx = useful_name_list.index(model_name)\n",
    "    cur_model = model_list[model_idx]\n",
    "    \n",
    "    layer_range = range(layer_idx, layer_idx+1) if not plot_all else range(len(cur_model.transformer.h))\n",
    "    \n",
    "\n",
    "    for level in layer_range:\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        h1 = cur_model.transformer.h[level].attn.c_attn.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}\")\n",
    "        )\n",
    "        \n",
    "\n",
    "        h2 = cur_model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}_iden\")\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "\n",
    "            out = cur_model(input_idx)\n",
    "            # decode and print out\n",
    "            y = decode([out[0].detach().cpu().numpy().argmax()])\n",
    "            print(y)\n",
    "            _ = cur_model(eqx, direct_input_modification=True)\n",
    "            # _ = cur_model(x, direct_input_modification=True)\n",
    "\n",
    "\n",
    "        h1.remove()\n",
    "        h2.remove()\n",
    "        y = activation[f\"layer_{level}\"]\n",
    "\n",
    "        q, k, v  = y.split(n_embd, dim=2)\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(q.detach().cpu().numpy().flatten())\n",
    "        plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots(1, cur_model.config.n_head, figsize=(14, 2))\n",
    "        q_reshape = q.reshape(1, -1, cur_model.config.n_head, n_embd//cur_model.config.n_head)\n",
    "        k_reshape = k.reshape(1, -1, cur_model.config.n_head, n_embd//cur_model.config.n_head)\n",
    "        for i in range(cur_model.config.n_head):\n",
    "            calc = (torch.nn.Softmax(dim=-1)(q_reshape[0, :, i, :]@k_reshape[0, :, i, :].transpose(0,1)))\n",
    "            calc = torch.ones_like(calc) * calc.mean()\n",
    "            result = calc.detach().cpu().numpy()\n",
    "\n",
    "            mat = ax[i].imshow(result, cmap='Reds', interpolation='nearest')\n",
    "            ax[i].set_title(f'head {i}')\n",
    "            plt.colorbar(mat, ax=ax[i], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "        plt.show()\n",
    "            \n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(5, 2))\n",
    "        plt.imshow((torch.nn.Softmax(dim=-1)(q@k.transpose(1, 2))).detach().cpu().numpy()[0], cmap='Reds', interpolation='nearest')\n",
    "        \n",
    "        calc = torch.nn.Softmax(dim=-1)(q@k.transpose(1, 2))\n",
    "        calc = torch.ones_like(calc) * calc.mean()\n",
    "        result = calc.detach().cpu().numpy()\n",
    "        mat = ax[0].imshow(result[0], cmap='Reds', interpolation='nearest')\n",
    "\n",
    "        plt.colorbar(mat, ax=ax[0], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "\n",
    "\n",
    "        q_ = q.detach().cpu().numpy()[0]\n",
    "        k_ = k.detach().cpu().numpy()[0]\n",
    "        mat = cosine_similarity(q_, k_)\n",
    "\n",
    "        mat = ax[1].imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "        plt.colorbar(mat, ax=ax[1], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(x.sum(), q.abs().sum(), k.abs().sum(), v.abs().sum())\n",
    "\n",
    "        y = activation[f\"layer_{level}_iden\"].detach().cpu().numpy()\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(y.flatten())\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        mat = cosine_similarity(y[0], y[0])\n",
    "        plt.imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "                \n",
    "        # maskout the upper triangle\n",
    "        calc = torch.nn.Softmax(dim=-1)(q@k.transpose(1, 2))\n",
    "        calc = torch.ones_like(calc) * calc.mean()\n",
    "        mask = np.triu(np.ones_like(calc[0].detach().cpu().numpy(), dtype=bool))[None, ...]\n",
    "        mask = torch.tensor(mask).to(device)\n",
    "\n",
    "        digit = calc[0, 0, 0].detach().cpu().clone()\n",
    "        calc[mask] = -np.inf\n",
    "        calc[0].diagonal().fill_(digit)\n",
    "        calc = torch.nn.Softmax(dim=-1)(calc)\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(5.7, 10))\n",
    "        gs = fig.add_gridspec(2, 1, height_ratios=[1, 1])  # Adjust hspace as needed\n",
    "\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        att_map = calc[0].detach().cpu().numpy()\n",
    "        cm1 = ax1.imshow(att_map, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "        for i in range(len(att_map)):\n",
    "            for j in range(len(att_map)):\n",
    "                content = f'{att_map[i, j]:.02f}' if att_map[i, j] != 0 else '0'\n",
    "                ax1.text(j, i, content, ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "        cbar1 = plt.colorbar(cm1, ax=ax1, orientation='vertical', fraction=0.05, )\n",
    "        cbar1.ax.tick_params(labelsize=12)\n",
    "        ax1.set_title('Causal Attention Matrix', fontdict={'fontsize':14})\n",
    "        ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        y = calc.cpu() @ v.cpu()\n",
    "        y = y.numpy()\n",
    "        mat = cosine_similarity(y[0], y[0])\n",
    "        cm2 = ax2.imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "        for i in range(len(mat)):\n",
    "            for j in range(len(mat)):\n",
    "                ax2.text(j, i, f'{mat[i, j]:.02f}', ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "        cbar2 = plt.colorbar(cm2, ax=ax2, orientation='vertical', fraction=0.05)\n",
    "        cbar2.ax.tick_params(labelsize=12)\n",
    "        ax2.set_title('Self-Cosine-Similarity Matrix', fontdict={'fontsize':14})\n",
    "        ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "        ax1.annotate('(a)', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=14, fontproperties=prop)\n",
    "        ax2.annotate('(b)', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=14, fontproperties=prop)\n",
    "\n",
    "        fig.subplots_adjust(left=0.0, right=0.92, top=0.97, bottom=0.07, hspace=0.2)\n",
    "        \n",
    "        if save_map:\n",
    "            os.makedirs(f'./saved_att_plots/', exist_ok=True)\n",
    "            # fig.savefig(f'./saved_att_plots/{imgname}_layer={layer_idx+1}.svg')\n",
    "            fig.savefig(f'./saved_att_plots/{imgname}_layer={layer_idx+1}.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        calc = torch.nn.Softmax(dim=-1)(torch.einsum('bhid,bhjd->bhij', q_reshape, k_reshape))\n",
    "        calc = torch.ones_like(calc) * calc.mean()\n",
    "        mask = np.triu(np.ones_like(calc[0].detach().cpu().numpy(), dtype=bool))[None, ...]\n",
    "        mask = torch.tensor(mask).to(device)\n",
    "\n",
    "        digit = calc[0, 0, 0,0 ].detach().cpu().clone()\n",
    "        print(digit)\n",
    "        calc[mask] = -np.inf \n",
    "        # for i in range(calc.shape)\n",
    "        #     calc[0, 0].diagonal().fill_(digit)\n",
    "        T = calc.shape[-1]\n",
    "        calc[:,:,np.arange(T),np.arange(T)] = digit\n",
    "        calc = torch.nn.Softmax(dim=-1)(calc)\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(calc[0,0].detach().cpu().numpy(), cmap='Reds', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "        # raise ValueError\n",
    "widgets.interact(plot_att, model_name=useful_name_list, layer_idx=(0, 5), plot_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most responsible neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48611781e1746e7b71e823eac07670c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_name', options=('acc=0_add3_ref_original_sd240_T2406121801/c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_att(model_name, layer_idx=0, plot_all=0, save_map=False, find_best=True, search_all_heads=False, metric_id=4, bestk=4, clean_plot=True, input_name=['eqx', 'randx', 'sample_1', 'sample_2', 'sample_3'], amp_head=2)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the difference in projection for the same vector\n",
    "# x = torch.rand(1, 1, 384).to(device)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from ast import literal_eval\n",
    "import ipywidgets as widgets\n",
    "\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['axes.labelsize'] = 12  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 12  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 12  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 12  # Title\n",
    "\n",
    "n_embd = 384\n",
    "sz = 12\n",
    "x = torch.zeros(1, sz, n_embd).to(device)\n",
    "sc = torch.rand(1, sz, n_embd).to(device)\n",
    "\n",
    "# idx = np.random.randint(0, n_embd)\n",
    "idx = np.random.randint(0, n_embd, size=(sz))\n",
    "\n",
    "x[0, :, idx] = 1\n",
    "x += sc\n",
    "# y = model_list[17].transformer.h[1].attn.c_attn(x)\n",
    "# input_idx = torch.tensor(encode('12modp(123)=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('$123+45=')).to(device)[None,...]\n",
    "input_idx = torch.tensor(encode('$333+444=777$\\n$123+54=')).to(device)[None,...]\n",
    "input_idx2 = torch.tensor(encode('$123+54=')).to(device)[None,...]\n",
    "input_idx3 = torch.tensor(encode('$992+299=')).to(device)[None,...]\n",
    "\n",
    "eqx = torch.tensor(model.create_equal_distancing_vecotrs(sz, n_embd, small_component=0.01)[0]).to(device).to(torch.float32)[None,...] * 0.1\n",
    "\n",
    "inputs_dict = {\n",
    "    'sample_1': input_idx,\n",
    "    'sample_2': input_idx2,\n",
    "    'sample_3': input_idx3,\n",
    "    'eqx': eqx,\n",
    "    'randx': x,\n",
    "} \n",
    "\n",
    "# Specify the path to your Times New Roman font file\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "def get_PE_tendency(mat):\n",
    "    r = 0\n",
    "    r2 = 0\n",
    "    r3 = 0\n",
    "    r4 = 0\n",
    "    r5 = 0\n",
    "    r5_counts = 0\n",
    "    r6 = 0\n",
    "    r6_count = 0\n",
    "    mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "    for lidx, layer in enumerate(mat[1:]):\n",
    "        r += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "        r3 += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "        r3 -= (np.diff(layer[:lidx+1], axis=0) <= 0).sum()\n",
    "\n",
    "        c2p = sum([((layer[j+1:lidx+1]-layer[j]) > 0).sum() for j in range(lidx+1)])\n",
    "        c2m = sum([((layer[j+1:lidx+1]-layer[j]) <= 0).sum() for j in range(lidx+1)])\n",
    "        r2 += c2p\n",
    "        r4 += c2p - c2m\n",
    "\n",
    "        valid_layer = layer[:lidx+1]\n",
    "        if len(valid_layer)<3: \n",
    "            continue\n",
    "\n",
    "        # cur_order = valid_layer\n",
    "        # original_order = valid_layer[np.argsort(valid_layer)]\n",
    "        # corr = spearmanr(original_order, cur_order).correlation\n",
    "        # if np.isnan(corr):\n",
    "            # continue\n",
    "        # r5 += spearmanr(original_order, cur_order).correlation \n",
    "\n",
    "\n",
    "        right_order = np.argsort(valid_layer-np.arange(len(valid_layer))*1e-5)\n",
    "        right_right_order = np.argsort(right_order)\n",
    "        original_order = np.arange(len(valid_layer))\n",
    "        edit_distance = levenshteinDistance(right_order, original_order)\n",
    "        r5 += edit_distance\n",
    "        r5_counts += len(valid_layer)\n",
    "        r_value = pearsonr(original_order, right_right_order)[0]\n",
    "        if np.isnan(r_value): continue\n",
    "        r6 += pearsonr(original_order, right_right_order)[0]\n",
    "        r6_count += 1\n",
    "        # r5 += pearsonr(original_order, cur_order)[0]\n",
    "\n",
    "    max_r = np.arange(mat.shape[0]-1).sum()\n",
    "    t1 = round(r/max_r, 2)\n",
    "\n",
    "    max_r2 = sum([ np.arange(n+1).sum() for n in np.arange(mat.shape[0]-1)])\n",
    "    t2 = round(r2/max_r2, 2)\n",
    "\n",
    "    t3 = round(r3/max_r, 2)\n",
    "\n",
    "    t4 = round(r4/max_r2, 2)\n",
    "    \n",
    "    t5 =  round((r5_counts-r5) / r5_counts, 2)\n",
    "\n",
    "    t6 = round(r6/r6_count, 2)\n",
    "    return f'({t1},{t2},{t3},{t4},{t5},{t6})'\n",
    "\n",
    "\n",
    "def generate_tendency_map(mat):\n",
    "    mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "    empty_mat = np.zeros_like(mat)\n",
    "    for lidx, layer in enumerate(mat):\n",
    "        if lidx == 0:\n",
    "            continue\n",
    "        counter = 0\n",
    "        for j in range(1, lidx+1):\n",
    "            if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            empty_mat[lidx, j] = counter\n",
    "    \n",
    "    return empty_mat\n",
    "\n",
    "def generate_tendency_map(mat):\n",
    "    mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "    empty_mat = np.zeros_like(mat)\n",
    "    for lidx, layer in enumerate(mat):\n",
    "      \n",
    "        counter = 0\n",
    "        for j in range(0, lidx+1):\n",
    "            if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            empty_mat[lidx, j] = counter\n",
    "            # empty_mat[lidx, j] = np.log(mat[lidx, j])\n",
    "    \n",
    "    return empty_mat\n",
    "\n",
    "    \n",
    "\n",
    "def plot_att(model_name, \n",
    "             layer_idx=0, \n",
    "             plot_all=0, \n",
    "             save_map=False, \n",
    "             find_best=True, \n",
    "             search_all_heads=False,\n",
    "             metric_id = 4,\n",
    "             bestk = 4,\n",
    "             clean_plot = True,\n",
    "             input_name = sorted(list(inputs_dict.keys())), \n",
    "             amp_head=2,):\n",
    "\n",
    "    acc = model_name.split('_')[0]\n",
    "    rest = '_'.join(model_name.split('_')[1:])\n",
    "    imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + '_' + acc\n",
    "\n",
    "    # cur_model = model_list[12]\n",
    "    # cur_model = model_list[2]\n",
    "    model_idx = useful_name_list.index(model_name)\n",
    "    cur_model = model_list[model_idx]\n",
    "    \n",
    "    layer_range = range(layer_idx, layer_idx+1) if plot_all==0 else range(plot_all)\n",
    "    \n",
    "    if len(layer_range) > len(cur_model.transformer.h):\n",
    "        layer_range = range(len(cur_model.transformer.h))\n",
    "\n",
    "    for level in layer_range:\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        h1 = cur_model.transformer.h[level].attn.c_attn.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}\")\n",
    "        )\n",
    "        \n",
    "        h1q = cur_model.transformer.h[level].attn.iq.register_forward_hook(\n",
    "            getActivation(f\"q{level}\")\n",
    "        ) \n",
    "        h1k = cur_model.transformer.h[level].attn.ik.register_forward_hook(\n",
    "            getActivation(f\"k{level}\")\n",
    "        ) \n",
    "        h1v = cur_model.transformer.h[level].attn.iv.register_forward_hook(\n",
    "            getActivation(f\"v{level}\")\n",
    "        ) \n",
    "\n",
    "        h2 = cur_model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}_iden\")\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_values = inputs_dict[input_name]\n",
    "            if 'sample' in input_name:\n",
    "                out = cur_model(input_values)\n",
    "                y = decode([out[0].detach().cpu().numpy().argmax()])\n",
    "                print(y)\n",
    "            else: # eqx or randx\n",
    "                _ = cur_model(input_values, direct_input_modification=True)\n",
    "      \n",
    "\n",
    "        h1.remove()\n",
    "        h1q.remove()\n",
    "        h1k.remove()\n",
    "        h1v.remove()\n",
    "        h2.remove()\n",
    "\n",
    "\n",
    "        '''Plot the attention maps'''\n",
    "        xw = activation[f\"layer_{level}\"]\n",
    "        q, k, v  = xw.split(n_embd, dim=2)\n",
    "        q_reshape = activation[f\"q{level}\"]\n",
    "        k_reshape = activation[f\"k{level}\"]\n",
    "        v_reshape = activation[f\"v{level}\"]\n",
    "\n",
    "        if not clean_plot:\n",
    "            # plt.figure(figsize=(12, 3))\n",
    "            k_numpy = k_reshape.detach().cpu().numpy()\n",
    "            bs, T, n_head, head_dim = k_numpy.shape\n",
    "            k_numpy = k_numpy.reshape(bs, T, -1)\n",
    "            print(k_numpy.shape)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "            axes[0].plot(k_numpy[:, 0].flatten())\n",
    "            axes[0].set_title('key 0')\n",
    "            axes[1].hist(k_numpy[:, 0].flatten(), bins=100)\n",
    "            axes[1].set_title('key 0 distribution')\n",
    "            # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "            # plt.show()\n",
    "\n",
    "            # fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "            k_last = k_numpy[:, -1].flatten()\n",
    "            axes[2].plot(k_last)\n",
    "            axes[2].set_title(f'key {T}')\n",
    "            axes[3].hist(k_last, bins=100)\n",
    "            axes[3].set_title(f'key {T} distribution')\n",
    "            # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "            plt.show()\n",
    "\n",
    "            v_numpy = v_reshape.detach().cpu().numpy()\n",
    "            bs, T, n_head, head_dim = v_numpy.shape\n",
    "            v_numpy = v_numpy.reshape(bs, T, -1)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "            axes[0].plot(v_numpy[:, 0].flatten())\n",
    "            axes[0].set_title('value 0')\n",
    "            axes[1].hist(v_numpy[:, 0].flatten(), bins=100)\n",
    "            axes[1].set_title('value 0 distribution')\n",
    "            # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "            # plt.show()\n",
    "\n",
    "            # fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "            v_last = v_numpy[:, -1].flatten()\n",
    "            axes[2].plot(v_last)\n",
    "            axes[2].set_title(f'value {T}')\n",
    "            axes[3].hist(v_last, bins=100)\n",
    "            axes[3].set_title(f'value {T} distribution')\n",
    "            # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "            plt.show()\n",
    "\n",
    "        from itertools import combinations\n",
    "        head_dim = n_embd//cur_model.config.n_head\n",
    "        idx_comb = [np.arange(head_dim)] + [[i] for i in range(head_dim)]\n",
    "\n",
    "        n_arrays = cur_model.config.n_head\n",
    "        all_records = [[] for _ in range(n_arrays)] \n",
    "        all_maps = [[] for _ in range(n_arrays)] \n",
    "        all_idxsets = [[] for _ in range(n_arrays)] \n",
    "        original_score = []\n",
    "        head2 = []\n",
    "\n",
    "        run_search = True\n",
    "        for cur_idxset in idx_comb:\n",
    "            if not run_search:\n",
    "                break\n",
    "            if not find_best:\n",
    "                run_search = False\n",
    "                cur_idxset = range(head_dim)\n",
    "\n",
    "            if not run_search:\n",
    "                fig, ax = plt.subplots(4, cur_model.config.n_head, figsize=(18, 10))\n",
    "\n",
    "            for hidx in range(cur_model.config.n_head):\n",
    "                \n",
    "\n",
    "                attmap = (torch.nn.Softmax(dim=-1)(q_reshape[0, hidx, :, cur_idxset]@k_reshape[0, hidx, :, cur_idxset].transpose(0,1)))\n",
    "                mask = np.triu(np.ones_like(attmap.detach().cpu().numpy(), dtype=bool))\n",
    "                mask = torch.tensor(mask).to(device)\n",
    "\n",
    "                digit = attmap.flatten()[0].detach().cpu().clone()\n",
    "                attmap[mask] = -np.inf \n",
    "                T = attmap.shape[-1]\n",
    "                attmap[np.arange(T), np.arange(T)] = digit\n",
    "\n",
    "                attmap_np = attmap.detach().cpu().numpy()\n",
    "                attmap_tend = get_PE_tendency(attmap_np)\n",
    "\n",
    " \n",
    "                \n",
    "                s_attmap = torch.nn.Softmax(dim=-1)(attmap)\n",
    "                # calc = torch.ones_like(calc) * calc.mean()\n",
    "                s_attmap_np = s_attmap.detach().cpu().numpy()\n",
    "                tendency_map = generate_tendency_map(s_attmap_np)\n",
    "                mat_tend = get_PE_tendency(s_attmap_np)\n",
    "\n",
    "                y_out = s_attmap.cpu() @ v_reshape[0, hidx, :, cur_idxset].cpu()\n",
    "                sim_map = cosine_similarity(y_out, y_out)\n",
    "\n",
    "\n",
    "                if hidx == amp_head or (search_all_heads and find_best):\n",
    "                    if find_best:\n",
    "                        adj_score = literal_eval(attmap_tend)[metric_id]\n",
    "\n",
    "                        \n",
    "                        if len(cur_idxset) == head_dim:\n",
    "                            original_score.append(adj_score)\n",
    "                            head2.append(attmap_np)\n",
    "                        else:\n",
    "                            all_records[hidx].append(adj_score)\n",
    "                            all_maps[hidx].append(attmap_np)\n",
    "                            all_idxsets[hidx].append(cur_idxset)\n",
    "                    else:\n",
    "                        head2.append(attmap_np) \n",
    "\n",
    "\n",
    "                if not run_search: \n",
    "                    mat = ax[0, hidx].imshow(attmap_np, cmap='Reds', interpolation='nearest')\n",
    "                    ax[0, hidx].set_title(f'head {hidx}={attmap_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[0, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    \n",
    "                    mat = ax[1, hidx].imshow(s_attmap_np, cmap='Reds', interpolation='nearest')\n",
    "                    ax[1, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[1, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[2, hidx].imshow(tendency_map, cmap='Reds', interpolation='nearest')\n",
    "                    ax[2, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[2, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[3, hidx].imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "                    sim_tend = get_PE_tendency(sim_map)\n",
    "\n",
    "                    ax[3, hidx].set_title(f'head {hidx}={sim_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[3, hidx], orientation='vertical', fraction=0.06,)\n",
    "\n",
    "            if not run_search: \n",
    "                plt.subplots_adjust(hspace=.4)\n",
    "                fig.suptitle(f'{input_name}_{imgname}_layer={level+1}', fontsize=16, y=0.96)\n",
    "                if save_map:\n",
    "                    os.makedirs(f'./saved_heads_plots/', exist_ok=True)\n",
    "                    fig.savefig(f'./saved_heads_plots/{input_name}_{imgname}_layer={level+1}.svg')\n",
    "                plt.show()\n",
    "\n",
    "        if find_best:\n",
    "            fig, ax = plt.subplots(2, len(head2), figsize=(3.5*len(head2), 6))\n",
    "            for amidx, attnmap_np in enumerate(head2):\n",
    "                ax_loc_0 = ax[0, amidx] if len(head2)>1 else ax[0]\n",
    "                ax_loc_1 = ax[1, amidx] if len(head2)>1 else ax[1]\n",
    "\n",
    "                mat = ax_loc_0.imshow(attnmap_np, cmap='Reds', interpolation='nearest')\n",
    "                head2_PE_scores = get_PE_tendency(attnmap_np)\n",
    "                ax_loc_0.set_title(f'head {amp_head}={head2_PE_scores}')\n",
    "                plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                tendmap_head2 = generate_tendency_map(attnmap_np)\n",
    "                mat = ax_loc_1.imshow(tendmap_head2, cmap='Reds', interpolation='nearest')\n",
    "                original_score[amidx]\n",
    "                ax_loc_1.set_title(f'head {amp_head}={original_score[amidx]}')\n",
    "                plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "            plt.show()\n",
    "            \n",
    "            fig, ax = plt.subplots(2, len(head2), figsize=(3.5*len(head2), 6))\n",
    "\n",
    "            for hridx, head_rec in enumerate(all_records):\n",
    "\n",
    "                if len(head_rec) == 0: \n",
    "                    continue\n",
    "\n",
    "                topk_records_idx = np.argsort(head_rec)[-bestk:]\n",
    "\n",
    "                best_maps = np.array([all_maps[hridx][i] for i in topk_records_idx])\n",
    "                best_map = best_maps.mean(axis=0)\n",
    "                best_score = np.mean([head_rec[i] for i in topk_records_idx])\n",
    "                best_idxcomb = [all_idxsets[hridx][i] for i in topk_records_idx]\n",
    "\n",
    "\n",
    "                ax_loc_0 = ax[0, hridx] if len(head2)>1 else ax[0]\n",
    "                ax_loc_1 = ax[1, hridx] if len(head2)>1 else ax[1]\n",
    "\n",
    "                mat = ax_loc_0.imshow(best_map, cmap='Reds', interpolation='nearest')\n",
    "                # best_PE_scores = get_PE_tendency(best_map)\n",
    "                ax_loc_0.set_title(f'best avg={best_score:.02f}')\n",
    "                plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                tendmap_best = generate_tendency_map(best_map)\n",
    "                mat = ax_loc_1.imshow(tendmap_best, cmap='Reds', interpolation='nearest')\n",
    "                ax_loc_1.set_title(f'best ={best_idxcomb}')\n",
    "                plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "            \n",
    "            plot_name = f'bestk={bestk}_{input_name}_{imgname}_layer={level+1}'\n",
    "            fig.suptitle(plot_name, fontsize=16, y=0.96)\n",
    "            if save_map:\n",
    "                os.makedirs(f'./saved_heads_plots/', exist_ok=True)\n",
    "                fig.savefig(f'./saved_heads_plots/{plot_name}.svg')\n",
    "            plt.show()\n",
    "                \n",
    "\n",
    "        # ================================================================\n",
    "        \n",
    "        # if not clean_plot:\n",
    "\n",
    "            # fig, ax = plt.subplots(1, 1, figsize=(20, 12.5))\n",
    "\n",
    "\n",
    "            # # map_to_show = result[0]\n",
    "            # map_to_show = head2[0]\n",
    "            # mat = ax.imshow(map_to_show, cmap='Reds', interpolation='nearest')\n",
    "            # value_amp = 20\n",
    "            # for hidx in range(len(map_to_show)):\n",
    "            #     for j in range(len(map_to_show)):\n",
    "            #         ax.text(j, hidx, f'{map_to_show[hidx, j]*value_amp:.02f}', ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "            # plt.colorbar(mat, ax=ax, orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "            # ax.set_title(f'showing head {amp_head}, values × {value_amp}')\n",
    "\n",
    "\n",
    "            # # q_ = q.detach().cpu().numpy()[0]\n",
    "            # # k_ = k.detach().cpu().numpy()[0]\n",
    "            # # sim_map = cosine_similarity(q_, k_)\n",
    "\n",
    "            # # mat = ax[1].imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "\n",
    "            # # plt.colorbar(mat, ax=ax[1], orientation='vertical', fraction=0.06, pad=0.04,)\n",
    "            # # plt.show()\n",
    "\n",
    "\n",
    "            # # print(x.sum(), q.abs().sum(), k.abs().sum(), v.abs().sum())\n",
    "\n",
    "            # y = activation[f\"layer_{level}_iden\"].detach().cpu().numpy()\n",
    "            # plt.figure(figsize=(12, 3))\n",
    "            # plt.plot(y.flatten())\n",
    "            # plt.show()\n",
    "\n",
    "            # plt.figure(figsize=(6, 5))\n",
    "            # mat = cosine_similarity(y[0], y[0])\n",
    "            # mat_tend = get_PE_tendency(mat)\n",
    "            # plt.imshow(mat, cmap='Reds', interpolation='nearest')\n",
    "            # plt.title(f'tendency={mat_tend}')\n",
    "            # plt.colorbar()\n",
    "            \n",
    "\n",
    "widgets.interact(plot_att, \n",
    "                 model_name=useful_name_list, \n",
    "                 layer_idx=(0, 11), \n",
    "                 plot_all=(0, 6), \n",
    "                 amp_head=(0, 5),\n",
    "                 metric_id = (0, 5),\n",
    "                 bestk=(1, 16))\n",
    "# widgets.interact(plot_att, model_name=useful_name_list, layer_idx=(0, 5), plot_all=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13963aec4e124b979bc22fbc0faaefad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfMerger \n",
    "import cairosvg\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def convert_svg_to_pdf(svg_files, output_pdf):\n",
    "    temp_pdfs = []\n",
    "\n",
    "    # Convert each SVG to a temporary PDF\n",
    "    for svg_file in svg_files:\n",
    "        pdf_file = f\"{svg_file}.pdf\"\n",
    "        cairosvg.svg2pdf(url=svg_file, write_to=pdf_file)\n",
    "        temp_pdfs.append(pdf_file)\n",
    "\n",
    "    # Merge all temporary PDFs into a single PDF\n",
    "    merger = PdfMerger ()\n",
    "    for pdf in temp_pdfs:\n",
    "        merger.append(pdf)\n",
    "\n",
    "    # Write out the final merged PDF\n",
    "    merger.write(output_pdf)\n",
    "    merger.close()\n",
    "\n",
    "    # Clean up temporary PDF files\n",
    "    for pdf in temp_pdfs:\n",
    "        os.remove(pdf)\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "root_dir = './saved_heads_plots'\n",
    "svg_files = sorted(glob.glob(root_dir + '/*.svg'))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "f_dict = {}\n",
    "for fpath in svg_files:\n",
    "    fname = fpath.split('/')[-1]\n",
    "    model_name = '_'.join(fname.split('_')[:-1])\n",
    "    if model_name not in f_dict:\n",
    "        f_dict[model_name] = []\n",
    "    f_dict[model_name].append(fpath)\n",
    "for model_name in tqdm(f_dict):\n",
    "    sorted_files = sorted(f_dict[model_name])\n",
    "    output_pdf = f'{root_dir}_pdf/{model_name}.pdf'\n",
    "    convert_svg_to_pdf(sorted_files, output_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c81ba76027c4312900d4572e9b47e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_name', options=('acc=0_add3_ref_original_sd240_T2406121801/c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.prob_model(model_name, model2_name=None, save_all_models=False, save=False, layer_idx=0, plot_all=False, rand_perm=True, before_after=[False, 'training', 'attention'], total_samples=512, max_epochs=20, fixed_length=4, input_type=['x', 'x1*x2', 'x1-x2', '[x1, x2]'], data_type=['sample', 'same', 'randx', 'eqx'], scaler=0.1, linear_mode=1, loss_type=['MSE', 'Cross Entropy'], hook_location='after_attn', plot_type=['violin', 'both', 'tsne'])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.stats import pearsonr, linregress\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "import re\n",
    "from thop import profile, clever_format\n",
    "\n",
    "\n",
    "# Specify the path to your Times New Roman font file\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 27})\n",
    "plt.rcParams['axes.labelsize'] = 27  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 27  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 27  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 27  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 27  # Title\n",
    "# Define the probe model\n",
    "class PositionalProbe(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 linear_mode=0,\n",
    "                 n_outs=1):\n",
    "        super(PositionalProbe, self).__init__()\n",
    "        # Only a single output unit as it predicts one position at a time\n",
    "        if linear_mode==0:\n",
    "            self.linear = nn.Linear(embedding_dim, n_outs)\n",
    "        elif linear_mode in [1, 2]:\n",
    "            self.linear = nn.Linear(embedding_dim, 64)\n",
    "            self.linear2 = nn.Linear(64, n_outs)\n",
    "        elif linear_mode in [3, 4]:\n",
    "            self.linear = nn.Linear(embedding_dim, 256)\n",
    "            self.linear2 = nn.Linear(256, 64)\n",
    "            self.linear3 = nn.Linear(64, n_outs)\n",
    "        elif linear_mode==5:\n",
    "            self.linear = nn.Linear(embedding_dim, 512)\n",
    "            self.linear2 = nn.Linear(512, 384)\n",
    "            self.linear3 = nn.Linear(384, 128)\n",
    "            self.linear4 = nn.Linear(128, n_outs)\n",
    "    \n",
    "                \n",
    "        self.n_outs = n_outs\n",
    "        self.linear_mode = linear_mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the output to match the target dimension which is [batch_size * seq_len]\n",
    "        if self.linear_mode==0:\n",
    "            x = self.linear(x)\n",
    "        elif self.linear_mode==1:\n",
    "            x = self.linear(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.linear2(x)\n",
    "        elif self.linear_mode==2:\n",
    "            x = self.linear(x)\n",
    "            x = torch.exp(x)  \n",
    "            x = self.linear2(x)\n",
    "        elif self.linear_mode==3:\n",
    "            x = self.linear(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.linear2(x)\n",
    "            x = torch.exp(x) # make it so that the coef is meaningful\n",
    "            x = self.linear3(x)\n",
    "\n",
    "        elif self.linear_mode==4:\n",
    "            x = self.linear(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.linear2(x)\n",
    "            # x = torch.log(x) # make it so that the coef is meaningful\n",
    "            # log_x = torch.where(x > 0, torch.log(x), 0)\n",
    "            x = self.linear3(x)\n",
    "\n",
    "        elif self.linear_mode==5:\n",
    "            x = self.linear(x)\n",
    "            x = torch.selu(x)\n",
    "            x = self.linear2(x)\n",
    "            x = torch.exp(x) # make it so that the coef is meaningful\n",
    "            x = self.linear3(x)\n",
    "            x = torch.selu(x)\n",
    "            x = self.linear4(x)\n",
    "\n",
    "        if self.n_outs!=1:\n",
    "            x = torch.nn.functional.softmax(x, dim=-1)\n",
    "            return x\n",
    "        else:\n",
    "            return x.squeeze(-1)\n",
    "\n",
    "# Register hooks to capture the outputs from the transformer layer\n",
    "def get_activation_hook(activations, name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def set_ticks(num_ticks, min_value=0):\n",
    "    if 20 > num_ticks-min_value > 10:\n",
    "        step = 2\n",
    "    elif 40>num_ticks-min_value > 20:\n",
    "        step = 5\n",
    "    elif num_ticks-min_value > 40:\n",
    "        step = 10\n",
    "    else:\n",
    "        step = 1\n",
    "\n",
    "    if min_value !=0:\n",
    "        min_value = (int(min_value/step))*step\n",
    "\n",
    "    return np.arange(min_value, num_ticks, step)\n",
    "    \n",
    "causal_training=True # set this global variable to true so that get batch will return (bs, 256) \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_tsne_colored_by_position(embeddings, labels, ax=None, training_status='',\n",
    "                                  perplexity=32):\n",
    "    \"\"\"\n",
    "    Takes a numpy array of embeddings of shape (bs, T, d) and plots the TSNE visualization,\n",
    "    with colors representing the position index within each sequence before reshaping and a detailed legend.\n",
    "    If an axis (ax) is provided, the plot will be drawn on it.\n",
    "    \"\"\"\n",
    "    # Reshape the embeddings from (bs, T, d) to (-1, d)\n",
    "    bs, T, d = embeddings.shape\n",
    "    flat_embeddings = embeddings.reshape(-1, d)\n",
    "    flat_embeddings = (flat_embeddings - flat_embeddings.mean(axis=0)) / flat_embeddings.std(axis=0)\n",
    "    labels = labels.reshape(-1)\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=500, n_jobs=-1)\n",
    "    tsne_results = tsne_model.fit_transform(flat_embeddings)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # tsne_results = tsne_results.reshape(bs, T, 2)\n",
    "    # for t in range(T):\n",
    "        # ax.scatter(tsne_results[:, t, 0], tsne_results[:, t, 1], marker='o', s=8, alpha=0.5)\n",
    "    # scatter = ax.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels.astype(int), cmap='tab10', marker='o', s=8, alpha=0.5)\n",
    "    scatter = ax.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels.astype(int), cmap='tab10', marker='o', s=8, alpha=0.5)\n",
    "\n",
    "    ax.set_title(f'{training_status}TSNE Plot')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "\n",
    "    # Create a legend with exact color for each index\n",
    "    legend_elements = []\n",
    "    cmap = plt.cm.viridis\n",
    "    for idx in range(T):\n",
    "        legend_elements.append(\n",
    "            plt.Line2D([0], [0], marker='o', color=cmap(idx / (T - 1)), label=str(idx + 1), markersize=2, linestyle='')\n",
    "        )\n",
    "    \n",
    "    ax.legend(handles=legend_elements, title_fontsize=10, fontsize=10, loc='upper right', framealpha=0.5)\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def prob_model(model_name, \n",
    "    model2_name=None, \n",
    "    save_all_models=False,\n",
    "    save = False,\n",
    "    layer_idx=0, \n",
    "    plot_all=False, \n",
    "    rand_perm = True,\n",
    "    before_after=[False, 'training', 'attention'],\n",
    "    total_samples = 512,\n",
    "    max_epochs = 20,\n",
    "    fixed_length = 4,\n",
    "    input_type = ['x', 'x1*x2', 'x1-x2', '[x1, x2]'],\n",
    "    # data_type = ['sample', 'eqx'],\n",
    "    data_type = ['sample', 'same', 'randx', 'eqx', ],\n",
    "    scaler = 0.1,\n",
    "    linear_mode = 1,\n",
    "    loss_type = ['MSE', 'Cross Entropy'],\n",
    "    hook_location = 'after_attn',\n",
    "    plot_type = [ 'violin', 'both', 'tsne'],\n",
    "    ):\n",
    "\n",
    "\n",
    "    total_tokens = total_samples * fixed_length\n",
    "    samples_to_generate = total_tokens//256 + total_tokens%256\n",
    "\n",
    "    probe_dim = 384\n",
    "    probe = PositionalProbe(embedding_dim=384,\n",
    "                            linear_mode=linear_mode,\n",
    "                            n_outs=1)\n",
    "                \n",
    "    # count n params\n",
    "    n_params = sum(p.numel() for p in probe.parameters() if p.requires_grad)\n",
    "    print('probe n params:', n_params, end=' ')\n",
    "    probe.eval()\n",
    "    with torch.no_grad():\n",
    "        flops, params = profile(probe, inputs=(torch.rand(1,1,probe_dim),), verbose=False)\n",
    "        flops, params = clever_format([flops, params], \"%.3f\")\n",
    "        print(f'FLOPS: {flops}, params: {params}')\n",
    "\n",
    "\n",
    "    if save_all_models:\n",
    "        models_to_show = useful_name_list\n",
    "    else:\n",
    "        models_to_show = [model_name]\n",
    "\n",
    "    \n",
    "    for model_name in tqdm(models_to_show):\n",
    "        \n",
    "        acc = model_name.split('_')[0]\n",
    "        iteration = int(re.search(r'acc_(\\d+).pt', model_name).group(1))\n",
    "        rest = '_'.join(model_name.split('_')[1:])\n",
    "        \n",
    "        model_idx = useful_name_list.index(model_name)\n",
    "        cur_model = model_list[model_idx]\n",
    "        if model2_name is not None:\n",
    "            model2_idx = useful_name_list.index(model2_name)\n",
    "            cur_model2 = model_list[model2_idx]\n",
    "            \n",
    "        cur_models = [cur_model]\n",
    "\n",
    "        if before_after=='training':\n",
    "            rest = '_'.join(rest.split('.')[0].split('_')[:-1])\n",
    "            if acc.split('=')[-1]!='0' or iteration!=0:\n",
    "                print(acc, iteration)\n",
    "                continue\n",
    "            trained_model_name = None\n",
    "            trained_cur_model = None\n",
    "            for m_idx, m_name in enumerate(useful_name_list):\n",
    "                if rest in m_name and not (m_name==model_name):\n",
    "                    trained_model_name = m_name\n",
    "                    trained_cur_model = model_list[m_idx]\n",
    "                    break\n",
    "\n",
    "            assert trained_model_name is not None, 'no trained model found'\n",
    "            cur_models.append(trained_cur_model)\n",
    "\n",
    "        elif before_after=='attention':\n",
    "            cur_models.append(cur_model)\n",
    "\n",
    "        addons = '_' + acc if not before_after=='training' else ''\n",
    "        imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + addons\n",
    "        dir_name = f'./saved_probe_plots_{before_after}/{plot_type}-{hook_location}-{data_type}/{input_type}_{total_samples}_{fixed_length}_{linear_mode}'\n",
    "        if save_all_models:\n",
    "            if f'{dir_name}/{imgname}.pdf' in glob.glob( f'{dir_name}/{imgname}.pdf'):\n",
    "                continue        \n",
    "\n",
    "        \n",
    "        layer_range = range(layer_idx, layer_idx+1) if not (plot_all or save_all_models) \\\n",
    "            else range(len(cur_model.transformer.h))\n",
    "        \n",
    "        plot_height = 2 if plot_type == 'both' else 1\n",
    "        nrows = 1 if not before_after else 2\n",
    "        fig, axs = plt.subplots(nrows*plot_height, len(layer_range), figsize=(6 * len(layer_range), 12.3/2*nrows*plot_height))\n",
    "\n",
    "        \n",
    "        for cm_idx, cur_model in enumerate(cur_models):\n",
    "            for lidx, layer in enumerate(tqdm(layer_range)):\n",
    "                if data_type == 'sample':\n",
    "                    # # X = get_batch(\"train\", batch_size=4096)[0]\n",
    "                    # # X_test = get_batch(\"valid\", batch_size=4096)[0]\n",
    "                    # X = get_batch(\"train\", batch_size=samples_to_generate)[0]\n",
    "                    # X_test = get_batch(\"valid\", batch_size=samples_to_generate//4)[0] # use a bit less for testing in order to plot QAQA\n",
    "                    # X = \"\".join([decode(X[i].tolist())\n",
    "                    #             for i in range(X.shape[0])])[:total_tokens] # truncate x to lower computation\n",
    "                    # X_test = \"\".join([decode(X_test[i].tolist())\n",
    "                    #             for i in range(X_test.shape[0])])[:total_tokens]\n",
    "                    # X_n = np.array(list(X[:len(X) // fixed_length * fixed_length])).reshape(\n",
    "                    #     -1, fixed_length\n",
    "                    # )\n",
    "                    # X = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "                    # X_n = np.array(list(X_test[:len(X_test) // fixed_length * fixed_length])).reshape(\n",
    "                    #     -1, fixed_length\n",
    "                    # )\n",
    "                    # X_test = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "\n",
    "                    # if rand_perm: # shuffle in input sequence\n",
    "                    #     for xidx in range(X.shape[0]):\n",
    "                    #         X[xidx] = X[xidx, torch.randperm(X[xidx].shape[0])]\n",
    "                    #     for xidx in range(X_test.shape[0]):\n",
    "                    #         X_test[xidx] = X_test[xidx, torch.randperm(X_test[xidx].shape[0])]\n",
    "                    X = [np.random.randint(0, 45, (fixed_length))\\\n",
    "                        for _ in range(total_tokens//fixed_length)]\n",
    "                    X = torch.tensor(X).long()\n",
    "                    X_test = [np.random.randint(45, 90, (fixed_length)) \\\n",
    "                            for _ in range(total_tokens//fixed_length)]\n",
    "                    X_test = torch.tensor(X_test).long()\n",
    "                    # print(X.shape, X_test.shape)\n",
    "                elif data_type=='eqx':\n",
    "                    X = [cur_model.create_equal_distancing_vecotrs(fixed_length, 384, small_component=0.1)[0] *0.05\\\n",
    "                        for _ in range(total_tokens//fixed_length)]\n",
    "                    X = torch.tensor(X).float()\n",
    "                    X_test = [cur_model.create_equal_distancing_vecotrs(fixed_length, 384, small_component=0.1)[0] *0.05\\\n",
    "                            for _ in range(total_tokens//fixed_length)]\n",
    "                    X_test = torch.tensor(X_test).float()\n",
    "\n",
    "                elif data_type=='randx':\n",
    "                    # X = [np.random.rand(fixed_length, 384) * scaler\\\n",
    "                    #     for _ in range(total_tokens//fixed_length)]\n",
    "                    X = np.random.rand(total_tokens//fixed_length, fixed_length, 384)* scaler\n",
    "                    X = torch.tensor(X).float()\n",
    "                    X_test = np.random.rand(total_tokens//fixed_length, fixed_length, 384)* scaler\n",
    "                    X_test = torch.tensor(X_test).float()\n",
    "                elif data_type == 'same':\n",
    "                    X = [np.ones((fixed_length, 384)) * scaler\\\n",
    "                        for _ in range(total_tokens//fixed_length)]\n",
    "                    X = torch.tensor(X).float()\n",
    "                    X_test = [np.ones((fixed_length, 384)) * scaler\\\n",
    "                            for _ in range(total_tokens//fixed_length)]\n",
    "                    X_test = torch.tensor(X_test).float()\n",
    "\n",
    "                \n",
    "\n",
    "                # X = X.to(device)\n",
    "                if before_after=='attention':\n",
    "                    hook_location = 'before_attn' if cm_idx==0 else 'after_attn'\n",
    "\n",
    "                # Function to create a dataset with position labels\n",
    "                layer_activations = {}\n",
    "                def create_position_dataset(model, inputs, layer, loss_type):\n",
    "                \n",
    "                    if hook_location == 'after_attn':\n",
    "                        hook = model.transformer.h[layer].attn.identity.register_forward_hook(\n",
    "                            get_activation_hook(layer_activations, f\"layer_{layer}\")\n",
    "                        )\n",
    "                    elif hook_location == 'before_attn':\n",
    "                        hook = model.transformer.h[layer].attn.pre_att_identity.register_forward_hook(\n",
    "                            get_activation_hook(layer_activations, f\"layer_{layer}\")\n",
    "                        )\n",
    "                    elif hook_location == 'mlp':\n",
    "                        hook = model.transformer.h[layer].layer_identity.register_forward_hook(\n",
    "                            get_activation_hook(layer_activations, f\"layer_{layer}\")\n",
    "                        )\n",
    "\n",
    "\n",
    "                    if hook_location != 'raw':\n",
    "                        with torch.no_grad():\n",
    "                            if data_type == 'sample':\n",
    "                                _ = model(inputs.to(device))\n",
    "                            elif data_type in ['eqx', 'randx', 'same']:\n",
    "                                _ = model(inputs.to(device), direct_input_modification=True)\n",
    "                        hook.remove()\n",
    "                    else:\n",
    "                        layer_activations[f\"layer_{layer}\"] = inputs\n",
    "\n",
    "                    \n",
    "                    # inputs.size(1) is the sequence length\n",
    "                    # if input_type == 'x':\n",
    "                    positions = torch.arange(inputs.size(1)).repeat(inputs.size(0), 1).to(inputs.device)\n",
    "                    activation_tensor = layer_activations[f\"layer_{layer}\"] # Adjust shape if necessary\n",
    "                    if input_type in ['x1*x2', 'x1-x2'] :\n",
    "                        new_positions = []\n",
    "                        new_activation_tensor = []\n",
    "                        for sidx, x1 in enumerate(activation_tensor):\n",
    "                            # pair_idx = np.random.randint(0, len(activation_tensor))\n",
    "                            # pair_odering = np.random.permutation(len(x1))\n",
    "                            # x2 = x1[pair_odering]\n",
    "                            # p1, p2 = positions[sidx], positions[sidx][pair_odering]\n",
    "\n",
    "                            x2 = x1[-1]\n",
    "                            \n",
    "                            p1, p2 = positions[sidx], positions[sidx, -1]\n",
    "                            if input_type == 'x1*x2':  # maybe normalize by their norms\n",
    "                                new_activation_tensor.append((x1*x2)[None, ...]/(x1.norm()*x2.norm()))\n",
    "                            elif input_type == 'x1-x2':\n",
    "                                new_activation_tensor.append((x1-x2).abs()[None, ...])\n",
    "                            elif input_type == '[x1, x2]':\n",
    "                                new_activation_tensor.append((np.hstack([x1,x2])).abs()[None, ...])\n",
    "                            new_positions.append((p1-p2).abs()[None, ...])\n",
    "                        # print((p1-p2).abs()[None, ...].shape, (p1-p2).abs())\n",
    "                        # print(x1.shape, x2.shape, (x1*x2).shape)\n",
    "                        positions = torch.vstack(new_positions)\n",
    "                        activation_tensor = torch.vstack(new_activation_tensor)\n",
    "                        # print('Pshape:', positions)\n",
    "                        # print('Ashape:', activation_tensor.shape)\n",
    "                        # print(': )', positions.shape, activation_tensor.shape)\n",
    "\n",
    "                    # print(inputs.shape, activation_tensor.shape, positions.shape, len(layer_activations))\n",
    "\n",
    "                    # return TensorDataset(activation_tensor, positions)\n",
    "                    if loss_type == 'Cross Entropy':\n",
    "                        positions = positions.long()\n",
    "                    elif loss_type == 'MSE':\n",
    "                        positions = positions.float()\n",
    "                    activation_tensor = activation_tensor.squeeze(0)\n",
    "                    return activation_tensor, positions\n",
    "\n",
    "                # Assuming `input_idx` and `cur_model` are defined\n",
    "                \n",
    "                train_activation_tensor, train_positions = create_position_dataset(cur_model,  X,  layer, loss_type)\n",
    "                dataset = TensorDataset(train_activation_tensor, train_positions)\n",
    "                \n",
    "\n",
    "                test_model = cur_model if model2_name is None else cur_model2\n",
    "                test_activation_tensor, test_positions =  create_position_dataset(test_model, X_test, layer, loss_type)\n",
    "                test_dataset = TensorDataset(test_activation_tensor, test_positions) \n",
    "\n",
    "                dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "                # Initialize the probe\n",
    "                \n",
    "                bs, T, probe_dim = layer_activations[f\"layer_{layer}\"].shape\n",
    "                n_outs = T if loss_type == 'Cross Entropy' else 1\n",
    "                probe = PositionalProbe(embedding_dim=probe_dim,\n",
    "                            linear_mode=linear_mode,\n",
    "                            n_outs=n_outs).to(device)\n",
    "                \n",
    "             \n",
    "\n",
    "                optimizer = torch.optim.AdamW(probe.parameters())\n",
    "\n",
    "                if loss_type == 'MSE':\n",
    "                    criterion = nn.MSELoss()  # Using Mean Squared Error Loss for regression\n",
    "                elif loss_type == 'Cross Entropy':\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                # Train the probe and evaluate Pearson correlation\n",
    "                # print(len(dataset), len(test_dataset))\n",
    "                # best_loss = float('inf')\n",
    "                # best_weights = None\n",
    "                for epoch in range(max_epochs):\n",
    "                    cur_loss = []\n",
    "                    for data, targets in dataloader:\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = probe(data.to(device))\n",
    "                        loss = criterion(outputs, targets.to(device))  # Ensure targets are float for MSE calculation\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        cur_loss.append(loss.item())\n",
    "                    cur_loss = np.mean(cur_loss)\n",
    "                    # if cur_loss < best_loss:\n",
    "                    #     best_loss = cur_loss\n",
    "                    #     best_weights = probe.state_dict()\n",
    "                    # if epoch % 15 == 0 or epoch == 0 or epoch == max_epochs-1:\n",
    "                        # print(f\"Epoch {epoch}, Loss: {cur_loss.item():.02f}\", end='\\t')\n",
    "                # print()\n",
    "\n",
    "                # probe.load_state_dict(best_weights)\n",
    "                \n",
    "                all_outputs = []\n",
    "                all_targets = []\n",
    "                all_test_loss = []\n",
    "                probe.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data, targets in test_loader:\n",
    "                        outputs = probe(data.to(device))\n",
    "                        test_loss = criterion(outputs, targets.to(device))\n",
    "                        all_test_loss.append(test_loss.item())\n",
    "                        if loss_type == 'Cross Entropy':\n",
    "                            outputs = torch.argmax(outputs, dim=-1)\n",
    "                        all_outputs.append(outputs.detach().cpu().numpy())\n",
    "                        all_targets.append(targets.detach().cpu().numpy())\n",
    "                \n",
    "                # print(f\"Test Loss: {np.mean(all_test_loss)}\", end = '\\t')\n",
    "\n",
    "                # Concatenate all collected data\n",
    "                all_outputs = np.concatenate(all_outputs).flatten()\n",
    "                all_targets = np.concatenate(all_targets).flatten()\n",
    "\n",
    "                # Compute the Pearson correlation coefficient\n",
    "                correlation, _ = pearsonr(all_outputs, all_targets)\n",
    "                # print(f'Pearson correlation coefficient: {correlation}')\n",
    "                \n",
    "                slope, intercept, r_value, p_value, std_err = linregress(all_targets, all_outputs)\n",
    "                best_fit_line = slope * np.array(all_targets) + intercept\n",
    "\n",
    "\n",
    "                data = pd.DataFrame({\n",
    "                    'Predicted Positions': all_outputs,\n",
    "                    'True Positions': all_targets.astype(int), \n",
    "                })  \n",
    "\n",
    "                data['Counts'] = data.groupby(['Predicted Positions', 'True Positions'])['Predicted Positions'].transform('count')\n",
    "                # Scatter plot on the first subplot\n",
    "                # if not before_after_training:\n",
    "\n",
    "                #     ax0loc = axs[0, lidx] if len(layer_range) > 1 else axs[0]\n",
    "                #     ax0loc.scatter(all_targets, all_outputs, alpha=0.4, s=5)\n",
    "                #     ax0loc.set_title(f'Layer {layer+1} (r={correlation:.2f}, loss={np.mean(all_test_loss):.2f})')\n",
    "                #     ax0loc.set_xlabel('True Positions')\n",
    "                #     ax0loc.set_ylabel('Predicted Positions')\n",
    "                #     ax0loc.grid(True)\n",
    "                #     ax0loc.set_ylim(min(all_outputs)-5, max(all_outputs)+5)\n",
    "                #     ax0loc.plot(all_targets, best_fit_line, 'r', label=f'y={slope:.2f}x+{intercept:.2f}')\n",
    "                #     # ax0loc.legend()\n",
    "                    \n",
    "                #     max_value = max(all_targets)\n",
    "                #     ax0loc.set_xticks(set_ticks(max_value+1))\n",
    "\n",
    "                # Violin plot on the second subplot\n",
    "\n",
    "                training_status = ''\n",
    "                if before_after=='training':\n",
    "                    training_status = 'Init ' if cm_idx==0 else 'Trained '\n",
    "\n",
    "                if plot_type in ['both', 'violin', 'scatter']:\n",
    "                    if len(layer_range) > 1 and before_after:\n",
    "                        ax1loc = axs[cm_idx*plot_height, lidx] \n",
    "                    elif len(layer_range) > 1:\n",
    "                        ax1loc = axs[lidx]\n",
    "                    else:\n",
    "                        ax1loc = axs[cm_idx*plot_height]\n",
    "                    \n",
    "                    if plot_type == 'violin':\n",
    "                        sns.violinplot(x='True Positions', y='Predicted Positions', \n",
    "                                data=data, ax=ax1loc, width=0.8, linewidth=0.5)\n",
    "                    if plot_type == 'scatter':\n",
    "                        ax1loc.scatter(all_targets, all_outputs, alpha=0.4, s=5)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    ax1loc.set_title(f'{training_status}Layer {layer+1} (r={correlation:.2f}, loss={np.mean(all_test_loss):.2f})')\n",
    "                    ax1loc.set_xlabel('True Positions')\n",
    "                    ax1loc.set_ylabel('Predicted Positions')\n",
    "                    ax1loc.grid(True)\n",
    "                    ax1loc.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                    ax1loc.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "                    # print(max(all_targets))\n",
    "                    extension = int(max(all_targets)*0.1)\n",
    "                    # ax1loc.set_ylim(min(all_targets)-extension, max(all_targets)+extension)\n",
    "                    ax1loc.set_xlim(int(min(all_targets))-extension, int(max(all_targets))+extension)\n",
    "                    ax1loc.plot(all_targets, best_fit_line, 'r', label=f'y={slope:.2f}x+{intercept:.2f}')\n",
    "                    # ax1loc.legend()\n",
    "\n",
    "                    max_value = max(all_targets)\n",
    "                    ax1loc.set_xticks(set_ticks(max_value+1).astype(int))\n",
    "                    max_value = max(all_outputs); min_value = min(all_outputs)\n",
    "                    ax1loc.set_yticks(set_ticks(max_value+1, min_value=int(min_value)).astype(int))\n",
    "                    \n",
    "\n",
    "                if plot_type in ['both', 'tsne']:\n",
    "                    add1 = 1 if plot_type == 'both' else 0\n",
    "\n",
    "                    # ax2loc = axs[cm_idx*plot_height + add1, lidx] if len(layer_range) > 1 else axs[cm_idx*plot_height]\n",
    "                    if len(layer_range) > 1 and before_after:\n",
    "                        ax2loc = axs[cm_idx*plot_height + add1, lidx] \n",
    "                    elif len(layer_range) > 1:\n",
    "                        ax2loc = axs[lidx]\n",
    "                    else:\n",
    "                        ax2loc = axs[cm_idx*plot_height + add1]\n",
    "                    embeddings = test_activation_tensor.detach().cpu().numpy()\n",
    "                    labels = test_positions.detach().cpu().numpy()\n",
    "                    seletion = np.arange(len(labels))[::2]\n",
    "                    embeddings = embeddings[seletion]\n",
    "                    labels = labels[seletion]\n",
    "\n",
    "                    print(embeddings.shape)\n",
    "                    plot_tsne_colored_by_position(embeddings, labels, ax2loc, training_status)\n",
    "\n",
    "\n",
    "                # if before_after_training:\n",
    "                #     annos = 'ab'\n",
    "                #     ax1loc.annotate(f'({annos[cm_idx]})', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=14, fontproperties=prop)\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(left=0.03, right=0.975, top=0.97, bottom=0.07, hspace=0.3)\n",
    "\n",
    "        \n",
    "        if save_all_models or save:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "            fig.savefig(f'{dir_name}/{imgname}.pdf', format='pdf')\n",
    "\n",
    "            plt.close()\n",
    "            del fig, axs, data, probe, optimizer, criterion, dataloader, test_loader\n",
    "            gc.collect()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "widgets.interact(prob_model, \n",
    "                 model_name=useful_name_list, \n",
    "                 model2_name=[None] + useful_name_list,\n",
    "                 save=False,\n",
    "                 before_after_training = False,\n",
    "                 layer_idx=(0, 5), \n",
    "                 plot_all=True, \n",
    "                 rand_perm=True, \n",
    "                 fixed_length=(4,128),\n",
    "                 total_samples=(1000, 5000),\n",
    "                 linear_mode=(0, 5),\n",
    "                 hook_location=['after_attn', 'before_attn', 'mlp', 'raw'],\n",
    "                 plot_type=['violin', 'tsne', 'scatter', 'both'],\n",
    "                 scaler=(0.001, 0.5, 0.001),)\n",
    "\n",
    "                # hook_location=['before_attn', 'after_attn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "1. It is the averaging effect of the softmax attention weights that generates the adjacency pattern.\n",
    "2. It requires the model weights to be with small variance.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the distribution of $ Vx $ when $ x $ and $ V $ are independent and identically distributed (iid) with a normal distribution $ N(\\mu, \\sigma^2) $, where $ x $ is a $ d \\times 1 $ vector and $ V $ is a $ d \\times d $ matrix, we proceed as follows:\n",
    "\n",
    "**Step 1: Understand the Components**\n",
    "- **Vector $ x $**: Each element $ x_i $ of $ x $ is distributed as $ N(\\mu, \\sigma^2) $.\n",
    "- **Matrix $ V $**: Each element $ V_{ij} $ of $ V $ is also distributed as $ N(\\mu, \\sigma^2) $.\n",
    "\n",
    "**Step 2: Multiplication $ Vx $**\n",
    "When $ V $ is multiplied by $ x $, the resulting vector $ y = Vx $ will have each element $ y_i $ given by:\n",
    "$ y_i = \\sum_{j=1}^d V_{ij} x_j $\n",
    "\n",
    "**Step 3: Distribution of Each $ y_i $**\n",
    "Since each $ V_{ij} $ and $ x_j $ are independent and normally distributed, the product $ V_{ij} x_j $ is not normally distributed. However, the sum of these products (i.e., the dot product forming each $ y_i $) could be approximated or analyzed further if $ V $ and $ x $ were linear transformations of Gaussian random variables.\n",
    "\n",
    "**Step 4: Central Limit Theorem (CLT)**\n",
    "When $ d $ (the dimension) is large, the sum of a large number of independent random variables (as is the case in the components of $ y_i $) will tend towards a normal distribution according to the Central Limit Theorem. Thus, each $ y_i $ can be approximated as normally distributed.\n",
    "\n",
    "**Step 5: Calculate Mean and Variance**\n",
    "The mean $ E[y_i] $ is:\n",
    "\n",
    "$ E[y_i] = E\\left[\\sum_{j=1}^d V_{ij} x_j\\right] = \\sum_{j=1}^d E[V_{ij}] E[x_j] = d \\mu^2 $\n",
    "\n",
    "The variance $ \\text{Var}(y_i) $ involves calculating:\n",
    "\n",
    "$ \\text{Var}(y_i) = \\sum_{j=1}^d \\text{Var}(V_{ij} x_j) $\n",
    "\n",
    "Given $ V_{ij} $ and $ x_j $ are independent, each product's variance $ \\text{Var}(V_{ij} x_j) $ will be:\n",
    "\n",
    "$ \\text{Var}(V_{ij} x_j) = E[(V_{ij} x_j)^2] - (E[V_{ij} x_j])^2 $\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$ \\text{Var}(V_{ij} x_j) = E[V_{ij}^2] E[x_j^2] - (E[V_{ij}] E[x_j])^2 $\n",
    "$ \\text{Var}(V_{ij} x_j) = (\\mu^2 + \\sigma^2)^2 - \\mu^4 $\n",
    "\n",
    "Adding these variances together (since the products are independent for different $ j $):\n",
    "\n",
    "$ \\text{Var}(y_i) = d((\\mu^2 + \\sigma^2)^2 - \\mu^4) $\n",
    "\n",
    "**Conclusion**\n",
    "If $ d $ is large enough for the Central Limit Theorem to apply, then each element of the vector $ y = Vx $ is approximately normally distributed, $ y_i \\approx N(d\\mu^2, d((\\mu^2 + \\sigma^2)^2 - \\mu^4)) $. This approach assumes that the elements of $ V $ and $ x $ are iid, and $ d $ is sufficiently large for the CLT approximation. If the matrix and vector dimensions or distributions differ, further adjustments would be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01ce13cd2fb404883350ee1955682a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='seq_len', max=4096, min=10), FloatSlider(value=0.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_attention(seq_len=20, mu_v=0, sig_v=1, mu_A=0, sig_A=1, shift=0, dimension=100, sim_func='cos', dist_type='normal', v_n=0)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# def causal_softmax(x):\n",
    "#     \"\"\"Compute causal softmax values for the vector 'x'.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "#     return e_x / e_x.sum()\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['axes.labelsize'] = 12  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 12  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 12  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 12  # Title\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "def plot_attention(\n",
    "    seq_len = 20,   # Number of vectors\n",
    "    mu_v = 0,\n",
    "    sig_v = 1,\n",
    "    mu_A = 0,\n",
    "    sig_A = 1,\n",
    "    shift = 0,\n",
    "    dimension = 100,  # Dimensionality of each vector\n",
    "    sim_func = 'cos',\n",
    "    dist_type = 'normal',\n",
    "    v_n = 0,\n",
    "):\n",
    "    v_n = min(v_n, seq_len-1)\n",
    "    sim_func_name = 'Dot Product' if sim_func == 'dot' else 'Cosine Similarity'\n",
    "    # Parameters\n",
    "\n",
    "\n",
    "    # Initialize e vectors from index 0 to n-1\n",
    "    v_vectors = np.array([np.random.normal(mu_v, sig_v, dimension) for _ in range(seq_len)])\n",
    "\n",
    "    # Initialize A coefficients from index 0 to n-1 and apply causal softmax\n",
    "    # A = np.zeros((n, n))\n",
    "    # A = np.random.normal(0, np.sqrt(n), (n, n))\n",
    "    if dist_type == 'normal':\n",
    "        A = np.random.normal(mu_A, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'gemma':\n",
    "        A = np.random.gamma(1, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'uniform':\n",
    "        A = np.random.uniform(mu_A, sig_A, (seq_len, seq_len))\n",
    "    elif dist_type == 'possion':\n",
    "        A = np.random.poisson(mu_A, (seq_len, seq_len))\n",
    "\n",
    "    # A = np.random.normal(10, 1, (n, n))\n",
    "    # A = np.random.gamma(0, 0.1, (n, n))\n",
    "    # A = np.random.chisquare(0.001, (n, n))\n",
    "    # A = np.random.uniform(0, 0.01, (n, n))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    _A = np.tril(A, 0)\n",
    "    axes[0].hist(_A[np.where(_A!=0)].flatten(), bins=200)\n",
    "    axes[0].set_title('A distribution before softmax')\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        # A[i, :i+1] = softmax(A[i, :i+1]) + 1 # amazing : )\n",
    "        A[i, :i+1] = softmax(A[i, :i+1]) + shift\n",
    "\n",
    "\n",
    "    A = np.tril(A, 0)\n",
    "\n",
    "    non_zero_A = A[np.where(A!=0)].flatten()\n",
    "    len_A = len(non_zero_A)\n",
    "    sorted_non_zero_A = sorted(non_zero_A)\n",
    "    axes[1].hist(non_zero_A, bins=200)\n",
    "    axes[1].set_title(f'A distribution after softmax (Abs_Avg = {np.abs(sorted_non_zero_A)[:len_A//4].mean():.06f})')\n",
    "\n",
    "    axes[2].plot(A[-1])\n",
    "    axes[2].set_title(f'The last row')\n",
    "\n",
    "    plt.show()\n",
    "    # Compute y vectors using matrix multiplication with a lower triangular matrix of A\n",
    "    y_vectors = A @ v_vectors\n",
    "    norms = np.linalg.norm(y_vectors, axis=1)\n",
    "    print(y_vectors.shape, norms.shape)\n",
    "\n",
    "    # plt.figure(figsize=(6, 5))\n",
    "    fig, axes = plt.subplots(1,2, figsize=(14, 5))\n",
    "    axes[0].hist(y_vectors[v_n], bins=200)\n",
    "    axes[0].set_title(f'Output Vector {v_n+1}, norm={np.linalg.norm(norms[v_n])}')\n",
    "    axes[1].hist(y_vectors[-1], bins=200)\n",
    "    axes[1].set_title(f'Output Vector {len(y_vectors)}, norm={np.linalg.norm(norms[-1])}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Compute cosine similarities using vector operations\n",
    "    # cosine_similarities = np.dot(y_vectors, y_vectors.T) / np.outer(norms, norms)\n",
    "    cosine_similarities = np.dot(y_vectors, y_vectors.T) \n",
    "    divisor = 1 if sim_func=='dot' else np.outer(norms, norms)\n",
    "    if sim_func == 'dot':\n",
    "        cosine_similarities = cosine_similarities / divisor\n",
    "    else:\n",
    "        cosine_similarities = cosine_similarity(y_vectors, y_vectors)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1,2, figsize=(14, 5))\n",
    "    axes[0].plot(cosine_similarities[-1])\n",
    "    axes[0].set_title(f'Last Vector {sim_func_name}')\n",
    "    axes[1].plot(np.outer(norms, norms)[-1])\n",
    "    axes[1].set_title(f'Last Norm Divisor')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Extracting upper triangular part of cosine similarities matrix, excluding diagonal\n",
    "    results = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i + 1, seq_len):\n",
    "            results[i, j] = cosine_similarities[i, j]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    cm = axes[0].imshow(A, cmap='Reds')\n",
    "    axes[0].set_title('Attention Map')\n",
    "    plt.colorbar(cm)\n",
    "\n",
    "    # cm = axes[1].imshow(cosine_similarities, cmap='Reds')\n",
    "    cm = axes[1].imshow(cosine_similarities, )\n",
    "    axes[1].set_title(f'{sim_func_name} Matrix')\n",
    "    plt.colorbar(cm)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "interact(plot_attention, \n",
    "         seq_len=(10, 4096), \n",
    "         mu_v = (-10, 10, 0.01), \n",
    "         sig_v = (0, 2, 0.0001),\n",
    "         mu_A = (-10, 10, 0.01), \n",
    "         sig_A = (0, 2, 0.0001),  \n",
    "         shift=(-2, 2, 0.001), \n",
    "         dimension=(4, 384),\n",
    "         sim_func = ['cos', 'dot'],\n",
    "         dist_type = ['normal', 'gemma', 'uniform', 'possion'],\n",
    "         v_n = (0, 2048)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff286f7eef644528cce98a657e63927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_name', options=('acc=0_add3_ref_original_sd240_T2406121801/c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_att(model_name, layer_idx=0, plot_all=0, save_map=False, find_best=True, search_all_heads=False, metric_id=4, bestk=4, clean_plot=False, input_name=['eqx', 'randx', 'sample_1', 'sample_2', 'sample_3'], amp_head=2)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the difference in projection for the same vector\n",
    "# x = torch.rand(1, 1, 384).to(device)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from ast import literal_eval\n",
    "import ipywidgets as widgets\n",
    "\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "from matplotlib import font_manager\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['axes.labelsize'] = 12  # Axis labels\n",
    "plt.rcParams['xtick.labelsize'] = 12  # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 12  # Legend\n",
    "plt.rcParams['axes.titlesize'] = 12  # Title\n",
    "\n",
    "n_embd = 384\n",
    "sz = 64\n",
    "x = torch.zeros(1, sz, n_embd).to(device)\n",
    "sc = torch.rand(1, sz, n_embd).to(device)\n",
    "\n",
    "# idx = np.random.randint(0, n_embd)\n",
    "idx = np.random.randint(0, n_embd, size=(sz))\n",
    "\n",
    "x[0, :, idx] = 1\n",
    "# x += sc\n",
    "x = np.random.normal(0, 0.01, (1, sz, n_embd))\n",
    "x = torch.tensor(x).float().to(device)\n",
    "\n",
    "# y = model_list[17].transformer.h[1].attn.c_attn(x)\n",
    "# input_idx = torch.tensor(encode('12modp(123)=')).to(device)[None,...]\n",
    "# input_idx = torch.tensor(encode('$123+45=')).to(device)[None,...]\n",
    "input_idx = torch.tensor(encode('$333+444=777$\\n$123+54=')).to(device)[None,...]\n",
    "input_idx2 = torch.tensor(encode('$123+54=')).to(device)[None,...]\n",
    "input_idx3 = torch.tensor(encode('$992+299=')).to(device)[None,...]\n",
    "\n",
    "eqx = torch.tensor(model.create_equal_distancing_vecotrs(sz, n_embd, small_component=0.01)[0]).to(device).to(torch.float32)[None,...] * 0.01\n",
    "\n",
    "inputs_dict = {\n",
    "    'sample_1': input_idx,\n",
    "    'sample_2': input_idx2,\n",
    "    'sample_3': input_idx3,\n",
    "    'eqx': eqx,\n",
    "    'randx': x,\n",
    "} \n",
    "\n",
    "# Specify the path to your Times New Roman font file\n",
    "font_path = './timr45w.ttf'  # Update this path\n",
    "# Add the font to Matplotlib's font manager\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "def get_PE_tendency(mat):\n",
    "    r = 0\n",
    "    r2 = 0\n",
    "    r3 = 0\n",
    "    r4 = 0\n",
    "    r5 = 0\n",
    "    r5_counts = 0\n",
    "    r6 = 0\n",
    "    r6_count = 0\n",
    "    mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "    for lidx, layer in enumerate(mat[1:]):\n",
    "        r += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "        r3 += (np.diff(layer[:lidx+1], axis=0) > 0).sum()\n",
    "        r3 -= (np.diff(layer[:lidx+1], axis=0) <= 0).sum()\n",
    "\n",
    "        c2p = sum([((layer[j+1:lidx+1]-layer[j]) > 0).sum() for j in range(lidx+1)])\n",
    "        c2m = sum([((layer[j+1:lidx+1]-layer[j]) <= 0).sum() for j in range(lidx+1)])\n",
    "        r2 += c2p\n",
    "        r4 += c2p - c2m\n",
    "\n",
    "        valid_layer = layer[:lidx+1]\n",
    "        if len(valid_layer)<3: \n",
    "            continue\n",
    "\n",
    "        # cur_order = valid_layer\n",
    "        # original_order = valid_layer[np.argsort(valid_layer)]\n",
    "        # corr = spearmanr(original_order, cur_order).correlation\n",
    "        # if np.isnan(corr):\n",
    "            # continue\n",
    "        # r5 += spearmanr(original_order, cur_order).correlation \n",
    "\n",
    "\n",
    "        right_order = np.argsort(valid_layer-np.arange(len(valid_layer))*1e-5)\n",
    "        right_right_order = np.argsort(right_order)\n",
    "        original_order = np.arange(len(valid_layer))\n",
    "        edit_distance = levenshteinDistance(right_order, original_order)\n",
    "        r5 += edit_distance\n",
    "        r5_counts += len(valid_layer)\n",
    "        r_value = pearsonr(original_order, right_right_order)[0]\n",
    "        if np.isnan(r_value): continue\n",
    "        r6 += pearsonr(original_order, right_right_order)[0]\n",
    "        r6_count += 1\n",
    "        # r5 += pearsonr(original_order, cur_order)[0]\n",
    "\n",
    "    max_r = np.arange(mat.shape[0]-1).sum()\n",
    "    t1 = round(r/max_r, 2)\n",
    "\n",
    "    max_r2 = sum([ np.arange(n+1).sum() for n in np.arange(mat.shape[0]-1)])\n",
    "    t2 = round(r2/max_r2, 2)\n",
    "\n",
    "    t3 = round(r3/max_r, 2)\n",
    "\n",
    "    t4 = round(r4/max_r2, 2)\n",
    "    \n",
    "    t5 =  round((r5_counts-r5) / r5_counts, 2)\n",
    "\n",
    "    t6 = round(r6/r6_count, 2)\n",
    "    return f'({t1},{t2},{t3},{t4},{t5},{t6})'\n",
    "\n",
    "\n",
    "def generate_tendency_map(mat):\n",
    "    mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "    empty_mat = np.zeros_like(mat)\n",
    "    for lidx, layer in enumerate(mat):\n",
    "        if lidx == 0:\n",
    "            continue\n",
    "        counter = 0\n",
    "        for j in range(1, lidx+1):\n",
    "            if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            empty_mat[lidx, j] = counter\n",
    "    \n",
    "    return empty_mat\n",
    "\n",
    "def generate_tendency_map(mat):\n",
    "    mat = mat.detach().cpu().numpy() if isinstance(mat, torch.Tensor) else mat\n",
    "    empty_mat = np.zeros_like(mat)\n",
    "    for lidx, layer in enumerate(mat):\n",
    "      \n",
    "        counter = 0\n",
    "        for j in range(0, lidx+1):\n",
    "            if layer[j]<=layer[j-1]: # weird but worked ...\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            empty_mat[lidx, j] = counter\n",
    "            # empty_mat[lidx, j] = np.log(mat[lidx, j])\n",
    "    \n",
    "    return empty_mat\n",
    "\n",
    "    \n",
    "\n",
    "def plot_att(model_name, \n",
    "             layer_idx=0, \n",
    "             plot_all=0, \n",
    "             save_map=False, \n",
    "             find_best=True, \n",
    "             search_all_heads=False,\n",
    "             metric_id = 4,\n",
    "             bestk = 4,\n",
    "             clean_plot = False,\n",
    "             input_name = sorted(list(inputs_dict.keys())), \n",
    "             amp_head=2,):\n",
    "\n",
    "    acc = model_name.split('_')[0]\n",
    "    rest = '_'.join(model_name.split('_')[1:])\n",
    "    imgname = rest.replace('10000_acc_', '').replace('/', '_').replace('.pt', '') + '_' + acc\n",
    "\n",
    "    # cur_model = model_list[12]\n",
    "    # cur_model = model_list[2]\n",
    "    model_idx = useful_name_list.index(model_name)\n",
    "    cur_model = model_list[model_idx]\n",
    "    \n",
    "    layer_range = range(layer_idx, layer_idx+1) if plot_all==0 else range(plot_all)\n",
    "    \n",
    "    if len(layer_range) > len(cur_model.transformer.h):\n",
    "        layer_range = range(len(cur_model.transformer.h))\n",
    "\n",
    "    for level in layer_range:\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        h1 = cur_model.transformer.h[level].attn.c_attn.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}\")\n",
    "        )\n",
    "        \n",
    "        h1q = cur_model.transformer.h[level].attn.iq.register_forward_hook(\n",
    "            getActivation(f\"q{level}\")\n",
    "        ) \n",
    "        h1k = cur_model.transformer.h[level].attn.ik.register_forward_hook(\n",
    "            getActivation(f\"k{level}\")\n",
    "        ) \n",
    "        h1v = cur_model.transformer.h[level].attn.iv.register_forward_hook(\n",
    "            getActivation(f\"v{level}\")\n",
    "        ) \n",
    "\n",
    "        h2 = cur_model.transformer.h[level].attn.identity.register_forward_hook(\n",
    "            getActivation(f\"layer_{level}_iden\")\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_values = inputs_dict[input_name].cuda()\n",
    "            cur_model = cur_model.cuda()\n",
    "            if 'sample' in input_name:\n",
    "                out = cur_model(input_values)\n",
    "                y = decode([out[0].detach().cpu().numpy().argmax()])\n",
    "                print(y)\n",
    "            else: # eqx or randx\n",
    "                _ = cur_model(input_values, direct_input_modification=True)\n",
    "            cur_model = cur_model.cpu()\n",
    "      \n",
    "\n",
    "        h1.remove()\n",
    "        h1q.remove()\n",
    "        h1k.remove()\n",
    "        h1v.remove()\n",
    "        h2.remove()\n",
    "\n",
    "\n",
    "        '''Plot the attention maps'''\n",
    "        xw = activation[f\"layer_{level}\"]\n",
    "        q, k, v  = xw.split(n_embd, dim=2)\n",
    "        q_reshape = activation[f\"q{level}\"]\n",
    "        k_reshape = activation[f\"k{level}\"]\n",
    "        v_reshape = activation[f\"v{level}\"]\n",
    "\n",
    "        # plt.figure(figsize=(12, 3))\n",
    "        k_numpy = k_reshape.detach().cpu().numpy()\n",
    "        k_numpy = k_numpy.transpose(0, 2, 1, 3)\n",
    "        bs, T, n_head, head_dim = k_numpy.shape\n",
    "        k_numpy = k_numpy.reshape(bs, T, -1)\n",
    "        print(k_numpy.shape)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "        axes[0].plot(k_numpy[:, 0].flatten())\n",
    "        axes[0].set_title('key 0')\n",
    "        axes[1].hist(k_numpy[:, 0].flatten(), bins=300)\n",
    "        axes[1].set_title('key 0 distribution')\n",
    "        # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        # plt.show()\n",
    "\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "        k_last = k_numpy[:, -1].flatten()\n",
    "        axes[2].plot(k_last)\n",
    "        axes[2].set_title(f'key vector {T}')\n",
    "        axes[3].hist(k_last, bins=300)\n",
    "        axes[3].set_title(f'key vector {T} distribution')\n",
    "        # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        plt.show()\n",
    "\n",
    "        v_numpy = v_reshape.detach().cpu().numpy()\n",
    "        v_numpy = v_numpy.transpose(0, 2, 1, 3)\n",
    "        bs, T, n_head, head_dim = v_numpy.shape\n",
    "        v_numpy = v_numpy.reshape(bs, T, -1)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "        axes[0].plot(v_numpy[:, 0].flatten())\n",
    "        axes[0].set_title('value 0')\n",
    "        axes[1].hist(v_numpy[:, 0].flatten(), bins=300)\n",
    "        axes[1].set_title('value 0 distribution')\n",
    "        # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        # plt.show()\n",
    "\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "        v_last = v_numpy[:, -1].flatten()\n",
    "        axes[2].plot(v_last)\n",
    "        axes[2].set_title(f'value vector {T}')\n",
    "        axes[3].hist(v_last, bins=300)\n",
    "        axes[3].set_title(f'value vector {T} distribution')\n",
    "        # plt.plot(k.detach().cpu().numpy().flatten()-1)\n",
    "        plt.show()\n",
    "\n",
    "        from itertools import combinations\n",
    "        head_dim = n_embd//cur_model.config.n_head\n",
    "        idx_comb = [np.arange(head_dim)] + [[i] for i in range(head_dim)]\n",
    "\n",
    "        n_arrays = cur_model.config.n_head\n",
    "        all_records = [[] for _ in range(n_arrays)] \n",
    "        all_maps = [[] for _ in range(n_arrays)] \n",
    "        all_idxsets = [[] for _ in range(n_arrays)] \n",
    "        original_score = []\n",
    "        head2 = []\n",
    "        s_head2 = []\n",
    "        sim_head2 = []\n",
    "\n",
    "        run_search = True\n",
    "        # for cur_idxset in tqdm(idx_comb):\n",
    "        for cur_idxset in idx_comb[:1]:\n",
    "\n",
    "            if not run_search:\n",
    "                break\n",
    "            if not find_best:\n",
    "                run_search = False\n",
    "                cur_idxset = range(head_dim)\n",
    "\n",
    "            if not run_search:\n",
    "                fig, ax = plt.subplots(4, cur_model.config.n_head, figsize=(18, 10))\n",
    "\n",
    "            for hidx in range(cur_model.config.n_head):\n",
    "                \n",
    "                \n",
    "                attmap = q_reshape[0, hidx, :, cur_idxset].cuda()@k_reshape[0, hidx, :, cur_idxset].cuda().transpose(0,1)\n",
    "                mask = np.triu(np.ones_like(attmap.detach().cpu().numpy(), dtype=bool), 1)\n",
    "                mask = torch.tensor(mask).to(device)\n",
    "\n",
    "                # digit = attmap.flatten()[0].detach().cpu().clone()\n",
    "                attmap[mask] = -np.inf \n",
    "                # T = attmap.shape[-1]\n",
    "                # attmap[np.arange(T), np.arange(T)] = digit\n",
    "\n",
    "                attmap_np = attmap.detach().cpu().numpy()\n",
    "                attmap_tend = get_PE_tendency(attmap_np)\n",
    "\n",
    " \n",
    "                \n",
    "                s_attmap = torch.nn.Softmax(dim=-1)(attmap)\n",
    "                # calc = torch.ones_like(calc) * calc.mean()\n",
    "                s_attmap_np = s_attmap.detach().cpu().numpy()\n",
    "                tendency_map = generate_tendency_map(s_attmap_np)\n",
    "                mat_tend = get_PE_tendency(s_attmap_np)\n",
    "\n",
    "                y_out = s_attmap.cpu() @ v_reshape[0, hidx, :, cur_idxset].cpu()\n",
    "                sim_map = cosine_similarity(y_out, y_out)\n",
    "\n",
    "\n",
    "                if hidx == amp_head or (search_all_heads and find_best):\n",
    "                    if find_best:\n",
    "                        adj_score = literal_eval(attmap_tend)[metric_id]\n",
    "\n",
    "                        \n",
    "                        if len(cur_idxset) == head_dim:\n",
    "                            original_score.append(adj_score)\n",
    "                            head2.append(attmap_np)\n",
    "                            s_head2.append(s_attmap_np)\n",
    "                            sim_head2.append(sim_map)\n",
    "                        else:\n",
    "                            all_records[hidx].append(adj_score)\n",
    "                            all_maps[hidx].append(attmap_np)\n",
    "                            all_idxsets[hidx].append(cur_idxset)\n",
    "                    else:\n",
    "                        head2.append(attmap_np) \n",
    "                        s_head2.append(s_attmap_np)\n",
    "                        sim_head2.append(sim_map)\n",
    "\n",
    "\n",
    "\n",
    "                if not run_search: \n",
    "                    mat = ax[0, hidx].imshow(attmap_np, cmap='Reds', interpolation='nearest')\n",
    "                    ax[0, hidx].set_title(f'head {hidx}={attmap_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[0, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[1, hidx].imshow(s_attmap_np, cmap='Reds', interpolation='nearest')\n",
    "                    ax[1, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[1, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[2, hidx].imshow(tendency_map, cmap='Reds', interpolation='nearest')\n",
    "                    ax[2, hidx].set_title(f'head {hidx}={mat_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[2, hidx], orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                    mat = ax[3, hidx].imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "                    sim_tend = get_PE_tendency(sim_map)\n",
    "\n",
    "                    ax[3, hidx].set_title(f'head {hidx}={sim_tend}')\n",
    "                    plt.colorbar(mat, ax=ax[3, hidx], orientation='vertical', fraction=0.06,)\n",
    "\n",
    "            if not run_search: \n",
    "                plt.subplots_adjust(hspace=.4)\n",
    "                fig.suptitle(f'{input_name}_{imgname}_layer={level+1}', fontsize=16, y=0.96)\n",
    "                if save_map:\n",
    "                    os.makedirs(f'./saved_heads_plots/', exist_ok=True)\n",
    "                    fig.savefig(f'./saved_heads_plots/{input_name}_{imgname}_layer={level+1}.svg')\n",
    "                plt.show()\n",
    "\n",
    "        if find_best:\n",
    "            fig, ax = plt.subplots(7, len(head2), figsize=(3.5*len(head2), 23))\n",
    "            for amidx, attnmap_np in enumerate(head2):\n",
    "                ax_loc_0 = ax[0, amidx] if len(head2)>1 else ax[0]\n",
    "                ax_loc_1 = ax[1, amidx] if len(head2)>1 else ax[1]\n",
    "                ax_loc_2 = ax[2, amidx] if len(head2)>1 else ax[2]\n",
    "                ax_loc_3 = ax[3, amidx] if len(head2)>1 else ax[3]\n",
    "                ax_loc_4 = ax[4, amidx] if len(head2)>1 else ax[4]\n",
    "                ax_loc_5 = ax[5, amidx] if len(head2)>1 else ax[5]\n",
    "                ax_loc_6 = ax[6, amidx] if len(head2)>1 else ax[6]\n",
    "\n",
    "\n",
    "                mat = ax_loc_0.imshow(attnmap_np, cmap='Reds', interpolation='nearest')\n",
    "                head2_PE_scores = get_PE_tendency(attnmap_np)\n",
    "                ax_loc_0.set_title(f'head {amidx}={head2_PE_scores}')\n",
    "                plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                tendmap_head2 = generate_tendency_map(attnmap_np)\n",
    "                mat = ax_loc_1.imshow(tendmap_head2, cmap='Reds', interpolation='nearest')\n",
    "                ax_loc_1.set_title(f'head {amidx}={original_score[amidx]}')\n",
    "                plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "                lower_tril_attnmap_np = np.tril(attnmap_np, 0)\n",
    "                \n",
    "                n_zeros = (lower_tril_attnmap_np<0).sum()\n",
    "                ax_loc_2.hist(lower_tril_attnmap_np.flatten(), bins=100)\n",
    "                ax_loc_2.set_title(f'head {amp_head} raw attention distribution ({n_zeros})')\n",
    "                # ax_loc_2.set_xlim(-0.01, 0.01)\n",
    "                \n",
    "                ax_loc_3.plot(lower_tril_attnmap_np[-1])\n",
    "                ax_loc_3.set_title(f'head {amidx} last row')\n",
    "\n",
    "                s_attmap_np = s_head2[amidx]\n",
    "                \n",
    "                lower_tril_s_attnmap_np = np.tril(s_attmap_np,0)\n",
    "                n_zeros = (lower_tril_s_attnmap_np<0).sum()\n",
    "                ax_loc_4.hist(lower_tril_s_attnmap_np.flatten(), bins=100)\n",
    "                # zero_map = np.zeros_like(s_attmap_np)\n",
    "                # zero_map[np.where(s_attmap_np==0)] = 1\n",
    "                cm = ax_loc_4.imshow(s_attmap_np)\n",
    "                plt.colorbar(cm, ax=ax_loc_4, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "\n",
    "                ax_loc_4.set_title(f'head {amidx} softmaxed attention distribution ({n_zeros})')\n",
    "\n",
    "                ax_loc_5.plot(lower_tril_s_attnmap_np[-1])\n",
    "                ax_loc_5.set_title(f'head {amidx} last row')\n",
    "\n",
    "                sim_map = sim_head2[amidx]\n",
    "                mat = ax_loc_6.imshow(sim_map, cmap='Reds', interpolation='nearest')\n",
    "                ax_loc_6.set_title('cosine_similarity')\n",
    "                plt.colorbar(mat, ax=ax_loc_6, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            # fig, ax = plt.subplots(2, len(head2), figsize=(3.5*len(head2), 6))\n",
    "\n",
    "            # for hridx, head_rec in enumerate(all_records):\n",
    "\n",
    "            #     if len(head_rec) == 0: \n",
    "            #         continue\n",
    "\n",
    "            #     topk_records_idx = np.argsort(head_rec)[-bestk:]\n",
    "\n",
    "            #     best_maps = np.array([all_maps[hridx][i] for i in topk_records_idx])\n",
    "            #     best_map = best_maps.mean(axis=0)\n",
    "            #     best_score = np.mean([head_rec[i] for i in topk_records_idx])\n",
    "            #     best_idxcomb = [all_idxsets[hridx][i] for i in topk_records_idx]\n",
    "\n",
    "\n",
    "            #     ax_loc_0 = ax[0, hridx] if len(head2)>1 else ax[0]\n",
    "            #     ax_loc_1 = ax[1, hridx] if len(head2)>1 else ax[1]\n",
    "\n",
    "            #     mat = ax_loc_0.imshow(best_map, cmap='Reds', interpolation='nearest')\n",
    "            #     # best_PE_scores = get_PE_tendency(best_map)\n",
    "            #     ax_loc_0.set_title(f'best avg={best_score:.02f}')\n",
    "            #     plt.colorbar(mat, ax=ax_loc_0, orientation='vertical', fraction=0.06, )\n",
    "\n",
    "            #     tendmap_best = generate_tendency_map(best_map)\n",
    "            #     mat = ax_loc_1.imshow(tendmap_best, cmap='Reds', interpolation='nearest')\n",
    "            #     ax_loc_1.set_title(f'best ={best_idxcomb}')\n",
    "            #     plt.colorbar(mat, ax=ax_loc_1, orientation='vertical', fraction=0.06, )\n",
    "            \n",
    "            plot_name = f'bestk={bestk}_{input_name}_{imgname}_layer={level+1}'\n",
    "            # fig.suptitle(plot_name, fontsize=16, y=0.96)\n",
    "            if save_map:\n",
    "                os.makedirs(f'./saved_dist_heads_plots/', exist_ok=True)\n",
    "                fig.savefig(f'./saved_dist_heads_plots/{plot_name}.svg')\n",
    "            plt.show()\n",
    "\n",
    "widgets.interact(plot_att, \n",
    "                 model_name=useful_name_list, \n",
    "                 layer_idx=(0, 11), \n",
    "                 plot_all=(0, 6), \n",
    "                 amp_head=(0, 5),\n",
    "                 metric_id = (0, 5),\n",
    "                 bestk=(1, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of $y_n$ and the distribution of the dot product $y_n \\cdot y_i$ for a sequence of iid normal vectors $x_0, x_1, \\ldots, x_n$ can be derived through the properties of normal distributions and linear transformations of these distributions.\n",
    "\n",
    "**Distribution of $y_n$**\n",
    "\n",
    "1. **Distribution of Each $x_i$**:\n",
    "   Assume each $x_i$ is a vector of independent and identically distributed (iid) standard normal random variables, say $x_i \\sim N(0, I)$, where $I$ is the identity matrix. This implies that each component of $x_i$ is $N(0,1)$.\n",
    "\n",
    "2. **Sum of Normal Vectors**:\n",
    "   The sum $S_k = x_0 + x_1 + \\ldots + x_k$ of iid normal vectors is also normally distributed. The mean of $S_k$ will be the sum of the means of each $x_i$, which is $0$, and the covariance matrix will be $(k+1)I$ because the covariance matrix of each $x_i$ is $I$.\n",
    "\n",
    "3. **Mean of Normal Vectors**:\n",
    "   $y_n = \\frac{1}{n+1} S_n = \\frac{1}{n+1}(x_0 + x_1 + \\ldots + x_n)$ is also normally distributed as a linear transformation of normal vectors. The mean of $y_n$ remains $0$, and the covariance matrix is scaled by $\\frac{1}{(n+1)^2}$ of $S_n$'s covariance, resulting in $\\frac{1}{n+1} I$.\n",
    "\n",
    "   Hence, $y_n \\sim N(0, \\frac{1}{n+1} I)$.\n",
    "\n",
    "**Distribution of $y_n \\cdot y_i$**\n",
    "\n",
    "1. **Vectors Involved**:\n",
    "   We know $y_i = \\frac{1}{i+1} S_i$ for $i \\leq n$. Thus, $y_i$ and $y_n$ are both linear transformations of sums of the normal vectors $x_0, \\ldots, x_i$ and $x_0, \\ldots, x_n$ respectively.\n",
    "\n",
    "2. **Dot Product**:\n",
    "   The dot product $y_n \\cdot y_i$ is a bilinear form of the Gaussian vectors. This product $y_n \\cdot y_i$ is a scalar random variable.\n",
    "\n",
    "   The expected value of $y_n \\cdot y_i$ can be computed considering $E[y_n \\cdot y_i] = \\frac{1}{(n+1)(i+1)} E[S_n \\cdot S_i]$. Since $S_i$ is a part of $S_n$ when $i \\leq n$, we have $E[S_n \\cdot S_i] = (i+1)$, hence $E[y_n \\cdot y_i] = \\frac{i+1}{(n+1)(i+1)} = \\frac{1}{n+1}$.\n",
    "\n",
    "3. **Distribution**:\n",
    "   Being a linear combination of Gaussian variables, $y_n \\cdot y_i$ is itself Gaussian. Its variance needs further calculation, usually involving expanding $(y_n \\cdot y_i)^2$ and using independence and variances of components of $y_n$ and $y_i$.\n",
    "\n",
    "In summary, $y_n$ is normally distributed with mean zero and covariance matrix $\\frac{1}{n+1} I$, and $y_n \\cdot y_i$ is a normally distributed scalar with mean $\\frac{1}{n+1}$ and a variance that needs further calculation involving their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4d422e0035404bb26972252cdadcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfMerger \n",
    "import cairosvg\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def convert_svg_to_pdf(svg_files, output_pdf):\n",
    "    temp_pdfs = []\n",
    "\n",
    "    # Convert each SVG to a temporary PDF\n",
    "    for svg_file in svg_files:\n",
    "        pdf_file = f\"{svg_file}.pdf\"\n",
    "        cairosvg.svg2pdf(url=svg_file, write_to=pdf_file)\n",
    "        temp_pdfs.append(pdf_file)\n",
    "\n",
    "    # Merge all temporary PDFs into a single PDF\n",
    "    merger = PdfMerger ()\n",
    "    for pdf in temp_pdfs:\n",
    "        merger.append(pdf)\n",
    "\n",
    "    # Write out the final merged PDF\n",
    "    merger.write(output_pdf)\n",
    "    merger.close()\n",
    "\n",
    "    # Clean up temporary PDF files\n",
    "    for pdf in temp_pdfs:\n",
    "        os.remove(pdf)\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "root_dir = './saved_dist_heads_plots'\n",
    "svg_files = sorted(glob.glob(root_dir + '/*.svg'))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "f_dict = {}\n",
    "for fpath in svg_files:\n",
    "    fname = fpath.split('/')[-1]\n",
    "    model_name = '_'.join(fname.split('_')[:-1])\n",
    "    if model_name not in f_dict:\n",
    "        f_dict[model_name] = []\n",
    "    f_dict[model_name].append(fpath)\n",
    "\n",
    "for model_name in tqdm(f_dict):\n",
    "    sorted_files = sorted(f_dict[model_name])\n",
    "    os.makedirs(f'{root_dir}_pdf', exist_ok=True)\n",
    "    output_pdf = f'{root_dir}_pdf/{model_name}.pdf'\n",
    "    convert_svg_to_pdf(sorted_files, output_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, bidirectional=True, num_buckets=32, max_distance=128, n_heads=2):\n",
    "        super(RelativePositionBias, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.n_heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        \"\"\"\n",
    "        Adapted from Mesh Tensorflow:\n",
    "        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "        Translate relative position to a bucket number for relative attention.\n",
    "        The relative position is defined as memory_position - query_position, i.e.\n",
    "        the distance in tokens from the attending position to the attended-to\n",
    "        position.  If bidirectional=False, then positive relative positions are\n",
    "        invalid.\n",
    "        We use smaller buckets for small absolute relative_position and larger buckets\n",
    "        for larger absolute relative_positions.  All relative positions >=max_distance\n",
    "        map to the same bucket.  All relative positions <=-max_distance map to the\n",
    "        same bucket.  This should allow for more graceful generalization to longer\n",
    "        sequences than the model has been trained on.\n",
    "        Args:\n",
    "            relative_position: an int32 Tensor\n",
    "            bidirectional: a boolean - whether the attention is bidirectional\n",
    "            num_buckets: an integer\n",
    "            max_distance: an integer\n",
    "        Returns:\n",
    "            a Tensor with the same shape as relative_position, containing int32\n",
    "            values in the range [0, num_buckets)\n",
    "        \"\"\"\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            ret += (n < 0).to(torch.long) * num_buckets  # mtf.to_int32(mtf.less(n, 0)) * num_buckets\n",
    "            n = torch.abs(n)\n",
    "        else:\n",
    "            n = torch.max(n, torch.zeros_like(n))\n",
    "        # now n is in the range [0, inf)\n",
    "\n",
    "        # half of the buckets are for exact increments in positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def compute_bias(self, qlen, klen):\n",
    "        \"\"\" Compute binned relative position bias \"\"\"\n",
    "        context_position = torch.arange(qlen, dtype=torch.long,\n",
    "                                        device=self.relative_attention_bias.weight.device)[:, None]\n",
    "        memory_position = torch.arange(klen, dtype=torch.long,\n",
    "                                       device=self.relative_attention_bias.weight.device)[None, :]\n",
    "        relative_position = memory_position - context_position  # shape (qlen, klen)\n",
    "        \"\"\"\n",
    "                   k\n",
    "             0   1   2   3\n",
    "        q   -1   0   1   2\n",
    "            -2  -1   0   1\n",
    "            -3  -2  -1   0\n",
    "        \"\"\"\n",
    "        rp_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (qlen, klen)\n",
    "            bidirectional=self.bidirectional,\n",
    "            num_buckets=self.num_buckets,\n",
    "        )\n",
    "        rp_bucket = rp_bucket.to(self.relative_attention_bias.weight.device)\n",
    "        values = self.relative_attention_bias(rp_bucket)  # shape (qlen, klen, num_heads)\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, qlen, klen)\n",
    "        return values\n",
    "\n",
    "    def forward(self, qlen, klen):\n",
    "        return self.compute_bias(qlen, klen)  # shape (1, num_heads, qlen, klen)\n",
    "\n",
    "\n",
    "rb = RelativePositionBias(False, num_buckets=32, max_distance=20, n_heads=1)\n",
    "print(rb(256,256).shape)\n",
    "plt.imshow(rb(256,256)[0,0].detach())\n",
    "plt.colorbar( orientation='vertical', fraction=0.06, pad=0.04,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, bidirectional=True, num_buckets=32, max_distance=128, n_heads=2):\n",
    "        super(RelativePositionBias, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.n_heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        \"\"\"\n",
    "        Adapted from Mesh Tensorflow:\n",
    "        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "\n",
    "        Translate relative position to a bucket number for relative attention. The relative position is defined as\n",
    "        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n",
    "        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n",
    "        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n",
    "        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n",
    "        This should allow for more graceful generalization to longer sequences than the model has been trained on\n",
    "\n",
    "        Args:\n",
    "            relative_position: an int32 Tensor\n",
    "            bidirectional: a boolean - whether the attention is bidirectional\n",
    "            num_buckets: an integer\n",
    "            max_distance: an integer\n",
    "\n",
    "        Returns:\n",
    "            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n",
    "        \"\"\"\n",
    "        relative_buckets = 0\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n",
    "            relative_position = torch.abs(relative_position)\n",
    "        else:\n",
    "            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))\n",
    "        # now relative_position is in the range [0, inf)\n",
    "\n",
    "        # half of the buckets are for exact increments in positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = relative_position < max_exact\n",
    "\n",
    "        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "        relative_postion_if_large = max_exact + (\n",
    "            torch.log(relative_position.float() / max_exact)\n",
    "            / math.log(max_distance / max_exact)\n",
    "            * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        relative_postion_if_large = torch.min(\n",
    "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
    "        )\n",
    "\n",
    "        relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)\n",
    "        return relative_buckets\n",
    "\n",
    "    def compute_bias(self, query_length, key_length):\n",
    "        \"\"\"Compute binned relative position bias\"\"\"\n",
    "        context_position = torch.arange(\n",
    "            query_length, dtype=torch.long, device=self.relative_attention_bias.weight.device\n",
    "        )[:, None]\n",
    "        memory_position = torch.arange(\n",
    "            key_length, dtype=torch.long, device=self.relative_attention_bias.weight.device\n",
    "        )[None, :]\n",
    "        relative_position = memory_position - context_position  # shape (query_length, key_length)\n",
    "        relative_position_bucket = self._relative_position_bucket(\n",
    "            relative_position,  # shape (query_length, key_length)\n",
    "            bidirectional=self.bidirectional,\n",
    "            num_buckets=self.num_buckets,\n",
    "        )\n",
    "        values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n",
    "        return values\n",
    "\n",
    "    def forward(self, qlen, klen):\n",
    "        return self.compute_bias(qlen, klen)  # shape (1, num_heads, qlen, klen)\n",
    "    \n",
    "rb = RelativePositionBias(False, num_buckets=32, max_distance=20, n_heads=1)\n",
    "print(rb(256,256).shape)\n",
    "plt.imshow(rb(256,256)[0,0].detach())\n",
    "plt.colorbar( orientation='vertical', fraction=0.06, pad=0.04,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "T = 10\n",
    "\n",
    "attn_mask = torch.ones(1, 1, T, T, dtype=torch.bool)\n",
    "# attn_mask = rb(T,T).detach().cpu()\n",
    "temp_mask = torch.ones(1, 1, T, T, dtype=torch.bool).tril(diagonal=0)\n",
    "diag_mask = torch.diag_embed(torch.ones(T, dtype=torch.bool)).unsqueeze(0).unsqueeze(0)\n",
    "plt.imshow(temp_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(diag_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(attn_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# attn_mask = attn_mask.masked_fill(temp_mask == 0, float('-inf'))\n",
    "attn_mask = attn_mask & temp_mask\n",
    "\n",
    "plt.imshow(attn_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "attn_mask = attn_mask | diag_mask\n",
    "\n",
    "plt.imshow(attn_mask[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight distributuion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kqv_weights = model.transformer.h[0].attn.c_attn.weight.detach().cpu().numpy()\n",
    "q_weights = kqv_weights[:kqv_weights.shape[0]//3]\n",
    "k_weights = kqv_weights[kqv_weights.shape[0]//3:2*kqv_weights.shape[0]//3]\n",
    "v_weights = kqv_weights[2*kqv_weights.shape[0]//3:]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(k_weights.flatten(), bins=100)\n",
    "plt.show()\n",
    "plt.hist(v_weights.flatten(), bins=100)\n",
    "plt.show()\n",
    "plt.hist(q_weights.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, dists = GPT_nope.create_equal_distancing_vecotrs(12, 384,)\n",
    "dists[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvecs = vecs@k_weights\n",
    "kdists = kvecs@kvecs.T\n",
    "vvecs = vecs@v_weights\n",
    "vdists = vvecs@vvecs.T\n",
    "qvecs = vecs@q_weights\n",
    "qdists = qvecs@qvecs.T\n",
    "# subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].imshow(kdists, cmap='coolwarm', interpolation='nearest')\n",
    "axs[0].set_title('K')\n",
    "axs[1].imshow(vdists, cmap='coolwarm', interpolation='nearest')\n",
    "axs[1].set_title('V')\n",
    "axs[2].imshow(qdists, cmap='coolwarm', interpolation='nearest')\n",
    "axs[2].set_title('Q')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = kdists.shape[0]\n",
    "bias = torch.tril(torch.ones(T, T)).numpy()\n",
    "kvmap = qvecs@kvecs.T\n",
    "kvmap_causal = kvmap.copy()\n",
    "kvmap_causal[bias==0] = 0\n",
    "\n",
    "plt.imshow(kvmap_causal, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvmap_softmax = kvmap.copy()\n",
    "kvmap_softmax[bias==0] = -np.inf\n",
    "kvmap_softmax = np.exp(kvmap_softmax)\n",
    "kvmap_softmax /= kvmap_softmax.sum(axis=1, keepdims=True)\n",
    "plt.imshow(kvmap_softmax, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "for hidx in range(kvmap_softmax.shape[0]):\n",
    "    for j in range(kvmap_softmax.shape[1]):\n",
    "        plt.text(j, hidx, f'{kvmap_softmax[hidx, j]:.02f}', ha='center', va='center', color='black', fontsize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_out = kvmap_softmax@vvecs\n",
    "self_dot_prod = att_out@att_out.T\n",
    "plt.imshow(self_dot_prod, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THEORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, simplify\n",
    "\n",
    "# Define symbols\n",
    "m, seq_len = symbols('m n')\n",
    "\n",
    "# Given values for dot products\n",
    "v1_v1 = v2_v2 = v3_v3 = seq_len\n",
    "v1_v2 = v1_v3 = v2_v3 = m\n",
    "\n",
    "# z.x\n",
    "zx = (1/4)*v1_v1 + (1/4)*v1_v2 + (1/2)*v1_v3\n",
    "# z.y\n",
    "zy = (1/4)*(1/3)*v1_v1 + (1/4)*(2/3)*v1_v2 + (1/4)*(1/3)*v1_v2 + (1/4)*(2/3)*v2_v2 + (1/2)*(1/3)*v1_v3 + (1/2)*(2/3)*v2_v3\n",
    "# z.zz\n",
    "zz = (1/4)**2*v1_v1 + 2*(1/4)**2*v1_v2 + (1/2)**2*v1_v3 + (1/4)**2*v2_v2 + 2*(1/4)*(1/2)*v2_v3 + (1/2)**2*v3_v3\n",
    "\n",
    "# Simplify expressions\n",
    "zx_simplified = simplify(zx)\n",
    "zy_simplified = simplify(zy)\n",
    "zz_simplified = simplify(zz)\n",
    "\n",
    "zx_simplified, zy_simplified, zz_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import exp, simplify\n",
    "\n",
    "# Define e as the base of the natural logarithm\n",
    "# e = exp(1)\n",
    "e = 2.718281828459045\n",
    "\n",
    "# Calculate a1, a2, b1, b2, b3\n",
    "a1 = e**(1/2) / (e**(1/2) + e**1)\n",
    "a2 = 1 - a1\n",
    "b1 = b2 = e**(1/2) / (e**(1/2) + e**(1/2) + e**1)\n",
    "b3 = 1 - b1 - b2\n",
    "\n",
    "# Define symbols\n",
    "m, seq_len = symbols('m n')\n",
    "\n",
    "# Given values for dot products\n",
    "v1_v1 = v2_v2 = v3_v3 = seq_len\n",
    "v1_v2 = v1_v3 = v2_v3 = m\n",
    "\n",
    "# z.x\n",
    "zx = b1*v1_v1 + b2*v1_v2 + b3*v1_v3\n",
    "# z.y\n",
    "zy = (b1*a1 + b2*a2)*v1_v1 + (b1*a2)*v1_v2 + (b2*a1)*v1_v2 + (b2*a2)*v2_v2 + b3*a1*v1_v3 + b3*a2*v2_v3\n",
    "# z.z\n",
    "zz = (b1**2 + b2**2)*v1_v1 + 2*(b1*b2)*v1_v2 + 2*(b1*b3 + b2*b3)*v1_v3 + b3**2*v3_v3 + (b2**2)*v2_v2 + 2*(b2*b3)*v2_v3\n",
    "\n",
    "# Simplify expressions\n",
    "a1_val, a2_val, b1_val, b2_val, b3_val = [simplify(val) for val in [a1, a2, b1, b2, b3]]\n",
    "zx_simplified = simplify(zx.subs({v1_v1: seq_len, v1_v2: m, v1_v3: m}))\n",
    "zy_simplified = simplify(zy.subs({v1_v1: seq_len, v1_v2: m, v1_v3: m, v2_v2: seq_len, v2_v3: m}))\n",
    "zz_simplified = simplify(zz.subs({v1_v1: seq_len, v1_v2: m, v1_v3: m, v2_v2: seq_len, v2_v3: m, v3_v3: seq_len}))\n",
    "\n",
    "a1_val, a2_val, b1_val, b2_val, b3_val, zx_simplified, zy_simplified, zz_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_value = 1\n",
    "n_value = 2  # Example values where n > m\n",
    "\n",
    "# Evaluate zx, zy, zz with substituted values of m and n\n",
    "zx_value = zx_simplified.subs({m: m_value, seq_len: n_value,})\n",
    "zy_value = zy_simplified.subs({m: m_value, seq_len: n_value,})\n",
    "zz_value = zz_simplified.subs({m: m_value, seq_len: n_value,})\n",
    "\n",
    "zx_value, zy_value, zz_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA visualizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "from IPython.utils import io\n",
    "\n",
    "\n",
    "input_act1_list = []\n",
    "# for config_dir, model_config_fold in exp_list:\n",
    "#   with open(f'{config_dir}/{model_config_fold}/config.yaml') as f:\n",
    "#     config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "#   ckpt = f\"{config_dir}/ckpt_10000_final.pt\"\n",
    "#   model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda')\n",
    "\n",
    "for config_dir, model_config_fold in exp_list:\n",
    "    glob_dir = config_dir.replace('[', '*').replace(']', '*')\n",
    "    yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "    config_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "    with open(yaml_path) as f:\n",
    "        config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    # ckpt = glob.glob(f\"{config_dir}/ckpt_**_acc.pt\", recursive=True)[0]\n",
    "    # ckpt = glob.glob(f'{glob_dir}/ckpt_**_acc.pt')[0]\n",
    "    ckpt = glob.glob(f'{glob_dir}/ckpt_**.pt')[0]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        # model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "        model, gptconfig = load_checkpoint(\n",
    "            ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "        # gptconfig.use_pe = 'sin'\n",
    "        # model = GPT_nope(gptconfig)\n",
    "\n",
    "    cur_input_act1_list = []\n",
    "    # for i in range(0, 3):\n",
    "    # model.transformer.h[0].permute = False\n",
    "    # prompts = [\n",
    "    #   f\"${823}\" + '+' + f\"{8}\"*3 + '=',\n",
    "    #   f\"${238}\" + '+' + f\"{8}\"*3 + '='\n",
    "    # ]\n",
    "    zs = np.zeros(12).astype(np.int64)\n",
    "    n_1s = 3\n",
    "    zs[np.random.permutation(12)[:n_1s]] = 1\n",
    "    str_zs = ''.join(map(str, zs))\n",
    "\n",
    "    prompts = [\n",
    "        # f'\\nparity({str_zs})=',\n",
    "        # '\\nparity(100101010000)='\n",
    "        # '\\nparity(010101000101)='\n",
    "        # 'parity(000010101001)=',\n",
    "        # '    $331+=',\n",
    "        'paridy(110001101010)=',\n",
    "        # '123+456'\n",
    "    ]\n",
    "    # prompts = [\n",
    "    #   f\"${623}\" + '+' + f\"{5}\"*3 + '=',\n",
    "    #   f\"${632}\" + '+' + f\"{5}\"*3 + '='\n",
    "    # ]\n",
    "    for hidx in range(0, 1):  # try 10 batches\n",
    "\n",
    "        # prompt = \"$\" + f\"{i}\"*3 + f\"{i}\"*6\n",
    "        # prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "        # prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "        prompt = prompts[hidx]\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def getActivation(name):\n",
    "            # the hook signature\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "            return hook\n",
    "        # register forward hooks on the layers of choice\n",
    "        h1 = model.transformer.h[1].register_forward_hook(\n",
    "            getActivation('layer_1'))\n",
    "        h2 = model.transformer.ln_f.register_forward_hook(\n",
    "            getActivation('x_out'))\n",
    "        # out_text = generate_output(model, prompt, max_new_tokens=5)\n",
    "        out_text = generate_output(model, prompt, max_new_tokens=4,)\n",
    "\n",
    "        h1.remove()\n",
    "        h2.remove()\n",
    "        model_name = config_dir.split('/')[-1]\n",
    "        print(model_name)\n",
    "        PCA_analysis(prompt, activation['x_out'][0], out_text, config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paridy(x):\n",
    "    x_trunc = str(x)\n",
    "    start_1 = x_trunc.find('1')\n",
    "    end_1 = x_trunc.rfind('1')\n",
    "    y_trunc = x_trunc[start_1:end_1+1].count('0') % 2\n",
    "    return y_trunc\n",
    "\n",
    "\n",
    "paridy(110001101010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import main_thread\n",
    "import main_utils\n",
    "# start_train = None\n",
    "reverse_ab = False\n",
    "reverse_c = True\n",
    "zero_pad = False\n",
    "algo_reason = False\n",
    "add_space = False\n",
    "config['causal_training'] = True\n",
    "\n",
    "\n",
    "\n",
    "config['start'] = start\n",
    "\n",
    "model, gptconfig = load_checkpoint(\n",
    "                \"./outputs_permute/add3_remove_8_nope_residual_exp/add3_remove_8_sd240_T2405280721_nope_lwpTrue_pmremove00000/ckpt_10000_acc_5000.pt\",\n",
    "                GPTConfig_nope,\n",
    "                GPT_nope,\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                return_config=True,\n",
    "                init=False,\n",
    "                init_additional_config={},\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "main_utils.evaluate_addition_batch(config, model, ctx, encode, decode, verbose=True, num_digit=num_digit, zero_pad=zero_pad,\n",
    "                                   reverse_ab=reverse_ab, reverse_c=reverse_c, algo_reason=algo_reason,\n",
    "                                   binary=binary, data_type=data_type, operator=operator, data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interesting: for a causal model with pe, with or without \"\\n\" makes a 180 degree difference in outcome !!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original_model_pe_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.transformer.wpe.weight.cpu().detach().numpy()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "x = model.transformer.wpe.weight.cpu().detach().numpy()\n",
    "# select sample maybe from test set\n",
    "# but if, different digits seems to be encoded the same way, than it has a patter\n",
    "pca = PCA(n_components=2)\n",
    "new_x = pca.fit_transform(x)\n",
    "new_x = new_x[::16]\n",
    "for text, pt in zip(range(len(new_x)), new_x, ):\n",
    "    plt.scatter(pt[0], pt[1], label=text)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(new_x)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import U\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_measure = lambda u1, u2: np.dot(u1, u2.T)\n",
    "# sim_measure = cosine_similarity\n",
    "\n",
    "\n",
    "def standardize_rows(matrix):\n",
    "    \"\"\"Standardize each row of the matrix.\"\"\"\n",
    "    mean = matrix.mean(axis=1, keepdims=True)\n",
    "    std = matrix.std(axis=1, keepdims=True)\n",
    "    return (matrix - mean) / std\n",
    "\n",
    "\n",
    "def plot_corr_mat(corr_mat, vec_dim, is_corr=True, absval=True):\n",
    "\n",
    "    show_text = False if not is_corr else True\n",
    "\n",
    "    # Create a heatmap of corr_mat\n",
    "    plt.figure(figsize=(6, 5), dpi=120)\n",
    "    # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "    plt.imshow(corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "\n",
    "    extra_text = \"Absolute\" if is_corr else \"\"\n",
    "    plot_type = f\"{extra_text} Correlation Coefficient\" if is_corr else \"Dot Product\"\n",
    "\n",
    "    plt.colorbar(\n",
    "        label=plot_type,\n",
    "        # fraction=0.01,\n",
    "        # pad=0.04,\n",
    "    )\n",
    "    if show_text:\n",
    "        for i in range(0, len(corr_mat), vec_dim):\n",
    "            for j in range(0, len(corr_mat), vec_dim):\n",
    "                plt.text(\n",
    "                    j + vec_dim // 2,\n",
    "                    i + vec_dim // 2,\n",
    "                    f\"{corr_mat[i:i+vec_dim, j:j+vec_dim].sum():.02f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"black\",\n",
    "                )\n",
    "\n",
    "\n",
    "level_corr_mat_accum = []\n",
    "corr_mat = None\n",
    "u1, u2, u1_name, u2_name = None, None, None, None\n",
    "\n",
    "task_name = \"mod_\"\n",
    "folder_name = f\"corr_{task_name}\" if not equal_distancing_exp else f\"dot_{task_name}\"\n",
    "folder_name += \"_trained\" if not model_init else \"_init\"\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "    idx1=(0, len(input_act1_list) - 1), idx2=(0, len(input_act1_list) - 1), level=(0, 5)\n",
    ")\n",
    "def get_corr(\n",
    "    idx1,\n",
    "    level=0,\n",
    "    save=False,\n",
    "    absval=True,\n",
    "    save_all=False,\n",
    "    accumulate_all=False,\n",
    "    is_rand_init=False,\n",
    "    subplot_layers=False,\n",
    "):\n",
    "    global corr_mat\n",
    "    global u1, u2, u1_name, u2_name\n",
    "    rand_state = \"init_\" if is_rand_init else \"\"\n",
    "    if not (save_all or accumulate_all):\n",
    "        if not subplot_layers:\n",
    "            idx2 = idx1\n",
    "            input_act1_list = all_level_input_act_list[level]\n",
    "            u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "\n",
    "            u1 = u1.T\n",
    "            u2 = u2.T\n",
    "            print(exp_list[idx1][0].split(\"sd\")[-1], exp_list[idx2][0].split(\"sd\")[-1])\n",
    "            print(u2.shape)\n",
    "\n",
    "            is_corr = False\n",
    "            if u2.shape[1] != 1:\n",
    "                # Standardize each row of u1 and u2\n",
    "                u1_standardized = standardize_rows(u1)\n",
    "                u2_standardized = standardize_rows(u2)\n",
    "                # u1_standardized = u1\n",
    "                # u2_standardized = u2\n",
    "\n",
    "                # Compute the correlation matrix\n",
    "                corr_mat = np.dot(u1_standardized, u2_standardized.T) / (u1.shape[1])\n",
    "            else:\n",
    "                u1 = u1.reshape(-1, 384)\n",
    "                u2 = u2.reshape(-1, 384)\n",
    "                # corr_mat = np.dot(u1, u2.T)\n",
    "                corr_mat = sim_measure(u1, u2)\n",
    "\n",
    "            if absval:\n",
    "                corr_mat = np.abs(corr_mat)\n",
    "            else:\n",
    "                pass\n",
    "            vec_dim = corr_mat.shape[0] // 8\n",
    "            total_sum = np.abs(corr_mat).sum()\n",
    "            block_sum = 0\n",
    "            for i in range(0, len(corr_mat), vec_dim):\n",
    "                block_sum += np.abs(corr_mat[i : i + vec_dim, i : i + vec_dim]).sum()\n",
    "            ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "            plot_corr_mat(corr_mat, vec_dim, is_corr=is_corr, absval=absval)\n",
    "\n",
    "            nope2 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "            u2_name = \"_\".join(exp_list[idx2][1].split(\"_\")[2:])\n",
    "            u2_name = (\n",
    "                nope2 + u2_name.split(\"_\")[-1] + \"_\" + \"_\".join(u2_name.split(\"_\")[:-1])\n",
    "            )\n",
    "\n",
    "            if save:\n",
    "                os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    f\"./saved_plots_{folder_name}/{task_name+folder_name}_{rand_state}_{u2_name}_layer{level}_{ratio:.03f}_{abs}.svg\"\n",
    "                )\n",
    "\n",
    "            # close img\n",
    "            # plt.close()\n",
    "            plt.show()\n",
    "        else:\n",
    "            fig, axs = plt.subplots(\n",
    "                1, 6, figsize=(36, 5)\n",
    "            )  # 6 subplots in a row, adjust size as needed\n",
    "\n",
    "            global_min, global_max = float(\"inf\"), float(\"-inf\")\n",
    "            corr_mat_list = []\n",
    "            for level in range(6):\n",
    "                idx2 = idx1\n",
    "                input_act1_list = all_level_input_act_list[level]\n",
    "                u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "\n",
    "                u1 = u1.T\n",
    "                u2 = u2.T\n",
    "\n",
    "                if u2.shape[1] != 1:\n",
    "                    u1_standardized = standardize_rows(u1)\n",
    "                    u2_standardized = standardize_rows(u2)\n",
    "                    corr_mat = np.dot(u1_standardized, u2_standardized.T) / (\n",
    "                        u1.shape[1]\n",
    "                    )\n",
    "                else:\n",
    "                    u1 = u1.reshape(-1, 384)\n",
    "                    u2 = u2.reshape(-1, 384)\n",
    "                    # corr_mat = np.dot(u1, u2.T)\n",
    "                    corr_mat = sim_measure(u1, u2)\n",
    "\n",
    "                if absval:\n",
    "                    corr_mat = np.abs(corr_mat)\n",
    "                corr_mat_list.append(corr_mat)\n",
    "\n",
    "                global_min = min(global_min, corr_mat.min())\n",
    "                global_max = max(global_max, corr_mat.max())\n",
    "\n",
    "            for level in range(6):\n",
    "\n",
    "                corr_mat = corr_mat_list[level]\n",
    "\n",
    "                # vec_dim = 384\n",
    "                vec_dim = corr_mat.shape[0] // fixed_length\n",
    "\n",
    "                total_sum = np.abs(corr_mat).sum()\n",
    "                block_sum = 0\n",
    "                for i in range(0, len(corr_mat), vec_dim):\n",
    "                    block_sum += np.abs(\n",
    "                        corr_mat[i : i + vec_dim, i : i + vec_dim]\n",
    "                    ).sum()\n",
    "                ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "                cm = axs[level].imshow(\n",
    "                    corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\"\n",
    "                )\n",
    "                #   , vmin=global_min, vmax=global_max)\n",
    "                axs[level].set_title(f\"Layer {level}\")\n",
    "                plt.colorbar(\n",
    "                    cm,\n",
    "                    ax=axs[level],\n",
    "                    orientation=\"vertical\",\n",
    "                    fraction=0.06,\n",
    "                    pad=0.04,\n",
    "                )\n",
    "\n",
    "            extra_text = \"Absolute \" if absval else \"\"\n",
    "            # fig.colorbar(cbar_ax, ax=axs, orientation='vertical', label=f'{extra_text}Correlation Coefficient')\n",
    "\n",
    "            # Set axis labels and title\n",
    "            # plt.xlabel('U2 Entries')\n",
    "            # plt.ylabel('U1 Entries')\n",
    "\n",
    "            # Show the plot\n",
    "            nope1 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "            u1_name = \"_\".join(exp_list[idx1][1].split(\"_\")[2:])\n",
    "            u1_name = (\n",
    "                nope1 + u1_name.split(\"_\")[-1] + \"_\" + \"_\".join(u1_name.split(\"_\")[:-1])\n",
    "            )\n",
    "            nope2 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "            u2_name = \"_\".join(exp_list[idx2][1].split(\"_\")[2:])\n",
    "            u2_name = (\n",
    "                nope2 + u2_name.split(\"_\")[-1] + \"_\" + \"_\".join(u2_name.split(\"_\")[:-1])\n",
    "            )\n",
    "\n",
    "            extra_self = \"Self\" if u1_name == u2_name else \"\"\n",
    "            # plt.title(f'{extra_self} Correlation Matrix for  {ratio:.2f}')\n",
    "            print(u2_name)\n",
    "            if save:\n",
    "                os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    f\"./saved_plots_{folder_name}/{task_name+folder_name}_{len(all_level_input_act_list)}layers_{rand_state}_{u2_name}_{ratio:.03f}_{abs}.svg\"\n",
    "                )\n",
    "\n",
    "            # close img\n",
    "            # plt.close()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        corr_mat_list = []\n",
    "        for level in range(len(all_level_input_act_list)):\n",
    "            input_act1_list = all_level_input_act_list[level]\n",
    "            global level_corr_mat_accum\n",
    "            for idx1 in tqdm(range(len(input_act1_list))):\n",
    "                idx2 = idx1\n",
    "                u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "                u1 = u1.T\n",
    "                u2 = u2.T\n",
    "\n",
    "                if u2.shape[1] != 1: # calculate correlation coefficient\n",
    "                    u1_standardized = standardize_rows(u1)\n",
    "                    u2_standardized = standardize_rows(u2)\n",
    "                    # u1_standardized = u1\n",
    "                    # u2_standardized = u2\n",
    "\n",
    "                    # Compute the correlation matrix\n",
    "                    corr_mat = np.dot(u1_standardized, u2_standardized.T) / (\n",
    "                        u1.shape[1]\n",
    "                    )\n",
    "                else:\n",
    "                    u1 = u1.reshape(-1, 384)\n",
    "                    u2 = u2.reshape(-1, 384)\n",
    "                    # corr_mat = np.dot(u1, u2.T)\n",
    "                    corr_mat = sim_measure(u1, u2)\n",
    "\n",
    "                if absval:\n",
    "                    corr_mat = np.abs(corr_mat)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if accumulate_all:\n",
    "                    level_corr_mat_accum.append(corr_mat[None, ...])\n",
    "                if not save_all:\n",
    "                    continue\n",
    "\n",
    "                if not subplot_layers:\n",
    "                    vec_dim = corr_mat.shape[0] // 8\n",
    "                    total_sum = np.abs(corr_mat).sum()\n",
    "                    block_sum = 0\n",
    "                    for i in range(0, len(corr_mat), vec_dim):\n",
    "                        block_sum += np.abs(\n",
    "                            corr_mat[i : i + vec_dim, i : i + vec_dim]\n",
    "                        ).sum()\n",
    "                    ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "                    # Assuming you have calculated 'corr_mat' as described in the previous answer\n",
    "\n",
    "                    # Create a heatmap of corr_mat\n",
    "                    plt.figure(figsize=(6, 5), dpi=120)\n",
    "                    # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "                    plt.imshow(corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "\n",
    "                    extra_text = \"Absolute \" if absval else \"\"\n",
    "                    plt.colorbar(label=f\"{extra_text}Correlation Coefficient\")\n",
    "\n",
    "                    # Set axis labels and title\n",
    "                    # plt.xlabel('U2 Entries')\n",
    "                    # plt.ylabel('U1 Entries')\n",
    "\n",
    "                    # Show the plot\n",
    "                    nope1 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "                    u1_name = \"_\".join(exp_list[idx1][1].split(\"_\")[2:])\n",
    "                    u1_name = (\n",
    "                        nope1\n",
    "                        + u1_name.split(\"_\")[-1]\n",
    "                        + \"_\"\n",
    "                        + \"_\".join(u1_name.split(\"_\")[:-1])\n",
    "                    )\n",
    "                    nope2 = \"nope_\" if \"nope\" in exp_list[idx1][1] else \"\"\n",
    "                    u2_name = \"_\".join(exp_list[idx2][1].split(\"_\")[2:])\n",
    "                    u2_name = (\n",
    "                        nope2\n",
    "                        + u2_name.split(\"_\")[-1]\n",
    "                        + \"_\"\n",
    "                        + \"_\".join(u2_name.split(\"_\")[:-1])\n",
    "                    )\n",
    "\n",
    "                    extra_self = \"Self\" if u1_name == u2_name else \"\"\n",
    "                    # plt.title(f'{extra_self} Correlation Matrix for  {ratio:.2f}')\n",
    "                    # print(ratio)\n",
    "                    # print(u1_name, u2_name)\n",
    "                    os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                    plt.savefig(\n",
    "                        f\"./saved_plots_{folder_name}/{task_name+folder_name}_{rand_state}_{u2_name}_layer{level}_{ratio:.03f}_{abs}.svg\"\n",
    "                    )\n",
    "\n",
    "                    # close img\n",
    "                    plt.close()\n",
    "                # plt.show()\n",
    "\n",
    "            if accumulate_all:\n",
    "                level_corr_mat_accum = np.vstack(level_corr_mat_accum)\n",
    "                print(level_corr_mat_accum.shape)\n",
    "                level_corr_mat_accum = level_corr_mat_accum.mean(axis=0)\n",
    "                # Create a heatmap of corr_mat\n",
    "                if not subplot_layers:\n",
    "                    plt.figure(figsize=(6, 5), dpi=120)\n",
    "                    # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "                    plt.imshow(\n",
    "                        level_corr_mat_accum, cmap=\"coolwarm\", interpolation=\"nearest\"\n",
    "                    )\n",
    "\n",
    "                    extra_text = \"Absolute \" if absval else \"\"\n",
    "                    plt.colorbar(label=f\"{extra_text}Correlation Coefficient\")\n",
    "                    plt.show()\n",
    "                corr_mat = level_corr_mat_accum\n",
    "                level_corr_mat_accum = []\n",
    "            corr_mat_list.append(corr_mat)\n",
    "        if subplot_layers:\n",
    "            fig, axs = plt.subplots(\n",
    "                1, 6, figsize=(36, 5)\n",
    "            )  # 6 subplots in a row, adjust size as needed\n",
    "            for level in range(len(all_level_input_act_list)):\n",
    "\n",
    "                corr_mat = corr_mat_list[level]\n",
    "\n",
    "                # vec_dim = 384\n",
    "                vec_dim = corr_mat.shape[0] // 8\n",
    "\n",
    "                total_sum = np.abs(corr_mat).sum()\n",
    "                block_sum = 0\n",
    "                for i in range(0, len(corr_mat), vec_dim):\n",
    "                    block_sum += np.abs(\n",
    "                        corr_mat[i : i + vec_dim, i : i + vec_dim]\n",
    "                    ).sum()\n",
    "                ratio = block_sum / (total_sum - block_sum)\n",
    "\n",
    "                cm = axs[level].imshow(\n",
    "                    corr_mat, cmap=\"coolwarm\", interpolation=\"nearest\"\n",
    "                )\n",
    "                #   , vmin=global_min, vmax=global_max)\n",
    "                axs[level].set_title(f\"Layer {level}\")\n",
    "                plt.colorbar(\n",
    "                    cm,\n",
    "                    ax=axs[level],\n",
    "                    orientation=\"vertical\",\n",
    "                    fraction=0.06,\n",
    "                    pad=0.04,\n",
    "                )\n",
    "\n",
    "            print(u2_name)\n",
    "            if save:\n",
    "                os.makedirs(f\"./saved_plots_{folder_name}/\", exist_ok=True)\n",
    "                plt.savefig(\n",
    "                    f\"./saved_plots_{folder_name}/{task_name+folder_name}_avg_{abs}.svg\"\n",
    "                )\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "fixed_length = 9\n",
    "all_level_input_act_list = []\n",
    "sample_num = 1024\n",
    "\n",
    "# model_list = []\n",
    "# for idx, (config_dir, model_config_fold) in enumerate(exp_list):\n",
    "#     glob_dir = config_dir.replace('[', '[[]')\n",
    "#     yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "#     revised_glob_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "#     exp_list[idx][0] = revised_glob_dir\n",
    "#     exp_list[idx][1] = revised_glob_dir.split('/')[-1]\n",
    "\n",
    "#     with open(yaml_path) as f:\n",
    "#         config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "#     ckpt = f\"{revised_glob_dir}/ckpt_10000_acc.pt\"\n",
    "#     model, gptconfig = load_checkpoint(\n",
    "#         ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "\n",
    "#     # gptconfig.not_causal = [1]*6\n",
    "#     # model = GPT_nope(GPTConfig_nope())\n",
    "#     # model = GPT_nope(gptconfig)\n",
    "\n",
    "#     # for i in range(len(model.transformer.h)):\n",
    "#     #   model.transformer.h[i].attn._reset_parameters()\n",
    "#     #   model.transformer.h[i].mlp._reset_parameters()\n",
    "#     # model.apply(model._init_weights)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "#     model_list.append(model)\n",
    "\n",
    "\n",
    "for level in range(0, 8):\n",
    "    level = level - 1\n",
    "    input_act1_list = [list() for _ in range(len(model_list))]\n",
    "\n",
    "    for hidx in range(0, 1):  # try 5 batches\n",
    "        X, Y = get_batch('train')\n",
    "        X = decode(X[0].tolist())\n",
    "        X_8 = list(map(lambda x: x[:fixed_length], filter(lambda x: len(\n",
    "            x) >= fixed_length and x[fixed_length-1] == '=', X.split('\\n'))))\n",
    "        X_8 = [''.join(list(np.array(list(x))[np.random.permutation(len(x))]))\n",
    "               for x in X_8]\n",
    "        X = torch.tensor(list(map(lambda x: encode(x), X_8)))\n",
    "        # X = X.reshape(-1, fixed_length)\n",
    "        # X = X[:sample_num]\n",
    "        X = X.to(device)\n",
    "\n",
    "        for midx, model in enumerate(model_list):\n",
    "            activation = {}\n",
    "\n",
    "            def getActivation(name):\n",
    "                # the hook signature\n",
    "                def hook(model, input, output):\n",
    "                    activation[name] = output.detach()\n",
    "                return hook\n",
    "            # register forward hooks on the layers of choice\n",
    "\n",
    "            if level == 6:\n",
    "                h1 = model.transformer.ln_f.register_forward_hook(\n",
    "                    getActivation(f'layer_{level}'))\n",
    "            elif level == -1:\n",
    "                h1 = model.transformer.wte.register_forward_hook(\n",
    "                    getActivation(f'layer_{level}'))\n",
    "            else:\n",
    "                h1 = model.transformer.h[level].ln_1.register_forward_hook(\n",
    "                    getActivation(f'layer_{level}'))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _ = model(X)\n",
    "\n",
    "            h1.remove()\n",
    "\n",
    "            acts = activation[f'layer_{level}'].detach().cpu().numpy()\n",
    "            input_act1_list[midx].append(acts)\n",
    "            # outs = activation['x_out'].detach().cpu().numpy()\n",
    "            # input_act1_list[midx].append(outs)\n",
    "\n",
    "    for hidx in range(len(input_act1_list)):\n",
    "        print(len(input_act1_list[hidx]))\n",
    "        cur_input_act1 = np.concatenate(input_act1_list[hidx], axis=0)\n",
    "        bs, l, dim = cur_input_act1.shape\n",
    "        targets = np.zeros((bs, l)) + np.arange(l)[None, ...]\n",
    "        print(cur_input_act1.shape)\n",
    "        input_act1_list[hidx] = (cur_input_act1, targets)\n",
    "\n",
    "    all_level_input_act_list.append(input_act1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "len(all_level_input_act_list[0][1][1])\n",
    "\n",
    "# do a scklearn linear regression\n",
    "\n",
    "\n",
    "def crossing(X, y):\n",
    "    X_cross = []\n",
    "    y_cross = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            if i != j:\n",
    "                X_cross.append(X[i] * X[j])\n",
    "                y_cross.append(np.abs(y[i] - y[j]))\n",
    "    X = np.array(X_cross)\n",
    "    y = np.array(y_cross)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "@widgets.interact(layer=(-1, 6), model_idx=(0, len(all_level_input_act_list[0])-1))\n",
    "def probe_layer(layer=-1, model_idx=0):\n",
    "    layer = layer + 1\n",
    "    print(exp_list[model_idx][0])\n",
    "    X = all_level_input_act_list[layer][model_idx][0]\n",
    "    y = all_level_input_act_list[layer][model_idx][1]\n",
    "\n",
    "    # X = np.random.rand(*y.shape,10)\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    y = y.reshape(-1)\n",
    "    X_train, X_test = X[:int(len(X)*0.8)], X[int(len(X)*0.8):]\n",
    "    y_train, y_test = y[:int(len(X)*0.8)], y[int(len(X)*0.8):]\n",
    "\n",
    "    # X_train, y_train = crossing(X, y)\n",
    "    # X_test, y_test = crossing(X, y)\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    # X_test = X_train\n",
    "    # y_test = y_train\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(mean_squared_error(y_test, y_pred))\n",
    "    print(reg.score(X_test, y_test))\n",
    "    print(y_test[:10])\n",
    "    print(y_pred[:10])\n",
    "\n",
    "    # print('coef:', reg.coef_)\n",
    "    plt.plot(reg.coef_)\n",
    "    plt.show()\n",
    "\n",
    "    mav = []\n",
    "    for i in range(10):\n",
    "        plt.plot(X_test[i])\n",
    "        mav.append(np.abs(X_test[i]).mean())\n",
    "    plt.show()\n",
    "\n",
    "    print(list(zip(y_test[:10], mav)))\n",
    "\n",
    "    mav = []\n",
    "    for i in range(len(y_test)):\n",
    "        mav.append(np.abs(X_test[i]).mean())\n",
    "\n",
    "    # plt.scatter(y_test, mav, s=20, alpha=0.2)\n",
    "    # plt.show()\n",
    "    print(y_test.shape)\n",
    "\n",
    "    plt.scatter(y_test, y_pred, s=20, alpha=0.2)\n",
    "\n",
    "# normalize the input and do again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Nope and original behaves differently\n",
    "2. When organized and unorganized, the activation output from the trained model is different, meaning that the model somehow also semantically managed the position of numbers and symbos \n",
    "3. For original pe, the regression model must be memorizing the absvalvalolute position initialized randomly at the start of the model. But that didn't explain why NOPE can also get positions right? Then there must be something permanent inside the model that indicates the positions, emm, such as a fixed bias?\n",
    "4. If looking closely at the activations from layers without skip \n",
    "\n",
    "\n",
    "Maybe check why noncausal still doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check random initialization problem\n",
    "\n",
    "* Want to actually check that empirically Qx \\cdot Ky is maximized\n",
    "(usually) when x\\approx y\n",
    "    - could be Wk making W_ky more similar to W_kx\n",
    "    - \\sum a_i b_i (W_Q Z_i) (W_k Z_i) where Z_i can be the pca basis / or any other spectral decomposition\n",
    "\n",
    "... an evidence that residual connection is preserving the locality of r.vec. x\n",
    "\n",
    "... Hypothesis: signs tend to be the same for z1=z2 and different otherwise\n",
    "\n",
    "* Want to compute the rank of\n",
    "    - with\n",
    "        - (1) Transformers with random initialization\n",
    "        - (2) Transformers at convergence\n",
    "\n",
    "    - for\n",
    "       - (a) full residual connections\n",
    "       - (b) no residual connections\n",
    "       - (c) some residual connections\n",
    "\n",
    "check rank degeneration: PCA -> compute the ratio \n",
    "- i.e. a1^2 / sum(ai^2) \n",
    "- plz see how the paper measures the rank degeneration \n",
    "\n",
    "* Want to check what happens when the residual connections we ablate\n",
    "are not consecutive (both non-consecutive layers, and when things\n",
    "taken out are pre-MLP/post-MLP)\n",
    "\n",
    "\n",
    "* Fix the description of the correlation img\n",
    "    - Collect the ratio from the graph and put into a table\n",
    "    - generate the image for all experiments we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- emperically validate qk, if vectors points in the same direction, othen kx dot qy should be high (for trained k,q)\n",
    "    - Plot the correlation on this: QxdotKx vs xdoty \n",
    "- description for correlation matrix\n",
    "\n",
    "or x being actual inputs $x \\in R^d$, project x using K and Q (for a lot of x) would be generally a projection into to the same subspace, namely $Kx \\cdot Qx$ be high\n",
    "    - PCA on K{x}, project on the first 100 components\n",
    "    - project on the first 100 components of Q{x}\n",
    "\n",
    "$K{x} : subspace {Kx| x \\in R^d}$\n",
    "compare projecting K{x} on the principal components of Q{x} to projecting K{x} on the standard basis\n",
    "(n.b., projecting on the first 100 components of the standard basis is just taking the first 100 coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 24\n",
    "plt.plot(X_train[idx])\n",
    "print(y_train[:10])\n",
    "print(reg.predict(X_train[:10][:]))\n",
    "print(X_train[:10].sum(axis=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
