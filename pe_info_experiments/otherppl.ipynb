{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from random import randrange, choice\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Adding extra padding is an easy way to improve performance, as it gives the\n",
    "# model more space to think. For example, without padding, the standard model\n",
    "# kind=transformer-lstm gets accuracies (1, 0.95, 0.66), but we just five extra\n",
    "# paddings on the left, it gets (1, 0.98, 0.80). Even better, if we add the\n",
    "# padding right before the equality sign, ...\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, base, number_length, pre_end_padding=0,\n",
    "                 flip=False, preferred_dtype='int64'):\n",
    "        self.base = base\n",
    "        self.number_length = number_length\n",
    "        self.pre_end_padding = pre_end_padding\n",
    "        self.flip = flip\n",
    "        self.preferred_dtype = preferred_dtype\n",
    "\n",
    "        self.start_token = base  # Before input\n",
    "        self.end_token = base + 1  # After input\n",
    "        self.separator_token = base + 2  # between inputs\n",
    "        self.padding_token = base + 3  # Before input and after target\n",
    "        self.eos_token = base + 4  # After target\n",
    "        self.n_tokens = base + 5\n",
    "\n",
    "        self.dic = {i: str(i) for i in range(self.base + 1)}\n",
    "        self.dic[self.padding_token] = \"\"\n",
    "        self.dic[self.start_token] = \"\"\n",
    "        self.dic[self.end_token] = \"=\"\n",
    "        self.dic[self.eos_token] = \"\"\n",
    "\n",
    "    def make_numbers(self, shape, number_length=None):\n",
    "        if number_length is None:\n",
    "            number_length = self.number_length\n",
    "        if np.dtype(self.preferred_dtype) is np.dtype(object):\n",
    "            powers = [self.base**(i+1) for i in range(number_length)]\n",
    "            n = math.prod(shape)\n",
    "            result = np.array([randrange(choice(powers)) for i in range(n)],\n",
    "                              dtype=object).reshape(shape)\n",
    "        else:\n",
    "            digits_shape = shape + (number_length,)\n",
    "            digits = np.random.randint(0, self.base, math.prod(digits_shape)).reshape(digits_shape)\n",
    "            n_digits = np.random.randint(0, number_length, shape)\n",
    "            mask = np.arange(number_length) < n_digits[..., None]\n",
    "            exponents = np.arange(number_length - 1, -1, -1, dtype=self.preferred_dtype)\n",
    "            bases = np.expand_dims(np.power(self.base, exponents), 0)\n",
    "            result = (digits * bases).sum(axis=-1)\n",
    "        return result\n",
    "\n",
    "    def to_digits(self, numbers, length=None):\n",
    "        if length is None:\n",
    "            length = self.number_length\n",
    "\n",
    "        # Convert numbers to digits\n",
    "        tensor = np.tile(np.expand_dims(numbers, 1), (1, length))\n",
    "        exponents = np.arange(length - 1, -1, -1, dtype=self.preferred_dtype)\n",
    "        bases = np.expand_dims(np.power(self.base, exponents), 0)\n",
    "        digits = (tensor // bases) % self.base\n",
    "\n",
    "        # Mask leading zeros\n",
    "        mask = digits.cumsum(1) == 0\n",
    "        mask[:, -1] = False\n",
    "        digits[mask] = self.padding_token\n",
    "        if self.flip:\n",
    "            return np.flip(digits, [1])\n",
    "        return digits\n",
    "\n",
    "    def move_padding_to_end(self, tensor, end=True):\n",
    "        \"\"\"Move all padding tokens in each row to the end without reordering the rest.\"\"\"\n",
    "\n",
    "        # Create a tensor with large values where there's padding and row-wise indices elsewhere\n",
    "        # This allows us to \"sort\" the padding to the end, while keeping everything else in its\n",
    "        # original order.\n",
    "        sorting_tensor = np.where(\n",
    "            tensor == self.padding_token,\n",
    "            tensor.shape[1] if end else -tensor.shape[1],\n",
    "            np.arange(tensor.shape[1])\n",
    "        )\n",
    "\n",
    "        # Get the indices that would sort the tensor\n",
    "        sorted_indices = np.argsort(sorting_tensor, axis=1)\n",
    "\n",
    "        # Use the sorted indices to rearrange the original tensor\n",
    "        sorted_tensor = np.take_along_axis(tensor, sorted_indices, 1)\n",
    "\n",
    "        return sorted_tensor\n",
    "\n",
    "    def generate_batch(self, bs):\n",
    "        res = self._generate_batch(bs)\n",
    "        res = self.move_padding_to_end(res)\n",
    "\n",
    "        # Insert COT padding\n",
    "        if self.pre_end_padding != 0:\n",
    "            indices_padding = (res == self.end_token).nonzero(as_tuple=True)\n",
    "            expanded_tensor = torch.zeros(bs, self.seq + self.pre_end_padding, dtype=res.dtype)\n",
    "            # Calculate the positions in the expanded tensor for all elements\n",
    "            positions = torch.arange(self.seq).unsqueeze(0).repeat(bs, 1)\n",
    "            positions += self.pre_end_padding * (positions >= indices_padding[1].unsqueeze(1))\n",
    "            # Use scatter to insert values at the correct positions\n",
    "            expanded_tensor.scatter_(1, positions, res)\n",
    "            res = expanded_tensor\n",
    "\n",
    "        # assert res.shape == (bs, self.seq)\n",
    "        return res\n",
    "\n",
    "    def _generate_batch(self, tokens):\n",
    "        assert False, \"Not implemented\"\n",
    "\n",
    "    def repr_example(self, example):\n",
    "        tokens = [\n",
    "            (tuple(group)[::-1] if self.flip else tuple(group))\n",
    "            if is_number\n",
    "            else next(group)\n",
    "            for is_number, group in itertools.groupby(\n",
    "                example.tolist(), key=lambda x: x < self.base\n",
    "            )\n",
    "        ]\n",
    "        return self._repr_tokens(tokens).strip()\n",
    "\n",
    "    def _repr_tokens(self, tokens):\n",
    "        res = []\n",
    "        for token in tokens:\n",
    "            if type(token) is tuple:\n",
    "                res.append(\"\".join(map(str, token)))\n",
    "            else:\n",
    "                res.append(self.dic[token])\n",
    "        return \" \".join(res)\n",
    "\n",
    "    @property\n",
    "    def seq(self):\n",
    "        assert False, \"Not implemented\"\n",
    "\n",
    "\n",
    "class AddModDataset(Dataset):\n",
    "    def __init__(self, base, number_length, pre_end_padding=0, flip=False, **kwargs):\n",
    "        super().__init__(base, number_length, pre_end_padding, flip, **kwargs)\n",
    "        self.separator_token2 = self.n_tokens\n",
    "        self.n_tokens += 1\n",
    "        self.dic[self.separator_token] = \"+\"\n",
    "        self.dic[self.separator_token2] = \"%\"\n",
    "\n",
    "    def _generate_batch(self, bs):\n",
    "        a, b = self.make_numbers((2, bs))\n",
    "        c = self.make_numbers((bs,), (self.number_length + 1) // 2)\n",
    "        out = (a + b) % np.clip(c, 1, None)\n",
    "        return np.concatenate(\n",
    "            [\n",
    "                np.full((bs, 1), self.start_token),\n",
    "                self.to_digits(a),\n",
    "                np.full((bs, 1), self.separator_token),\n",
    "                self.to_digits(b),\n",
    "                np.full((bs, 1), self.separator_token2),\n",
    "                self.to_digits(c),\n",
    "                np.full((bs, 1), self.end_token),\n",
    "                self.to_digits(out),\n",
    "                np.full((bs, 1), self.eos_token),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def seq(self):\n",
    "        return self.number_length * 4 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'146 + 428 % 27 = 7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = AddModDataset(10, 3)\n",
    "exmples = ds.generate_batch(2)\n",
    "ds.repr_example(exmples[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
