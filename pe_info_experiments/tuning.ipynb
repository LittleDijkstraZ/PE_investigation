{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters in a dictionary\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import optuna\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def run_tuning(trial, out_name):\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 6e-4, log=True),\n",
    "    warmup_iters = trial.suggest_int(\"warmup_iters\", 200, 1000, log=True),\n",
    "    layerwise_pe = '[4]'\n",
    "    params = {\n",
    "        'max_iters': 10000,\n",
    "        'lr_decay_iters': 10000,\n",
    "        'general_seed': 888,\n",
    "        'out_dir': 'outputs',\n",
    "        'pe_type': 'original',  # or 'sin'\n",
    "        'use_residual': '[0,1,2,4,5]',\n",
    "\n",
    "    }\n",
    "\n",
    "    # Updating the out_name format\n",
    "\n",
    "    # Updating the out_name format\n",
    "    params['out_name'] = out_name\n",
    "\n",
    "    print(f\"Running with general_seed: {params['out_name']}\")\n",
    "\n",
    "    # Loop through the parameters\n",
    "\n",
    "    \n",
    "    # Construct the output directory and other variables\n",
    "    output_directory = os.path.join(params['out_dir'], f\"{params['out_name']}/addition_reverse_sd{params['general_seed']}_lr{learning_rate}_wu{warmup_iters}\")\n",
    "    wandb_run_name = f\"addition_reverse_sd{params['general_seed']}_lr{learning_rate}_wu{warmup_iters}\"\n",
    "    wandb_project = params['out_name']\n",
    "\n",
    "    # Construct the command using parameters from the dictionary\n",
    "    command_params = {\n",
    "        'use_pe': params['pe_type'],\n",
    "        'use_residual': params['use_residual'],\n",
    "        'max_iters': params['max_iters'],\n",
    "        'lr_decay_iters': params['lr_decay_iters'],\n",
    "        'layerwise_pe': layerwise_pe,\n",
    "        'out_dir': output_directory,\n",
    "        'wandb_run_name': wandb_run_name,\n",
    "        'learning_rate': learning_rate,\n",
    "        'warmup_iters': warmup_iters,\n",
    "        'wandb_project': wandb_project\n",
    "    }\n",
    "\n",
    "    command = \"python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py\"\n",
    "    for key, value in command_params.items():\n",
    "        command += f\" --{key}={value}\"\n",
    "\n",
    "    output_files = glob.glob('../'+output_directory + '**/**/**.csv', recursive=True)\n",
    "    output_files.sort(key=os.path.getctime, reverse=True)\n",
    "    latest_file = output_files[0]\n",
    "    df = pd.read_csv(latest_file)\n",
    "    train_loss = df['train_loss'].values[-1]\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    out_dir = \"./outputs\"\n",
    "    out_name = f\"out3_tuning\"\n",
    "    storage = JournalStorage(\n",
    "        JournalFileStorage(f\"{out_dir}/{out_name}/tuning.log\")\n",
    "    )\n",
    "    try:\n",
    "        study = optuna.load_study(\n",
    "            storage=storage,\n",
    "            study_name=out_name,\n",
    "        )\n",
    "    except:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=storage,  # Specify the storage URL here.\n",
    "            study_name=out_name,\n",
    "            pruner=optuna.pruners.MedianPruner(),\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "    trial_function = partial(\n",
    "        run_tuning, out_name = out_name\n",
    "    )\n",
    "    study.optimize(trial_function, n_trials=32)\n",
    "    print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import sys\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from io import StringIO\n",
    "import traceback\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "import yaml\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "except ImportError:\n",
    "    logging.info(\"Local machine detected\")\n",
    "    sys.path.append(os.path.realpath(\"..\"))\n",
    "else:\n",
    "    logging.info(\"Colab detected\")\n",
    "    drive.mount(\"/content/drive\")\n",
    "    sys.path.append(\"/content/drive/MyDrive/ecg-reconstruction/src\")\n",
    "\n",
    "from ecg.trainer import Trainer, TrainerConfig\n",
    "from ecg.reconstructor.cnn.cnn import StackedCNN\n",
    "from ecg.reconstructor.transformer.transformer import UFormer, NaiveTransformerEncoder\n",
    "from ecg.reconstructor.transformer.fastformer import (\n",
    "    Fastformer,\n",
    "    UFastformer,\n",
    "    FastformerPlus,\n",
    ")\n",
    "from ecg.reconstructor.lstm.lstm import LSTM, CNNLSTM\n",
    "from ecg.util.device import get_device\n",
    "from ecg.util.tree import deep_merge\n",
    "from ecg.util.path import get_project_root_dir\n",
    "\n",
    "\n",
    "def train_experiment(\n",
    "    trial: optuna.Trial,\n",
    "    base_config: TrainerConfig,\n",
    "    tuning_dir: Path,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    This is the main function for optuna to tune a model.\n",
    "    \"\"\"\n",
    "    reconstructor_type = base_config[\"reconstructor\"][\"type\"]\n",
    "    config = deep_merge(base_config, reconstructor_type.suggest_config(trial))\n",
    "\n",
    "    # The followings are the configs after tuning.\n",
    "    config[\"optimizer\"][\"args\"] = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 6e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 5e-6, 5e-2, log=True),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    config_stream = StringIO()\n",
    "    yaml.dump(config, config_stream, yaml.Dumper, indent=4)\n",
    "    logging.info(\"Config:\\n%s\", config_stream.getvalue())\n",
    "\n",
    "    config_to_log = config.copy()\n",
    "    del config_to_log[\"reconstructor\"]\n",
    "    logging.info(json.dumps(config_to_log, indent=4))\n",
    "    trainer = Trainer(config)\n",
    "    tuning_config_dir = tuning_dir / f\"trial_{trial.number}\"\n",
    "    with tuning_config_dir.open(\"w\", encoding=\"utf-8\") as config_file:\n",
    "        yaml.dump(trainer.config, config_file, Dumper=yaml.Dumper)\n",
    "    try:\n",
    "        loss = trainer.fit(trial=trial)\n",
    "    except RuntimeError:\n",
    "        error_stream = StringIO()\n",
    "        traceback.print_exc(file=error_stream)\n",
    "        logging.error(\"Training failed\\n%s\", error_stream.getvalue())\n",
    "        loss = None\n",
    "    del trainer\n",
    "    if get_device().type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    return loss\n",
    "\n",
    "\n",
    "base_config: TrainerConfig = {\n",
    "    \"in_leads\": [0, 1, 8],\n",
    "    \"out_leads\": [6, 7, 9, 10, 11],\n",
    "    \"max_epochs\": trial_epochs,\n",
    "    \"accumulate_grad_batches\": 1,\n",
    "    \"dataset\": {\n",
    "        \"common\": {\n",
    "            # \"predicate\": \"lambda f: f['SB'][:]\",\n",
    "            \"predicate\": None,\n",
    "            \"signal_dtype\": \"float32\",\n",
    "            \"filter_type\": \"butter\",\n",
    "            \"filter_args\": {\"N\": 3, \"Wn\": (0.5, 60), \"btype\": \"bandpass\"},\n",
    "            \"mean_normalization\": True,\n",
    "            \"feature_scaling\": False,\n",
    "            \"include_original_signal\": False,\n",
    "            \"include_filtered_signal\": False,\n",
    "            \"include_labels\": {},\n",
    "        },\n",
    "        \"train\": {\"hdf5_filename\": f\"{dataset_name}/train.hdf5\"},\n",
    "        \"eval\": {\"hdf5_filename\": f\"{dataset_name}/validation.hdf5\"},\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"common\": {\"num_workers\": 6},\n",
    "    },\n",
    "    \"reconstructor\": {\"type\": MODEL_TYPE},\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tuning_dir = get_project_root_dir() / \"src\" / \"tuning\" / MODEL_TYPE.__name__\n",
    "    tuning_dir.mkdir(exist_ok=True, parents=True)\n",
    "    experiment_name = f\"tuning_logs_{MODEL_TYPE.__name__}\"\n",
    "    storage = JournalStorage(\n",
    "        JournalFileStorage(str(tuning_dir / f\"{experiment_name}.log\"))\n",
    "    )\n",
    "    try:\n",
    "        study = optuna.load_study(\n",
    "            storage=storage,\n",
    "            study_name=experiment_name,\n",
    "        )\n",
    "    except:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=storage,  # Specify the storage URL here.\n",
    "            study_name=experiment_name,\n",
    "            pruner=optuna.pruners.MedianPruner(),\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "    trial_function = partial(\n",
    "        train_experiment, base_config=base_config, tuning_dir=tuning_dir\n",
    "    )\n",
    "    study.optimize(trial_function, n_trials=32)\n",
    "    best_number = study.best_trial.number\n",
    "    with open(tuning_dir / \"best_trial\", \"w\") as f:\n",
    "        f.write(f\"{best_number}\")\n",
    "    print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
