{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "os.chdir('../')\n",
    "# Preparation\n",
    "## Addition\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "\n",
    "# from model import GPTConfig, GPT\n",
    "from pe_info.model_nope import GPTConfig as GPTConfig_nope, GPT as GPT_nope\n",
    "from main_utils import *\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "resume_dir = None\n",
    "resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_entity = 'ssdd'\n",
    "wandb_log = True # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "exp_name = 'default_exp_name'\n",
    "# data\n",
    "train_data_path = 'train_3digit_10000.txt'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "test_batch_size = 128\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "val_data_path = 'val.bin'\n",
    "multi_digit = False\n",
    "num_digit = 5\n",
    "binary = False\n",
    "# using two data - data1 = text / data2 = addition\n",
    "train_both = False # use seperate text/add data for train/val (get_batch uses this to sample from two differernt datasets)\n",
    "data_ratio = 0.2 # ratio of data_path2 compared with data_path1\n",
    "train_data_path2 = 'train_addition.bin' # only used when train_both = True\n",
    "val_data_path2 = 'val_addition.bin'\n",
    "# evaluation\n",
    "eval_text = False # if True get perplexity using eval_text_data_path\n",
    "eval_text_data_path = None # directory to text data (.bin file) - ex. 'data/shakespeare_add_ar_mixed/val_text.bin' \n",
    "eval_addition = False # if True compute test accuracy of \"a+b=\"\n",
    "start = \"FILE:data/bal/test_10000.txt\"\n",
    "eval_addition_ar = False\n",
    "start_ar = None\n",
    "eval_other = False # use this to evaluate other operations (ex. train on operator '-' but evaluate on other_operator '+')\n",
    "start_other = None\n",
    "other_operator = '+'\n",
    "eval_addition_train = False\n",
    "start_train = None\n",
    "reverse_ab = False\n",
    "reverse_c = False\n",
    "zero_pad = False\n",
    "algo_reason = False\n",
    "add_space = False\n",
    "# model\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "ckpt_path_name = 'ckpt.pt'\n",
    "save_final = True\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = None # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "use_flash = True\n",
    "data_type='text'\n",
    "dataset = 'bal'\n",
    "operator = '+' # can be '+', '-', '*', 'sin', 'sqrt'\n",
    "data_shuffle = True\n",
    "# data_format = 'algo_reasoning' # 'plain' or 'reverse' or 'algo_reasoning'\n",
    "data_format = 'plain' # 'plain' or 'reverse' or 'algo_reasoning'\n",
    "\n",
    "vocabulary = 'all_ascii_chars' # can be 'all_ascii_chars' or 'numbers_only' or 'custom_input_data'\n",
    "meta_path_specified = False # use saved meta_file (False if data_type='text')\n",
    "eps = 0\n",
    "tokenizer = 'char' # by default, use char level tokenizer. but for pretrained models, use openai tokenizer eg: 'gpt2'\n",
    "\n",
    "simple=False\n",
    "random_A=False\n",
    "random_C=False\n",
    "\n",
    "use_lora = False # use lora (from minLoRA)\n",
    "print_interval = 2  # if we're using gpt-2 model, I want to see it prompted on text\n",
    "# -----------------------------------------------------------------------------\n",
    "# import ipdb; ipdb.set_trace()\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str, type(None)))]\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if min_lr == None:\n",
    "    min_lr = learning_rate/10\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "print('ddp: ', ddp)\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "torch.backends.cudnn.benchmark = True # cudnn auto-tuner\n",
    "torch.backends.cudnn.deterministic = False # cudnn auto-tuner\n",
    "\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "if data_type == 'binary':\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data = np.memmap(os.path.join(data_dir, train_data_path), dtype=np.uint16, mode='r')\n",
    "    val_data = np.memmap(os.path.join(data_dir, val_data_path), dtype=np.uint16, mode='r')\n",
    "    if train_both:\n",
    "        train_data2 = np.memmap(os.path.join(data_dir, train_data_path2), dtype=np.uint16, mode='r')\n",
    "        val_data2 = np.memmap(os.path.join(data_dir, val_data_path2), dtype=np.uint16, mode='r')\n",
    "    if eval_text:\n",
    "        if eval_text_data_path is None:\n",
    "            print('eval_text_data_path is None!!! No binary file to evaluate perplexity on.') \n",
    "        eval_text_data = np.memmap(eval_text_data_path, dtype=np.uint16, mode='r')\n",
    "    # test_data_str = None # test_data for addition testing will be handled with \"start\"\n",
    "    meta_path = None\n",
    "else:\n",
    "    # check for data_format\n",
    "    if data_type == 'text':\n",
    "        if (data_format == 'reverse' and not reverse_c) or (reverse_c and data_format != 'reverse'):\n",
    "            raise ValueError('reverse_c must be True for data_format == \"reverse\"')\n",
    "        elif (data_format == 'algo_reasoning' and not algo_reason) or (algo_reason and data_format != 'algo_reasoning'):\n",
    "            raise ValueError('algo_reason must be True for data_format == \"algo_reasoning\"')\n",
    "    meta_path_specified = False\n",
    "\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data_path = os.path.join(data_dir, train_data_path)\n",
    "    # val_data = os.path.join(data_dir, val_data_path)\n",
    "    train_data_list = get_data_list(train_data_path, operator=operator)\n",
    "    val_data_list = get_data_list(filename=None, operator=operator) # get_data_list(val_data, operator='+')\n",
    "    train_data_str = generate_data_str(train_data_list, operator=operator, format=data_format, train=True, shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    val_data_str = generate_data_str(val_data_list, operator=operator, format=data_format, train=True, shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    meta, meta_path, data_encoder, data_decoder = create_meta_file(vocabulary=vocabulary, input_data_str=train_data_str, tokenizer=tokenizer)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    train_data = data_encoder(train_data_str)\n",
    "    val_data = data_encoder(val_data_str)\n",
    "    if eval_addition_train and start_train is None:\n",
    "        # specify the start_train to be oour train data file\n",
    "        start_train = f\"FILE:{train_data_path}\"\n",
    "        \n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    if train_both:\n",
    "        data2 = train_data2 if split == 'train' else val_data2\n",
    "        batch_size2 = int(batch_size*data_ratio)\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size-batch_size2,))\n",
    "        ix2 = torch.randint(len(data2) - block_size, (batch_size2,))\n",
    "    else:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if train_both:\n",
    "        x2 = torch.stack([torch.from_numpy((data2[i:i+block_size]).astype(np.int64)) for i in ix2])\n",
    "        y2 = torch.stack([torch.from_numpy((data2[i+1:i+1+block_size]).astype(np.int64)) for i in ix2])\n",
    "        x = torch.cat([x,x2])\n",
    "        y = torch.cat([y,y2])    \n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "best_perplexity = 1e9 # on text data\n",
    "best_accuracy = -1 # on addition data\n",
    "\n",
    "if meta_path_specified:\n",
    "    # attempt to derive vocab_size from the dataset\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "    else:\n",
    "        meta_path = None\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "# determine the vocab size we'll use for from-scratch training\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "# gptconf = GPTConfig(**model_args)\n",
    "# model = GPT(gptconf)\n",
    "\n",
    "encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer)\n",
    "def load_checkpoint(ckpt_path, model_config, model_type, device='cuda', return_config=False):\n",
    "        # load ckpt into model\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "        model_args = checkpoint['model_args']\n",
    "        # for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                # model_args[k] = checkpoint_model_args[k]\n",
    "        # for k in checkpoint_model_args:\n",
    "        #         model_args[k] = checkpoint_model_args[k]\n",
    "        # create the model\n",
    "        original_gptconf = model_config(**model_args)\n",
    "        gptconf = model_config(**model_args)\n",
    "        model = model_type(original_gptconf)\n",
    "        state_dict = checkpoint['model']\n",
    "        # fix the keys of the state dictionary :(\n",
    "        # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "                if k.startswith(unwanted_prefix):\n",
    "                        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "        if return_config:\n",
    "                return model, gptconf\n",
    "        else:\n",
    "                return model\n",
    "\n",
    "def generate_output(model, prompt, max_new_tokens=5):\n",
    "    # temperature = 0.8\n",
    "    # top_k = 200\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "    # run generation\n",
    "    \n",
    "    start_ids = encode(prompt)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            num_samples = 1\n",
    "            for k in range(num_samples):\n",
    "                # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                y = model.generate(x, max_new_tokens, )\n",
    "\n",
    "                # print(decode(y[0].tolist()))\n",
    "                # print('---------------')\n",
    "    return decode(y[0].tolist())\n",
    "\n",
    "# data_type='text'\n",
    "# data_format='plain'\n",
    "# operator='+'\n",
    "# dataset = 'bal'\n",
    "# batch_size = 256\n",
    "# block_size = 256 # context of up to 256 previous characters\n",
    "# train_data_path = 'train_3digit_10000.txt'\n",
    "# # val_data_path = 'val.bin'\n",
    "# ckpt_path_name = 'out2/addition_plain/ckpt_final.pt'\n",
    "# eval_addition = True\n",
    "# reverse_c = False\n",
    "# start = \"FILE:data/bal/test_10000.txt\"\n",
    "# eval_addition_train = True\n",
    "\n",
    "## Operation\n",
    "\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "\n",
    "# from model import GPTConfig, GPT\n",
    "from pe_info.model_nope import GPTConfig as GPTConfig_nope, GPT as GPT_nope\n",
    "from main_utils import *\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "resume_dir = None\n",
    "resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_entity = 'ssdd'\n",
    "wandb_log = True # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "exp_name = 'default_exp_name'\n",
    "# data\n",
    "train_data_path = 'train_3digit_10000.txt'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "test_batch_size = 128\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "val_data_path = 'val.bin'\n",
    "multi_digit = False\n",
    "num_digit = 5\n",
    "binary = False\n",
    "# using two data - data1 = text / data2 = addition\n",
    "train_both = False # use seperate text/add data for train/val (get_batch uses this to sample from two differernt datasets)\n",
    "data_ratio = 0.2 # ratio of data_path2 compared with data_path1\n",
    "train_data_path2 = 'train_addition.bin' # only used when train_both = True\n",
    "val_data_path2 = 'val_addition.bin'\n",
    "# evaluation\n",
    "eval_text = False # if True get perplexity using eval_text_data_path\n",
    "eval_text_data_path = None # directory to text data (.bin file) - ex. 'data/shakespeare_add_ar_mixed/val_text.bin' \n",
    "eval_addition = False # if True compute test accuracy of \"a+b=\"\n",
    "start = \"FILE:data/bal/test_10000.txt\"\n",
    "eval_addition_ar = False\n",
    "start_ar = None\n",
    "eval_other = False # use this to evaluate other operations (ex. train on operator '-' but evaluate on other_operator '+')\n",
    "start_other = None\n",
    "other_operator = '+'\n",
    "eval_addition_train = False\n",
    "start_train = None\n",
    "reverse_ab = False\n",
    "reverse_c = False\n",
    "zero_pad = False\n",
    "algo_reason = False\n",
    "add_space = False\n",
    "# model\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "ckpt_path_name = 'ckpt.pt'\n",
    "save_final = True\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = None # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "use_flash = True\n",
    "data_type='text'\n",
    "dataset = 'bal'\n",
    "operator = '+' # can be '+', '-', '*', 'sin', 'sqrt'\n",
    "data_shuffle = True\n",
    "# data_format = 'algo_reasoning' # 'plain' or 'reverse' or 'algo_reasoning'\n",
    "data_format = 'plain' # 'plain' or 'reverse' or 'algo_reasoning'\n",
    "\n",
    "vocabulary = 'all_ascii_chars' # can be 'all_ascii_chars' or 'numbers_only' or 'custom_input_data'\n",
    "meta_path_specified = False # use saved meta_file (False if data_type='text')\n",
    "eps = 0\n",
    "tokenizer = 'char' # by default, use char level tokenizer. but for pretrained models, use openai tokenizer eg: 'gpt2'\n",
    "\n",
    "simple=False\n",
    "random_A=False\n",
    "random_C=False\n",
    "\n",
    "use_lora = False # use lora (from minLoRA)\n",
    "print_interval = 2  # if we're using gpt-2 model, I want to see it prompted on text\n",
    "\n",
    "general_seed = 1337\n",
    "# general_seed = 1227\n",
    "resume_metric_from_best = True\n",
    "use_pe = 'original'\n",
    "use_residual = True\n",
    "no_att_residual = False\n",
    "no_mlp_residual = False\n",
    "layerwise_pe = False\n",
    "permute = False\n",
    "not_causal = False\n",
    "\n",
    "causal_training=True\n",
    "\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str, type(None)))]\n",
    "exec(open('./pe_info/config2_pe/parity/jason_train_addition_bal.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "print(causal_training)\n",
    "causal_training = False\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "if data_type == 'binary':\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data = np.memmap(os.path.join(data_dir, train_data_path), dtype=np.uint16, mode='r')\n",
    "    val_data = np.memmap(os.path.join(data_dir, val_data_path), dtype=np.uint16, mode='r')\n",
    "    if train_both:\n",
    "        train_data2 = np.memmap(os.path.join(data_dir, train_data_path2), dtype=np.uint16, mode='r')\n",
    "        val_data2 = np.memmap(os.path.join(data_dir, val_data_path2), dtype=np.uint16, mode='r')\n",
    "    if eval_text:\n",
    "        if eval_text_data_path is None:\n",
    "            print('eval_text_data_path is None!!! No binary file to evaluate perplexity on.') \n",
    "        eval_text_data = np.memmap(eval_text_data_path, dtype=np.uint16, mode='r')\n",
    "    # test_data_str = None # test_data for addition testing will be handled with \"start\"\n",
    "    meta_path = None\n",
    "else:\n",
    "    # check for data_format\n",
    "    if data_type == 'text':\n",
    "        if ('reverse' in data_format and not reverse_c) or (reverse_c and 'reverse' not in data_format):\n",
    "            raise ValueError('reverse_c must be True for data_format == \"reverse\"')\n",
    "        elif (data_format == 'algo_reasoning' and not algo_reason) or (algo_reason and data_format != 'algo_reasoning'):\n",
    "            raise ValueError('algo_reason must be True for data_format == \"algo_reasoning\"')\n",
    "    meta_path_specified = False\n",
    "\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data_path = os.path.join(data_dir, train_data_path)\n",
    "    # val_data = os.path.join(data_dir, val_data_path)\n",
    "    train_data_list = get_data_list(train_data_path, operator=operator) # a list of (x, y, op)\n",
    "    val_data_list = get_data_list(filename=None, operator=operator) # get_data_list(val_data, operator='+')\n",
    "    train_data_str = generate_data_str(train_data_list, operator=operator, format=data_format, train=True, shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    val_data_str = generate_data_str(val_data_list, operator=operator, format=data_format, train=True, shuffle=data_shuffle, add_space=add_space, simple=simple, random_A=random_A, random_C=random_C)\n",
    "    meta, meta_path, data_encoder, data_decoder = create_meta_file(vocabulary=vocabulary, input_data_str=train_data_str, tokenizer=tokenizer)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    train_data = data_encoder(train_data_str)\n",
    "    val_data = data_encoder(val_data_str)\n",
    "    if eval_addition_train and start_train is None:\n",
    "        # specify the start_train to be our train data file\n",
    "        start_train = f\"FILE:{train_data_path}\"\n",
    "        \n",
    "    if train_both:\n",
    "        # This is for the case where we use two different datasets for training\n",
    "        # we sample from both with a specified ratio - data_ratio\n",
    "        # TODO: let's leave this here for now.\n",
    "        train_data2 = np.memmap(os.path.join(data_dir, train_data_path2), dtype=np.uint16, mode='r')\n",
    "        val_data2 = np.memmap(os.path.join(data_dir, val_data_path2), dtype=np.uint16, mode='r')\n",
    "    \n",
    "    if eval_text:\n",
    "        # eval_text_data = np.memmap(eval_text_data_path, dtype=np.uint16, mode='r')\n",
    "        text_data_list = get_data_list(eval_text_data_path, operator='text')\n",
    "        text_data_str = generate_data_str(text_data_list, operator='text', format=data_format, train=False, shuffle=False)\n",
    "        eval_text_data = data_encoder(text_data_str)\n",
    "\n",
    "\n",
    "    \n",
    "space_token = data_encoder(' ')[0]\n",
    "switch_line_token = data_encoder('\\n')[0]\n",
    "# non_causal_fix_length = 15\n",
    "# non_causal_fix_length = 27\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        if train_both:\n",
    "            data2 = train_data2 if split == 'train' else val_data2\n",
    "            batch_size2 = int(batch_size*data_ratio)\n",
    "            ix = torch.randint(len(data) - block_size, (batch_size-batch_size2,))\n",
    "            ix2 = torch.randint(len(data2) - block_size, (batch_size2,))\n",
    "        else:\n",
    "            if causal_training:\n",
    "                ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "            else:\n",
    "                split_points = np.where(data==(switch_line_token))[0]\n",
    "                split_points = np.hstack([np.array([0]), split_points.flatten()])\n",
    "\n",
    "                diff_split_points = np.diff(split_points)\n",
    "                start_points = split_points[:-1]\n",
    "\n",
    "                randidx = np.random.permutation(len(start_points))[:batch_size]\n",
    "                ix = start_points[randidx]\n",
    "                diff_split_points = diff_split_points[randidx]\n",
    "\n",
    "        if causal_training:\n",
    "            x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "            y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "        else:\n",
    "            x = torch.stack([torch.from_numpy(np.pad(data[ix[i]:ix[i]+diff_split_points[i]-1].astype(np.int64), \n",
    "                                            (non_causal_fix_length-diff_split_points[i], 0),  # this pads space at front\n",
    "                                            mode='constant', \n",
    "                                            constant_values=space_token)) for i in range(len(ix))])\n",
    "            y = torch.stack([torch.from_numpy((data[ix[i]+diff_split_points[i]-1:ix[i]+1+diff_split_points[i]-1]).astype(np.int64)) for i in range(len(ix))])\n",
    "\n",
    "        if train_both:\n",
    "            x2 = torch.stack([torch.from_numpy((data2[i:i+block_size]).astype(np.int64)) for i in ix2])\n",
    "            y2 = torch.stack([torch.from_numpy((data2[i+1:i+1+block_size]).astype(np.int64)) for i in ix2])\n",
    "            x = torch.cat([x,x2])\n",
    "            y = torch.cat([y,y2])    \n",
    "\n",
    "        if device_type == 'cuda':\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "split= \"train\"\n",
    "# data = train_data if split == 'train' else val_data\n",
    "# if train_both:\n",
    "#     data2 = train_data2 if split == 'train' else val_data2\n",
    "#     batch_size2 = int(batch_size*data_ratio)\n",
    "#     ix = torch.randint(len(data) - block_size, (batch_size-batch_size2,))\n",
    "#     ix2 = torch.randint(len(data2) - block_size, (batch_size2,))\n",
    "# else:\n",
    "#     if causal_training:\n",
    "#         ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#     else:\n",
    "#         split_points = np.where(data==(switch_line_token))[0]\n",
    "#         split_points = np.hstack([np.array([0]), split_points.flatten()])\n",
    "\n",
    "#         diff_split_points = np.diff(split_points)\n",
    "#         start_points = split_points[:-1]\n",
    "\n",
    "#         randidx = np.random.permutation(len(start_points))[:batch_size]\n",
    "#         ix = start_points[randidx]\n",
    "#         diff_split_points = diff_split_points[randidx]\n",
    "\n",
    "# if causal_training:\n",
    "#     x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "#     y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "# else:\n",
    "#     x = torch.stack([torch.from_numpy(np.pad(data[ix[i]:ix[i]+diff_split_points[i]-1].astype(np.int64), \n",
    "#                                     (non_causal_fix_length-diff_split_points[i], 0),  # this pads space at front\n",
    "#                                     mode='constant', \n",
    "#                                     constant_values=space_token)) for i in range(len(ix))])\n",
    "#     y = torch.stack([torch.from_numpy((data[ix[i]+diff_split_points[i]-1:ix[i]+1+diff_split_points[i]-1]).astype(np.int64)) for i in range(len(ix))])\n",
    "\n",
    "# if train_both:\n",
    "#     x2 = torch.stack([torch.from_numpy((data2[i:i+block_size]).astype(np.int64)) for i in ix2])\n",
    "#     y2 = torch.stack([torch.from_numpy((data2[i+1:i+1+block_size]).astype(np.int64)) for i in ix2])\n",
    "#     x = torch.cat([x,x2])\n",
    "#     y = torch.cat([y,y2])    \n",
    "\n",
    "# if device_type == 'cuda':\n",
    "#     # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "#     x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "# else:\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "\n",
    "x, y = get_batch('train')\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "print(data_decoder(x[0].cpu().numpy()), data_decoder(y[0].cpu().numpy()))   \n",
    "for xi in x[0].cpu().numpy():\n",
    "    print(data_decoder(xi[..., None]), end=',')\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "best_perplexity = 1e9 # on text data\n",
    "best_accuracy = -1 # on addition data\n",
    "\n",
    "if meta_path_specified:\n",
    "    # attempt to derive vocab_size from the dataset\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "    else:\n",
    "        meta_path = None\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "# determine the vocab size we'll use for from-scratch training\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "# gptconf = GPTConfig(**model_args)\n",
    "# model = GPT(gptconf)\n",
    "\n",
    "encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer)\n",
    "# Predicting on reverse\n",
    "## Define functions \n",
    "import torch.nn.functional as F\n",
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= model.config.block_size else idx[:, -model.config.block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, _ = model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        print((probs<0).any())\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "## Model Examination: Original\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.decomposition import PCA \n",
    "def PCA_analysis(prompt, embs, out_text, config_dir):\n",
    "  pca = PCA(n_components=2)\n",
    "  new_x = pca.fit_transform(embs.cpu().numpy())\n",
    "  data = []\n",
    "  for i, (text, pt) in enumerate(zip(prompt, new_x)):\n",
    "      trace = go.Scatter(\n",
    "          x=[pt[0]],\n",
    "          y=[pt[1]],\n",
    "          mode='markers+text',\n",
    "          marker=dict(size=10),  # Adjust the size of the points\n",
    "          text=[str(i+1)],\n",
    "          textposition='middle center',  # Center the text within the marker\n",
    "          name=text,\n",
    "          textfont=dict(\n",
    "            family='Times New Rotman',  # Specify the font family\n",
    "            size=18,  # Adjust the font size\n",
    "            color='black',  # Adjust the font color\n",
    "          ),\n",
    "      )\n",
    "      data.append(trace)\n",
    "\n",
    "  layout = go.Layout(\n",
    "      xaxis=dict(title='Principal Component 1'),\n",
    "      yaxis=dict(title='Principal Component 2'),\n",
    "      title=f'PCA visualization for {prompt}'\n",
    "  )\n",
    "\n",
    "  fig = go.Figure(data=data, layout=layout)\n",
    "  fig.show()\n",
    "  out_num = out_text.split('=')[-1][:-1]\n",
    "  eqn = out_text.split('=')[0]\n",
    "  out_text = eqn+'='+out_num[::-1]+out_text[-1]\n",
    "  print(out_text)\n",
    "  # print(new_x)\n",
    "  print(pca.explained_variance_ratio_)\n",
    "  import plotly.io as pio\n",
    "  pio.write_html(fig, f'./{config_dir}/{prompt}.html')\n",
    "# config_dir = \"out2/addition_reverse/\"\n",
    "\n",
    "# ckpt = f\"{config_dir}/ckpt_10000_final.pt\"\n",
    "# import yaml\n",
    "# with open(f'{config_dir}/addition_reverse/config.yaml') as f:\n",
    "#   config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# gptconf = GPTConfig_nope(**model_args)\n",
    "# model = GPT_nope(gptconf)\n",
    "# model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope)\n",
    "# model = load_checkpoint(ckpt, GPTConfig, GPT)\n",
    "\n",
    "\n",
    "import yaml\n",
    "# config_dir = \"out3/addition_reverse_res=[0, 2, 3, 4, 5]\"\n",
    "# model_config_fold = 'addition_reverse_res=[0, 2, 3, 4, 5]'\n",
    "# config_dir = \"out3/addition_reverse_res=[0, 1, 2, 3, 4, 5]\"\n",
    "# model_config_fold = 'addition_reverse_res=[0, 1, 2, 3, 4, 5]'\n",
    "\n",
    "# config_dir = \"out3/addition_reverse_res=[0, 2, 3, 4, 5]_lwp1\"\n",
    "# model_config_fold = 'addition_reverse_res=[0, 2, 3, 4, 5]_lwp1'\n",
    "\n",
    "# config_dir = \"out3/addition_reverse_res=[0, 2, 3, 4, 5]_lwp2\"\n",
    "# model_config_fold = 'addition_reverse_res=[0, 2, 3, 4, 5]_lwp2'\n",
    "\n",
    "# config_dir = \"out3/addition_reverse_res=[0, 2, 3, 4, 5]_lwp3\"\n",
    "# model_config_fold = 'addition_reverse_res=[0, 2, 3, 4, 5]_lwp3'\n",
    "\n",
    "# config_dir = \"out3/addition_reverse_res=[0, 2, 3, 4, 5]_lwp4\"\n",
    "# model_config_fold = 'addition_reverse_res=[0, 2, 3, 4, 5]_lwp4'\n",
    "\n",
    "\n",
    "# with open(f'{config_dir}/{model_config_fold}/config.yaml') as f:\n",
    "#   config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# config_dict['start']\n",
    "# ckpt = f\"{config_dir}/ckpt_10000_final.pt\"\n",
    "# model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda')\n",
    "\n",
    "exp_list = [\n",
    "  # [\"../outputs/out3/addition_reverse_res=[0, 2, 3, 4, 5]\",  \"addition_reverse_res=[0, 2, 3, 4, 5]\"],\n",
    "  # [\"../outputs/out3/addition_reverse_res=[0, 1, 2, 3, 4, 5]\",  \"addition_reverse_res=[0, 1, 2, 3, 4, 5]\"],\n",
    "  # [\"../outputs/out3/addition_reverse_res=[0, 2, 3, 4, 5]_lwp1\",  \"addition_reverse_res=[0, 2, 3, 4, 5]_lwp1\"],\n",
    "\n",
    "  # [\"./outputs/out4_1202_2/addition_reverse_sd888_res=[2, 3, 4, 5]\",  \"addition_reverse_sd888_res=[2, 3, 4, 5]\"],\n",
    "  # [\"./outputs/out4_1202_2/addition_reverse_sd888_res=[2, 3, 4, 5]_lwp01\",  \"addition_reverse_sd888_res=[2, 3, 4, 5]_lwp01\"],\n",
    "  # [\"./outputs/out4_1201/addition_reverse_sd888_res=[0, 2, 3, 4, 5]\",  \"addition_reverse_sd888_res=[0, 2, 3, 4, 5]\"],\n",
    "  # [\"./outputs/out4_1201/addition_reverse_sd888_res=[0, 2, 3, 4, 5]_lwp1\",  \"addition_reverse_sd888_res=[0, 2, 3, 4, 5]_lwp1\"],\n",
    "  \n",
    "  # [\"./outputs/out3_control/addition_reverse\",  \"addition_reverse\"],\n",
    "  # [\"./outputs/out3/addition_reverse_res=[0, 1, 2, 3, 4, 5]\", \"addition_reverse_res=[0, 1, 2, 3, 4, 5]\"],\n",
    "    \n",
    "  # \"./outputs/out3_control/addition_reverse_res=[0, 1, 2, 3, 4, 5]_2312081936\",\n",
    "  # # \"./outputs/out4_1201/addition_reverse_sd888_res=[0, 2, 3, 4, 5]\",\n",
    "  # # \"./outputs/out3_control/no_compile_addition_reverse_sd888_sin_res=[0, 1, 2, 3, 4, 5]_nc012345_2401181525\",\n",
    "  # # \"./outputs/out4_1203/addition_reverse_sd888_res=[3, 4, 5]_lwp012\",\n",
    "  # # \"./outputs/out3_control/addition_reverse_sd888_sin__T2401212129\",\n",
    "  # # \"./outputs/out4_1203/addition_reverse_sd888_res=[3, 4, 5]\",\n",
    "  # \"./outputs/out4_1201/addition_reverse_sd888_res=[1, 2, 3, 4, 5]\",\n",
    "  # \"./outputs/out4_1202_2/addition_reverse_sd888_res=[2, 3, 4, 5]\",\n",
    "  # \"./outputs/out4_1203/normal_init_addition_reverse_sd888_res=[3, 4, 5]_T2401271423\",\n",
    "  # \"./outputs/out3/addition_reverse_res=[0, 1, 2, 3, 4, 5]\",\n",
    "  # \"./outputs/out4_1203/T2401281707_addition_reverse_sd888_res=[3, 4, 5]\",\n",
    "    \n",
    "  # './outputs/residual_exp/addition_reverse_sd111_T2402040225_res=[0, 1, 2, 3, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd222_T2402040249_res=[0, 1, 2, 3, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd333_T2402040314_res=[0, 1, 2, 3, 4, 5]',\n",
    "\n",
    "  # './outputs/residual_exp/addition_reverse_sd555_T2402030324_res=[1, 2, 3, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd444_T2402030115_res=[1, 2, 3, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd333_T2402012051_res=[1, 2, 3, 4, 5]',\n",
    "\n",
    "  # './outputs/residual_exp/addition_reverse_sd555_T2402030428_res=[0, 1, 2, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd444_T2402030219_res=[0, 1, 2, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd333_T2402012135_res=[0, 1, 2, 4, 5]',\n",
    "\n",
    "  # './outputs/residual_exp/addition_reverse_sd555_T2402021605_res=[2, 3, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd444_T2402021415_res=[2, 3, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd333_T2402011832_res=[2, 3, 4, 5]',\n",
    "  \n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd111_*_res=[0, 1, 2, 3, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd222_*_res=[0, 1, 2, 3, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd333_*_res=[0, 1, 2, 3, 4, 5]',\n",
    "\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd555_*_res=[1, 2, 3, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd444_*_res=[1, 2, 3, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd333_*_res=[1, 2, 3, 4, 5]',\n",
    "\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd555_*_res=[0, 1, 2, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd444_*_res=[0, 1, 2, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd333_*_res=[0, 1, 2, 4, 5]',\n",
    "  \n",
    "\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd555_*_res=[2, 3, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd444_*_res=[2, 3, 4, 5]',\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd333_*_res=[2, 3, 4, 5]',\n",
    "\n",
    "# ==============================\n",
    "\n",
    "\n",
    "  # './outputs/nope_residual_exp/addition_reverse_sd555_*_nope_res=[1, 2, 3, 4, 5]',\n",
    "\n",
    "  # './outputs/residual_exp/addition_reverse_sd555_T2402030306_res=[3, 4, 5]',\n",
    "  # './outputs/residual_exp/addition_reverse_sd444_T2402022123_res=[3, 4, 5]',\n",
    "\n",
    "# './outputs/nope_residual_exp/addition_reverse_sd222_T2402011546_nope_res=[1, 2, 3, 4, 5]',\n",
    "# './outputs/nope_residual_exp/addition_reverse_sd222_T2402012307_nope_res=[0, 1, 2, 3, 4]',\n",
    "\n",
    "# './outputs/nope_residual_exp/addition_reverse_sd333_T2402040302_nope_res=[0, 1, 2, 3, 4, 5]',\n",
    "# './outputs/nope_residual_exp/addition_reverse_sd222_T2402011323_nope_res=[2, 3, 4, 5]',\n",
    "# './outputs/nope_residual_exp/addition_reverse_sd666_T2402020458_nope_res=[2, 3, 4, 5]',\n",
    "\n",
    "  # [\"./outputs/out4_1202_2/addition_reverse_sd888_res=[2, 3, 4, 5]\",  \"addition_reverse_sd888_res=[2, 3, 4, 5]\"],\n",
    "  # [\"./outputs/out4_1201/addition_reverse_sd888_res=[0, 2, 3, 4, 5]_lwp1\",  \"addition_reverse_sd888_res=[0, 2, 3, 4, 5]_lwp1\"],\n",
    "    \n",
    "# ==============================\n",
    "  \n",
    "  './outputs/parity_residual_exp/oddc_sd240_T2403201845_res=[0, 1, 2, 3, 4, 5]',\n",
    "\n",
    "]\n",
    "# exp_list\n",
    "import glob\n",
    "# exp_list = glob.glob('./outputs/residual_exp/*') + glob.glob('./outputs/nope_residual_exp/*')\n",
    "exp_list = [[x, x.split('/')[-1]] for x in exp_list]\n",
    "\n",
    "# calc ratio\n",
    "# increase contrast\n",
    "## Attention Map\n",
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "fixed_length = 7\n",
    "\n",
    "total_tokens = 2048\n",
    "sample_num = 1024\n",
    "level = 1\n",
    "\n",
    "model_list = []\n",
    "\n",
    "# remove output for load_checkpoint\n",
    "from IPython.utils import io\n",
    "\n",
    "for idx, (config_dir, model_config_fold) in enumerate(exp_list):\n",
    "  glob_dir = config_dir.replace('[', '[[]')\n",
    "  yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "  revised_glob_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "  exp_list[idx][0] = revised_glob_dir\n",
    "  exp_list[idx][1] = revised_glob_dir.split('/')[-1]\n",
    "  \n",
    "  with open(yaml_path) as f:\n",
    "    config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "  ckpt = f\"{revised_glob_dir}/ckpt_10000_acc.pt\"\n",
    "\n",
    "  with io.capture_output() as captured:\n",
    "    # model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "    model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "    # gptconfig.not_causal = [1]*6  \n",
    "\n",
    "    # gptconfig.use_pe = 'sin'\n",
    "    # model = GPT_nope(gptconfig)\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  model_list.append(model)\n",
    "\n",
    "\n",
    "input_var_list = [list() for _ in range(len(model_list))]\n",
    "res_list = [list() for _ in range(len(model_list))]\n",
    "\n",
    "X_list, Y_list = [], []\n",
    "for b in range(1):\n",
    "  x, y = get_batch('train')\n",
    "  X_list.append(x)\n",
    "  Y_list.append(y)\n",
    "\n",
    "X = torch.cat(X_list, dim=0)\n",
    "Y = torch.cat(Y_list, dim=0)\n",
    "\n",
    "\n",
    "X = ''.join([decode(X[i].tolist()) for i in range(X.shape[0])])[:total_tokens]\n",
    "\n",
    "if fixed_length == 8:\n",
    "  X_n = list(map(lambda x: x[:fixed_length], filter(lambda x: len(x)>=fixed_length and x[fixed_length-1]=='=', X.split('\\n'))))\n",
    "  X = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "  # print('random shuffle in place')\n",
    "  # X = X[..., torch.randperm(X.shape[-1])]\n",
    "else:\n",
    "  # X = '6' * total_tokens\n",
    "  X_n = np.array(list(X[:len(X)//fixed_length*fixed_length])).reshape(-1, fixed_length)\n",
    "  # X_n = list(map(lambda x: x[:fixed_length], X))\n",
    "  X = torch.tensor(list(map(lambda x: encode(x), X_n)))\n",
    "  print('random shuffle in place')\n",
    "  X = X[..., torch.randperm(X.shape[-1])]\n",
    "# X = X.reshape(-1, fixed_length)\n",
    "# X = X[:sample_num]\n",
    "X = X.to(device)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "for midx, model in enumerate(tqdm(model_list)): \n",
    "  activation = {}\n",
    "  def getActivation(name, f=lambda x: x):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "      activation[name] = f(output.detach())\n",
    "    return hook\n",
    "  # register forward hooks on the layers of choice\n",
    "  for i in range(len(model.transformer.h)):\n",
    "    # head_split = lambda x: x.split(384, dim=2)[0] # get the query\n",
    "    # q_proj = model.transformer.h[i].attn.c_attn.register_forward_hook(getActivation(f'pre_attn_proj', head_split))\n",
    "    q_proj = model.transformer.h[i].attn.c_attn.register_forward_hook(getActivation(f'pre_attn_proj'))\n",
    "\n",
    "    raw_q = model.transformer.h[i].ln_1.register_forward_hook(getActivation(f'raw_q'))\n",
    "    # h2 = model.transformer.ln_f.register_forward_hook(getActivation('x_out'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "      _ = model(X)\n",
    "    \n",
    "\n",
    "    q_proj.remove()\n",
    "    q, k, v = activation['pre_attn_proj'].split(gptconfig.n_embd, dim=2)\n",
    "    rq = activation['raw_q']\n",
    "    input_var_list[midx].append([q, k, v, rq])\n",
    "  \n",
    "  for prompt in X_n:\n",
    "    out_text = generate_output(model, prompt, max_new_tokens=5)\n",
    "    res_list[midx].append(out_text.split(\"=\")[-1][::-1])\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def standardize_rows(matrix):\n",
    "    \"\"\"Standardize each row of the matrix.\"\"\"\n",
    "    mean = matrix.mean()\n",
    "    std = matrix.std()\n",
    "    return (matrix - mean) / std\n",
    "  \n",
    "@widgets.interact(model_idx=(0, len(model_list)-1), layer_idx=(0, 5), head_idx=(0, 5), batch_idx=(0, len(X)-1))\n",
    "def plot_attention_weights(model_idx, layer_idx, head_idx, batch_idx):\n",
    "  print(exp_list[model_idx][0])\n",
    "  # print(exp_list[model_idx])\n",
    "  try:\n",
    "    print(X_n[batch_idx] + str(res_list[model_idx][batch_idx]).replace(\"$\", ''))\n",
    "  except:\n",
    "    pass\n",
    "  q, k, v, x = input_var_list[model_idx][layer_idx]\n",
    "  print(x.shape)\n",
    "\n",
    "  # print(xdotx.shape)\n",
    "  # NOTE: pure self multiplication stacking through layers produces the pattern\n",
    "  # k = q ## This one give u the pattern\n",
    "\n",
    "  # TODO: finish projection thing\n",
    "  # pca_k = PCA(n_components=64).fit_transform(k.cpu().numpy())\n",
    "  # pca_qk = PCA(n_components=64).fit(q.cpu().numpy()).transform(k.cpu().numpy())    \n",
    "\n",
    "  B, T, C = q.shape\n",
    "  k = k.view(B, T, gptconfig.n_head, C // gptconfig.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "  q = q.view(B, T, gptconfig.n_head, C // gptconfig.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "  # att_w = np.matmul(q_proj, k_proj)\n",
    "  scores = torch.matmul(q, k.permute(0, 1, 3, 2)) / (gptconfig.n_embd ** 0.5)\n",
    "  x = x.view(B, T, gptconfig.n_head, C // gptconfig.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "  x_scores = torch.matmul(x, x.permute(0, 1, 3, 2)) / (gptconfig.n_embd ** 0.5)\n",
    "  q_scores = torch.matmul(q, q.permute(0, 1, 3, 2)) / (gptconfig.n_embd ** 0.5)\n",
    "  k_scores = torch.matmul(k, k.permute(0, 1, 3, 2)) / (gptconfig.n_embd ** 0.5)\n",
    "  x_norm =  (x**2).sum(dim=-1, keepdim=True)**0.5\n",
    "  q_norm = (q**2).sum(dim=-1, keepdim=True)**0.5\n",
    "  k_norm = (k**2).sum(dim=-1, keepdim=True)**0.5\n",
    "  x_theta_score = x_scores / (x_norm*x_norm)\n",
    "  qk_theta_score = scores / (q_norm*k_norm)\n",
    "  kk_theta_score = k_scores / (k_norm*k_norm)\n",
    "\n",
    "  x_h_scores = x_scores[:, head_idx, :, :]\n",
    "  qk_h_scores = scores[:, head_idx, :, :]\n",
    "  q_h_scores = q_scores[:, head_idx, :, :]\n",
    "  k_h_scores = k_scores[:, head_idx, :, :]\n",
    "\n",
    "  x_h_theta_scores = x_theta_score[:, head_idx, :, :]\n",
    "  qk_h_theta_scores = qk_theta_score[:, head_idx, :, :]\n",
    "  kk_h_theta_scores = kk_theta_score[:, head_idx, :, :]\n",
    "  \n",
    "\n",
    "  u1 = x_h_theta_scores.flatten().detach().cpu().numpy()\n",
    "  # u2 = qk_h_theta_scores.flatten().detach().cpu().numpy()\n",
    "  u2 = kk_h_theta_scores.flatten().detach().cpu().numpy()\n",
    "\n",
    "  \n",
    "  u1_standardized = standardize_rows(u1)\n",
    "  # u1_standardized = standardize_rows(qk_h_scores.flatten().detach().cpu().numpy())\n",
    "\n",
    "  # u2_standardized = standardize_rows(scores.flatten().detach().cpu().numpy())\n",
    "\n",
    "  u2_standardized = standardize_rows(u2)\n",
    "  # u2_standardized = standardize_rows(q_h_scores.flatten().detach().cpu().numpy())\n",
    "  # u2_standardized = standardize_rows(q_h_scores.flatten().detach().cpu().numpy())\n",
    "\n",
    "  \n",
    "  # Compute the correlation matrix\n",
    "  corr_mat = np.dot(u1_standardized, u2_standardized.T) / (u1_standardized.shape[0] )\n",
    "  print(corr_mat)\n",
    "  \n",
    "  # corr_dist = u1_standardized * u2_standardized.T\n",
    "  # plt.plot(u1_standardized, u2_standardized, 'o')\n",
    "  # plt.scatter(u1_standardized, u2_standardized)\n",
    "  plt.scatter(u1, u2, s=5, alpha=0.3)\n",
    "  \n",
    "  print(scores.shape)\n",
    "  # sz = scores.shape[-1]\n",
    "  # scores[..., np.arange(sz), np.arange(sz)] = float('-inf')\n",
    "\n",
    "\n",
    "  # Apply the softmax activation to obtain attention weights\n",
    "  attention_weights = F.softmax(scores, dim=-1).detach().cpu().numpy()\n",
    "  x_attention_weights = F.softmax(x_scores, dim=-1).detach().cpu().numpy()\n",
    "  q_attention_weights = F.softmax(q_scores, dim=-1).detach().cpu().numpy()\n",
    "  distr = attention_weights.mean(axis=0)\n",
    "  # input_var_list[model_idx][0] = attention_weights.detach().cpu().numpy()\n",
    "  \n",
    "  # TODO show attmap using seaborn heatmap\n",
    "  import seaborn as sns\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  sns.heatmap(attention_weights[batch_idx][head_idx], vmin=0, vmax=1)\n",
    "  # sns.heatmap(q_attention_weights[batch_idx][head_idx], vmin=0, vmax=1)\n",
    "  plt.show()\n",
    "  normalize = lambda x: (x-x.mean())/x.std()\n",
    "  for cidx, curve in enumerate(distr[head_idx]):\n",
    "    plt.plot(normalize(curve**2)*0.5 - cidx)\n",
    "  plt.show()\n",
    "  import plotly.express as px\n",
    "  \n",
    "  #  fig = px.line()\n",
    "\n",
    "  # for cidx, curve in enumerate(distr[head_idx]):\n",
    "  #     normalized_curve = normalize(curve**2) * 0.5 - cidx\n",
    "  #     fig.add_scatter(y=normalized_curve)\n",
    "\n",
    "  # fig.show()\n",
    "    # TODO: show att_map plot using plotly\n",
    "    # import plotly.graph_objs as go\n",
    "    # adjust plot size to make figure like a squar\n",
    "\n",
    "    # fig = go.Figure(data=go.Heatmap(z=input_var_list[i][0][0][0], zmin=0, zmax=1))\n",
    "    # fig.update_layout(\n",
    "    #   autosize=False,\n",
    "    #   width=500,\n",
    "    #   height=500,\n",
    "    #   margin=dict(\n",
    "    #     l=50,\n",
    "    #     r=50,\n",
    "    #     b=100,\n",
    "    #     t=100,\n",
    "    #     pad=4\n",
    "    #   ),\n",
    "    #   paper_bgcolor=\"LightSteelBlue\",\n",
    "    # )\n",
    "    # fig.show()\n",
    "# confirm what happends with the bar\n",
    "# scipy correlation (verify that it is the same)\n",
    "1. For initialized original NanoGPT, after computing the attention weights using different shuffled inputs and combine them to approximate an attention weight distribution, for the followings:\n",
    "    1. distribution for NoPE at the first layer\n",
    "    2. distribution for NoPE at the 2nd layer\n",
    "    3. distribution for NoPE at the 3rd layer\n",
    "    4. distribution for NoPE at the last layer\n",
    "    - ![image.png](attachment:image.png) ![image-4.png](attachment:image-4.png) ![image-5.png](attachment:image-5.png) ![image-7.png](attachment:image-7.png)\n",
    "\n",
    "2. For initialized NoPE NanoGPT, after computing the attention weights using different shuffled inputs and combine them to approximate an attention weight distribution, for the followings:\n",
    "    1. distribution for NoPE at the first layer\n",
    "    2. distribution for NoPE at the 2nd layer\n",
    "    3. distribution for NoPE at the 3rd layer\n",
    "    4. distribution for NoPE at the last layer\n",
    "\n",
    "    - ![image-6.png](attachment:image-6.png) ![image-2.png](attachment:image-2.png) ![image-3.png](attachment:image-3.png) ![alt text](image.png)\n",
    "\n",
    "3. Even when all tokens of the sequence are the same, for NoPE, the distribution will be broken due to numerical error.\n",
    "\n",
    "\n",
    "4. For trained original NanoGPT, after computing the attention weights using different shuffled inputs and combine them to approximate an attention weight distribution, for the followings:\n",
    "    1. distribution for NoPE at the first layer\n",
    "    2. distribution for NoPE at the 2nd layer\n",
    "    3. distribution for NoPE at the 3rd layer\n",
    "    4. distribution for NoPE at the last layer\n",
    "    - ![image-11.png](attachment:image-11.png) ![image-13.png](attachment:image-13.png) ![image-14.png](attachment:image-14.png) ![image-15.png](attachment:image-15.png)\n",
    "\n",
    "5. For trained NoPE NanoGPT, after computing the attention weights using different shuffled inputs and combine them to approximate an attention weight distribution, for the followings:\n",
    "    1. distribution for NoPE at the first layer\n",
    "    2. distribution for NoPE at the 2nd layer\n",
    "    3. distribution for NoPE at the 3rd layer\n",
    "    4. distribution for NoPE at the 4th layer\n",
    "\n",
    "    - ![image-8.png](attachment:image-8.png) ![image-9.png](attachment:image-9.png) ![image-10.png](attachment:image-10.png) ![image-12.png](attachment:image-12.png)\n",
    "look at initialization of QK, different initialization to see if it is the same pattern.\n",
    "fixing up thet paper (naccl)\n",
    "### Scratch calculation\n",
    "import numpy as np\n",
    "s = np.exp([1,2]).sum()\n",
    "np.exp(1)/s, np.exp(2)/s\n",
    "import numpy as np\n",
    "s = np.exp([1, 1, 2]).sum()\n",
    "np.exp(1)/s, np.exp(2)/s\n",
    "## Activation Correlation\n",
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "\n",
    "fixed_length = 8\n",
    "sample_num = 1024\n",
    "all_level_input_act_list = []\n",
    "\n",
    "model_list = []\n",
    "for idx, (config_dir, model_config_fold) in enumerate(exp_list):\n",
    "  glob_dir = config_dir.replace('[', '*').replace(']', '*')\n",
    "  try:\n",
    "\n",
    "    yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "    revised_glob_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "    exp_list[idx][0] = revised_glob_dir\n",
    "    exp_list[idx][1] = revised_glob_dir.split('/')[-1]\n",
    "\n",
    "    config_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "    with open(yaml_path) as f:\n",
    "      config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    csv_path = f\"{config_dir}/results.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "  except:\n",
    "    continue\n",
    "\n",
    "  ckpt = f\"{config_dir}/ckpt_10000_acc.pt\"\n",
    "  # model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda')\n",
    "  model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "  \n",
    "  # model = GPT_nope(gptconfig)\n",
    "\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  model_list.append(model)\n",
    "\n",
    "\n",
    "for level in range(1,7):\n",
    "  level = level - 1\n",
    "  input_act1_list = [list() for _ in range(len(model_list))]\n",
    "\n",
    "  for i in range(0, 10): # try 5 batches\n",
    "    X, Y = get_batch('train')\n",
    "    X = decode(X[0].tolist())\n",
    "    X_8 = list(map(lambda x: x[:fixed_length], filter(lambda x: len(x)>=fixed_length and x[fixed_length-1]=='=', X.split('\\n'))))\n",
    "    X = torch.tensor(list(map(lambda x: encode(x), X_8)))\n",
    "    # X = X.reshape(-1, fixed_length)\n",
    "    # X = X[:sample_num]\n",
    "    X = X.to(device)\n",
    "    # print(X.shape)\n",
    "\n",
    "    for midx, model in enumerate(model_list): \n",
    "      activation = {}\n",
    "      def getActivation(name):\n",
    "        # the hook signature\n",
    "        def hook(model, input, output):\n",
    "          activation[name] = output.detach()\n",
    "        return hook\n",
    "      # register forward hooks on the layers of choice\n",
    "      if level >=0:\n",
    "        h1 = model.transformer.h[level].register_forward_hook(getActivation(f'layer_{level}'))\n",
    "        h2 = model.transformer.ln_f.register_forward_hook(getActivation('x_out'))\n",
    "      else:\n",
    "        h1 = model.transformer.wte.register_forward_hook(getActivation(f'layer_{level}'))\n",
    "        h2 =  model.transformer.ln_f.register_forward_hook(getActivation('x_out'))\n",
    "\n",
    "      with torch.no_grad():\n",
    "        _ = model(X)\n",
    "\n",
    "      h1.remove()\n",
    "      h2.remove()\n",
    "\n",
    "      acts = activation[f'layer_{level}'].detach().cpu().numpy()\n",
    "      input_act1_list[midx].append(acts)\n",
    "\n",
    "\n",
    "  for i in range(len(input_act1_list)):\n",
    "    print(len(input_act1_list[i]))\n",
    "    cur_input_act1 = np.concatenate(input_act1_list[i], axis=0)\n",
    "    bs, l, dim = cur_input_act1.shape\n",
    "    # print(cur_input_act1.shape)\n",
    "    input_act1_list[i] = cur_input_act1.reshape(bs, l*dim)\n",
    "    \n",
    "  all_level_input_act_list.append(input_act1_list)\n",
    "  \n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def standardize_rows(matrix):\n",
    "    \"\"\"Standardize each row of the matrix.\"\"\"\n",
    "    mean = matrix.mean(axis=1, keepdims=True)\n",
    "    std = matrix.std(axis=1, keepdims=True)\n",
    "    return (matrix - mean) / std\n",
    "\n",
    "@widgets.interact(idx1=(0, len(input_act1_list)-1), idx2=(0, len(input_act1_list)-1), level=(0,5))\n",
    "def get_corr(idx1, level=0, save=False,  abs=True, save_all=False, is_rand_init=False):\n",
    "    rand_state = 'init_' if is_rand_init else ''\n",
    "    if not save_all:\n",
    "        idx2 = idx1\n",
    "        input_act1_list = all_level_input_act_list[level]\n",
    "        u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "        u1 = u1.T\n",
    "        u2 = u2.T\n",
    "        print(exp_list[idx1][0].split('sd')[-1], exp_list[idx2][0].split('sd')[-1])\n",
    "\n",
    "        print(u2.shape)\n",
    "\n",
    "        # Standardize each row of u1 and u2\n",
    "        u1_standardized = standardize_rows(u1)\n",
    "        u2_standardized = standardize_rows(u2)\n",
    "        # u1_standardized = u1\n",
    "        # u2_standardized = u2\n",
    "        \n",
    "\n",
    "        # Compute the correlation matrix\n",
    "        corr_mat = np.dot(u1_standardized, u2_standardized.T) / (u1.shape[1] )\n",
    "        if abs:\n",
    "            corr_mat = np.abs(corr_mat)\n",
    "        else:\n",
    "            pass\n",
    "        vec_dim = corr_mat.shape[0]//8\n",
    "        total_sum = np.abs(corr_mat).sum()\n",
    "        block_sum = 0\n",
    "        for i in range(0, len(corr_mat), vec_dim):\n",
    "            block_sum += np.abs(corr_mat[i:i+vec_dim, i:i+vec_dim]).sum()\n",
    "        ratio = block_sum/(total_sum-block_sum)\n",
    "\n",
    "        # Assuming you have calculated 'corr_mat' as described in the previous answer\n",
    "\n",
    "        # Create a heatmap of corr_mat\n",
    "        plt.figure(figsize=(12, 10), dpi=120)\n",
    "        # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "        plt.imshow(corr_mat, cmap='coolwarm', interpolation='nearest')\n",
    "\n",
    "        extra_text = 'Absolute ' if abs else ''\n",
    "        plt.colorbar(label=f'{extra_text}Correlation Coefficient')\n",
    "\n",
    "        # Set axis labels and title\n",
    "        # plt.xlabel('U2 Entries')\n",
    "        # plt.ylabel('U1 Entries')\n",
    "\n",
    "        # Show the plot\n",
    "        nope1 = 'nope_' if 'nope' in exp_list[idx1][1] else ''\n",
    "        u1_name = '_'.join(exp_list[idx1][1].split('_')[2:])\n",
    "        u1_name = nope1 + u1_name.split('_')[-1] + '_' + '_'.join(u1_name.split('_')[:-1])\n",
    "        nope2 = 'nope_' if 'nope' in exp_list[idx1][1] else ''\n",
    "        u2_name = '_'.join(exp_list[idx2][1].split('_')[2:])\n",
    "        u2_name = nope2+u2_name.split('_')[-1] + '_' +'_'.join(u2_name.split('_')[:-1])\n",
    "\n",
    "        extra_self = 'Self' if u1_name == u2_name else ''\n",
    "        # plt.title(f'{extra_self} Correlation Matrix for  {ratio:.2f}')\n",
    "        print(u1_name, u2_name)\n",
    "        if save:\n",
    "            os.makedirs('./saved_plots_corr/', exist_ok=True)\n",
    "            plt.savefig(f'./saved_plots_corr/{rand_state}_{u1_name}_VS_{u2_name}_layer{level}_{ratio:.03f}_{abs}.svg')\n",
    "        \n",
    "        # close img\n",
    "        # plt.close()\n",
    "        plt.show()\n",
    "    else:\n",
    "        for level in range(len(all_level_input_act_list)):\n",
    "            input_act1_list = all_level_input_act_list[level]\n",
    "            for idx1 in tqdm(range(len(input_act1_list))):\n",
    "                idx2 = idx1\n",
    "                u1, u2 = input_act1_list[idx1], input_act1_list[idx2]\n",
    "                u1 = u1.T\n",
    "                u2 = u2.T\n",
    "\n",
    "                u1_standardized = standardize_rows(u1)\n",
    "                u2_standardized = standardize_rows(u2)\n",
    "                # u1_standardized = u1\n",
    "                # u2_standardized = u2\n",
    "                \n",
    "\n",
    "                # Compute the correlation matrix\n",
    "                corr_mat = np.dot(u1_standardized, u2_standardized.T) / (u1.shape[1] )\n",
    "                if abs:\n",
    "                    corr_mat = np.abs(corr_mat)\n",
    "                else:\n",
    "                    pass\n",
    "                vec_dim = corr_mat.shape[0]//8\n",
    "                total_sum = np.abs(corr_mat).sum()\n",
    "                block_sum = 0\n",
    "                for i in range(0, len(corr_mat), vec_dim):\n",
    "                    block_sum += np.abs(corr_mat[i:i+vec_dim, i:i+vec_dim]).sum()\n",
    "                ratio = block_sum/(total_sum-block_sum)\n",
    "\n",
    "                # Assuming you have calculated 'corr_mat' as described in the previous answer\n",
    "\n",
    "                # Create a heatmap of corr_mat\n",
    "                plt.figure(figsize=(12, 10), dpi=120)\n",
    "                # plt.imshow(corr_mat, cmap='seismic', interpolation='nearest')\n",
    "                plt.imshow(corr_mat, cmap='coolwarm', interpolation='nearest')\n",
    "\n",
    "                extra_text = 'Absolute ' if abs else ''\n",
    "                plt.colorbar(label=f'{extra_text}Correlation Coefficient')\n",
    "\n",
    "                # Set axis labels and title\n",
    "                # plt.xlabel('U2 Entries')\n",
    "                # plt.ylabel('U1 Entries')\n",
    "\n",
    "                # Show the plot\n",
    "                nope1 = 'nope_' if 'nope' in exp_list[idx1][1] else ''\n",
    "                u1_name = '_'.join(exp_list[idx1][1].split('_')[2:])\n",
    "                u1_name = nope1 + u1_name.split('_')[-1] + '_' + '_'.join(u1_name.split('_')[:-1])\n",
    "                nope2 = 'nope_' if 'nope' in exp_list[idx1][1] else ''\n",
    "                u2_name = '_'.join(exp_list[idx2][1].split('_')[2:])\n",
    "                u2_name = nope2+u2_name.split('_')[-1] + '_' +'_'.join(u2_name.split('_')[:-1])\n",
    "\n",
    "                extra_self = 'Self' if u1_name == u2_name else ''\n",
    "                # plt.title(f'{extra_self} Correlation Matrix for  {ratio:.2f}')\n",
    "                # print(ratio)\n",
    "                # print(u1_name, u2_name)\n",
    "                os.makedirs('./saved_plots_corr/', exist_ok=True)\n",
    "                plt.savefig(f'./saved_plots_corr/{rand_state}{u1_name}_VS_{u2_name}_layer{level}_{ratio:.03f}_{abs}.svg')\n",
    "                \n",
    "                # close img\n",
    "                plt.close()\n",
    "                # plt.show()\n",
    "\n",
    "# idxes = [\n",
    "# # [0,0], [0,1], [0,2], [0,3], [0,4],\n",
    "# # [1,1], [2,2], [3,3], [4,4],\n",
    "# # [4,1], [4,2], [4,3], \n",
    "# [1,2], [1,3],\n",
    "# ]\n",
    "# from tqdm.auto import tqdm\n",
    "# for idx1, idx2 in tqdm(idxes):\n",
    "#     get_corr(idx1, idx2, save=True)\n",
    "    \n",
    "# all residual vs picking ones that are illustrative\n",
    "TH = 0.9\n",
    "print((corr_mat >= TH).sum())\n",
    "idxes = np.nonzero(corr_mat >= TH)\n",
    "size = np.vstack(idxes).max()+1\n",
    "sub_mat = np.zeros((size, size))\n",
    "sub_mat[idxes] = 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have calculated 'corr_mat' as described in the previous answer\n",
    "\n",
    "# Create a heatmap of corr_mat\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "plt.imshow(sub_mat, cmap='grey', interpolation='nearest')\n",
    "# plt.colorbar(label=)\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('u2 Index')\n",
    "plt.ylabel('u1 Index')\n",
    "plt.title('Where Correlation Coefficient >= 0.9')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import pearsonr\n",
    "# @widgets.interact(idx=(0, len(u1)-1), idx2=(0, len(u2)-1), continuous_update=True)\n",
    "@widgets.interact(idx=(0,len(idxes[0])), save=[False, True], continuous_update=True,)\n",
    "def show(idx, save=False):\n",
    "  idx, idx2 = idxes[0][idx], idxes[1][idx]\n",
    "  plt.figure(figsize=(4, 3), dpi=100)\n",
    "  plt.scatter(u1[idx, :], u2[idx2, :], alpha=0.5)\n",
    "  print(pearsonr(u1[idx, :], u2[idx2, :])[0])\n",
    "  plt.title(f\"U1[{idx}] vs U2[{idx2}] (corr={pearsonr(u1[idx, :], u2[idx2, :])[0]:.3f})\")\n",
    "  # plt.show()\n",
    "  if save:\n",
    "    os.makedirs(f'./saved_plots/{u1_name}_VS_{u2_name}/', exist_ok=True)\n",
    "    plt.savefig(f'./saved_plots/{u1_name}_VS_{u2_name}/U1[{idx}]_vs_U2[{idx2}].svg')\n",
    "  plt.show()\n",
    "  # return\n",
    "## PCA visualizaiton\n",
    "glob.glob(f\"{config_dir}/**\", recursive=True)\n",
    "config_dir\n",
    "f'{glob_dir}/ckpt_**_acc.ckpt'\n",
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "from IPython.utils import io\n",
    "\n",
    "\n",
    "input_act1_list = []\n",
    "# for config_dir, model_config_fold in exp_list:\n",
    "#   with open(f'{config_dir}/{model_config_fold}/config.yaml') as f:\n",
    "#     config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "#   ckpt = f\"{config_dir}/ckpt_10000_final.pt\"\n",
    "#   model = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda')\n",
    "\n",
    "for config_dir, model_config_fold in exp_list:\n",
    "  glob_dir = config_dir.replace('[', '*').replace(']', '*')\n",
    "  yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "  config_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "  with open(yaml_path) as f:\n",
    "    config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "  # ckpt = glob.glob(f\"{config_dir}/ckpt_**_acc.pt\", recursive=True)[0]\n",
    "  ckpt = glob.glob(f'{glob_dir}/ckpt_**_acc.pt')[0]\n",
    "  with io.capture_output() as captured:\n",
    "    # model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "    model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "    # gptconfig.use_pe = 'sin'\n",
    "    # model = GPT_nope(gptconfig)\n",
    "\n",
    "  cur_input_act1_list = []\n",
    "  # for i in range(0, 3):\n",
    "  # model.transformer.h[0].permute = False\n",
    "  # prompts = [\n",
    "  #   f\"${823}\" + '+' + f\"{8}\"*3 + '=',\n",
    "  #   f\"${238}\" + '+' + f\"{8}\"*3 + '='\n",
    "  # ]\n",
    "  zs = np.zeros(12).astype(np.int64)\n",
    "  n_1s = 3\n",
    "  zs[np.random.permutation(12)[:n_1s]] = 1\n",
    "  str_zs = ''.join(map(str, zs))\n",
    "  prompts = [\n",
    "    # f'\\nparity({str_zs})=',\n",
    "    # '\\nparity(100101010000)='\n",
    "    # '\\nparity(010101000101)='\n",
    "    # '\\nparity(000010101001)=',\n",
    "    'parity(001111000001)='\n",
    "  ]\n",
    "  # prompts = [\n",
    "  #   f\"${623}\" + '+' + f\"{5}\"*3 + '=',\n",
    "  #   f\"${632}\" + '+' + f\"{5}\"*3 + '='\n",
    "  # ]\n",
    "  for i in range(0, 1): # try 10 batches\n",
    "\n",
    "    # prompt = \"$\" + f\"{i}\"*3 + f\"{i}\"*6  \n",
    "    # prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "    # prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "    prompt = prompts[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation = {}\n",
    "    def getActivation(name):\n",
    "      # the hook signature\n",
    "      def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "      return hook\n",
    "    # register forward hooks on the layers of choice\n",
    "    h1 = model.transformer.h[1].register_forward_hook(getActivation('layer_1'))\n",
    "    h2 = model.transformer.ln_f.register_forward_hook(getActivation('x_out'))\n",
    "    # out_text = generate_output(model, prompt, max_new_tokens=5)\n",
    "    out_text = generate_output(model, prompt, max_new_tokens=1)\n",
    "\n",
    "\n",
    "    h1.remove()\n",
    "    h2.remove()\n",
    "    model_name = config_dir.split('/')[-1]\n",
    "    print(model_name)\n",
    "    PCA_analysis(prompt, activation['x_out'][0], out_text, config_dir)\n",
    "    \n",
    "\n",
    "\n",
    "from threading import main_thread\n",
    "from pe_investigation import main_utils\n",
    "start_train = None\n",
    "reverse_ab = False\n",
    "reverse_c = False\n",
    "zero_pad = False\n",
    "algo_reason = False\n",
    "add_space = False\n",
    "config['causal_training'] = False\n",
    "\n",
    "main_utils.evaluate_addition_batch(config, model, ctx, encode, decode, verbose=True, num_digit=num_digit, zero_pad=zero_pad, \n",
    "                                                    reverse_ab=reverse_ab, reverse_c=reverse_c, algo_reason=algo_reason, \n",
    "                                                    binary=binary, data_type=data_type, operator=operator, data_format=data_format)\n",
    "interesting: for a causal model with pe, with or without \"\\n\" makes a 180 degree difference in outcome !!!!!\n",
    "## Original_model_pe_PCA\n",
    "x =  model.transformer.wpe.weight.cpu().detach().numpy()\n",
    "x.shape\n",
    "x =  model.transformer.wpe.weight.cpu().detach().numpy()\n",
    "from sklearn.decomposition import PCA\n",
    "# select sample maybe from test set\n",
    "# but if, different digits seems to be encoded the same way, than it has a patter\n",
    "pca = PCA(n_components=2)\n",
    "new_x = pca.fit_transform(x)\n",
    "new_x = new_x[::16]\n",
    "import matplotlib.pyplot as plt\n",
    "for text, pt in zip(range(len(new_x)), new_x, ):\n",
    "  plt.scatter(pt[0], pt[1], label=text)\n",
    "  \n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(new_x)\n",
    "print(pca.explained_variance_ratio_)\n",
    "## Probing\n",
    "gptconfig\n",
    "exp_list\n",
    "# prompt = \"$\" + f\"{i}\"*3 + '+' + f\"{i}\"*3 + '='\n",
    "import glob\n",
    "fixed_length = 8\n",
    "sample_num = 1024\n",
    "all_level_input_act_list = []\n",
    "\n",
    "model_list = []\n",
    "for idx, (config_dir, model_config_fold) in enumerate(exp_list):\n",
    "  glob_dir = config_dir.replace('[', '[[]')\n",
    "  yaml_path = glob.glob(f'{glob_dir}/**/config.yaml')[0]\n",
    "  revised_glob_dir = '/'.join(yaml_path.split('/')[:-2])\n",
    "  exp_list[idx][0] = revised_glob_dir\n",
    "  exp_list[idx][1] = revised_glob_dir.split('/')[-1]\n",
    "  \n",
    "  with open(yaml_path) as f:\n",
    "    config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "  ckpt = f\"{revised_glob_dir}/ckpt_10000_acc.pt\"\n",
    "  model, gptconfig = load_checkpoint(ckpt, GPTConfig_nope, GPT_nope, device='cuda', return_config=True)\n",
    "\n",
    "  # gptconfig.not_causal = [1]*6  \n",
    "  # model = GPT_nope(GPTConfig_nope())\n",
    "  # model = GPT_nope(gptconfig)\n",
    "\n",
    "  # for i in range(len(model.transformer.h)):\n",
    "  #   model.transformer.h[i].attn._reset_parameters()\n",
    "  #   model.transformer.h[i].mlp._reset_parameters()\n",
    "  # model.apply(model._init_weights)\n",
    "\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  model_list.append(model)\n",
    "\n",
    "\n",
    "for level in range(0, 8):\n",
    "  level = level - 1\n",
    "  input_act1_list = [list() for _ in range(len(model_list))]\n",
    "\n",
    "  for i in range(0, 1): # try 5 batches\n",
    "    X, Y = get_batch('train')\n",
    "    X = decode(X[0].tolist())\n",
    "    X_8 = list(map(lambda x: x[:fixed_length], filter(lambda x: len(x)>=fixed_length and x[fixed_length-1]=='=', X.split('\\n'))))\n",
    "    X_8 = [''.join(list(np.array(list(x))[np.random.permutation(len(x))])) for x in X_8]\n",
    "    X = torch.tensor(list(map(lambda x: encode(x), X_8)))\n",
    "    # X = X.reshape(-1, fixed_length)\n",
    "    # X = X[:sample_num]\n",
    "    X = X.to(device)\n",
    "\n",
    "    for midx, model in enumerate(model_list): \n",
    "      activation = {}\n",
    "      def getActivation(name):\n",
    "        # the hook signature\n",
    "        def hook(model, input, output):\n",
    "          activation[name] = output.detach()\n",
    "        return hook\n",
    "      # register forward hooks on the layers of choice\n",
    "\n",
    "      if level == 6:\n",
    "        h1 = model.transformer.ln_f.register_forward_hook(getActivation(f'layer_{level}'))\n",
    "      elif level == -1:\n",
    "        h1 = model.transformer.wte.register_forward_hook(getActivation(f'layer_{level}'))\n",
    "      else:\n",
    "        h1 = model.transformer.h[level].ln_1.register_forward_hook(getActivation(f'layer_{level}'))\n",
    "        \n",
    "\n",
    "      with torch.no_grad():\n",
    "        _ = model(X)\n",
    "\n",
    "      h1.remove()\n",
    "\n",
    "      acts = activation[f'layer_{level}'].detach().cpu().numpy()\n",
    "      input_act1_list[midx].append(acts)\n",
    "      # outs = activation['x_out'].detach().cpu().numpy()\n",
    "      # input_act1_list[midx].append(outs)\n",
    "\n",
    "\n",
    "  for i in range(len(input_act1_list)):\n",
    "    print(len(input_act1_list[i]))\n",
    "    cur_input_act1 = np.concatenate(input_act1_list[i], axis=0)\n",
    "    bs, l, dim = cur_input_act1.shape\n",
    "    targets = np.zeros((bs, l)) + np.arange(l)[None, ...]\n",
    "    print(cur_input_act1.shape)\n",
    "    input_act1_list[i] = (cur_input_act1, targets)\n",
    "    \n",
    "  all_level_input_act_list.append(input_act1_list)\n",
    "  \n",
    "len(all_level_input_act_list[0][1][1])\n",
    "\n",
    "# do a scklearn linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def crossing(X, y):\n",
    "    X_cross = []\n",
    "    y_cross = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            if i != j:\n",
    "                X_cross.append(X[i] * X[j])\n",
    "                y_cross.append(np.abs(y[i] - y[j]))\n",
    "    X = np.array(X_cross)\n",
    "    y = np.array(y_cross)\n",
    "    return X, y\n",
    "\n",
    "@widgets.interact(layer=(-1, 6), model_idx=(0, len(all_level_input_act_list[0])-1))\n",
    "def probe_layer(layer=-1, model_idx = 0):\n",
    "    layer = layer + 1\n",
    "    print(exp_list[model_idx][0])\n",
    "    X = all_level_input_act_list[layer][model_idx][0]\n",
    "    y = all_level_input_act_list[layer][model_idx][1]\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # X = np.random.rand(*y.shape,10)\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    y = y.reshape(-1)\n",
    "    X_train, X_test = X[:int(len(X)*0.8)], X[int(len(X)*0.8):]\n",
    "    y_train, y_test = y[:int(len(X)*0.8)], y[int(len(X)*0.8):]\n",
    "\n",
    "    X_train, y_train = crossing(X, y)\n",
    "    X_test, y_test = crossing(X, y)\n",
    "\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    # X_test = X_train\n",
    "    # y_test = y_train\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(mean_squared_error(y_test, y_pred))\n",
    "    print(reg.score(X_test, y_test))\n",
    "    print(y_test[:10])\n",
    "    print(y_pred[:10])\n",
    "\n",
    "    # print('coef:', reg.coef_)\n",
    "    plt.plot(reg.coef_)\n",
    "    plt.show()\n",
    "\n",
    "    mav = []\n",
    "    for i in range(10):\n",
    "        plt.plot(X_test[i])\n",
    "        mav.append(np.abs(X_test[i]).mean())\n",
    "    plt.show()\n",
    "\n",
    "    print(list(zip(y_test[:10], mav)))\n",
    "\n",
    "    mav = []\n",
    "    for i in range(len(y_test)):\n",
    "        mav.append(np.abs(X_test[i]).mean())\n",
    "\n",
    "\n",
    "    # plt.scatter(y_test, mav, s=20, alpha=0.2)\n",
    "    # plt.show()\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    \n",
    "    plt.scatter(y_test, y_pred, s=20, alpha=0.2)\n",
    "\n",
    "# normalize the input and do again\n",
    "1. Nope and original behaves differently\n",
    "2. When organized and unorganized, the activation output from the trained model is different, meaning that the model somehow also semantically managed the position of numbers and symbos \n",
    "3. For original pe, the regression model must be memorizing the absolute position initialized randomly at the start of the model. But that didn't explain why NOPE can also get positions right? Then there must be something permanent inside the model that indicates the positions, emm, such as a fixed bias?\n",
    "4. If looking closely at the activations from layers without skip \n",
    "\n",
    "\n",
    "Maybe check why noncausal still doesn't work\n",
    "* check random initialization problem\n",
    "\n",
    "* Want to actually check that empirically Qx \\cdot Ky is maximized\n",
    "(usually) when x\\approx y\n",
    "    - could be Wk making W_ky more similar to W_kx\n",
    "    - \\sum a_i b_i (W_Q Z_i) (W_k Z_i) where Z_i can be the pca basis / or any other spectral decomposition\n",
    "\n",
    "... an evidence that residual connection is preserving the locality of r.vec. x\n",
    "\n",
    "... Hypothesis: signs tend to be the same for z1=z2 and different otherwise\n",
    "\n",
    "* Want to compute the rank of\n",
    "    - with\n",
    "        - (1) Transformers with random initialization\n",
    "        - (2) Transformers at convergence\n",
    "\n",
    "    - for\n",
    "       - (a) full residual connections\n",
    "       - (b) no residual connections\n",
    "       - (c) some residual connections\n",
    "\n",
    "check rank degeneration: PCA -> compute the ratio \n",
    "- i.e. a1^2 / sum(ai^2) \n",
    "- plz see how the paper measures the rank degeneration \n",
    "\n",
    "* Want to check what happens when the residual connections we ablate\n",
    "are not consecutive (both non-consecutive layers, and when things\n",
    "taken out are pre-MLP/post-MLP)\n",
    "\n",
    "\n",
    "* Fix the description of the correlation img\n",
    "    - Collect the ratio from the graph and put into a table\n",
    "    - generate the image for all experiments we have\n",
    "- emperically validate qk, if vectors points in the same direction, othen kx dot qy should be high (for trained k,q)\n",
    "    - Plot the correlation on this: QxdotKx vs xdoty \n",
    "- description for correlation matrix\n",
    "\n",
    "or x being actual inputs $x \\in R^d$, project x using K and Q (for a lot of x) would be generally a projection into to the same subspace, namely $Kx \\cdot Qx$ be high\n",
    "    - PCA on K{x}, project on the first 100 components\n",
    "    - project on the first 100 components of Q{x}\n",
    "\n",
    "$K{x} : subspace {Kx| x \\in R^d}$\n",
    "compare projecting K{x} on the principal components of Q{x} to projecting K{x} on the standard basis\n",
    "(n.b., projecting on the first 100 components of the standard basis is just taking the first 100 coordinates)\n",
    "idx = 24\n",
    "plt.plot(X_train[idx])\n",
    "print(y_train[:10])\n",
    "print(reg.predict(X_train[:10][ :]))\n",
    "print(X_train[:10].sum(axis=-1))\n",
    "\n",
    "## define a torch lstm model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "## use a tokenizer from the bert model\n",
    "# bert_model = transformers.pipeline('sentiment-analysis', top_k=None)\n",
    "\n",
    "# class Distilbert_LSTM_regressor(nn.Module):\n",
    "#     def __init__(self, input_size=768, hidden_size=256, num_layers=3, bidirectional=True):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         # self.bert_model = bert_model\n",
    "#         self.embedding = bert_model.model.distilbert.eval()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "#         out_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "#         self.fc = nn.Linear(out_size, 1)\n",
    "#         self.double()\n",
    "#     def forward(self, x):\n",
    "#         with torch.no_grad():\n",
    "#             bert_embeddings = self.embedding(**x)\n",
    "#         self.lstm.flatten_parameters()\n",
    "#         out, _ = self.lstm(bert_embeddings.last_hidden_state)\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "\n",
    "class Linear_regressor(nn.Module):\n",
    "    def __init__(self, input_size=, hidden_size=256, num_layers=3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(50265, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        out_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc = nn.Linear(out_size, 1)\n",
    "        self.double()\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(embeddings)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class LSTM_regressor(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=256, num_layers=3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(50265, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        out_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc = nn.Linear(out_size, 1)\n",
    "        self.double()\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(embeddings)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class LLM_LSTM_regressor(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # self.bert_model = bert_model\n",
    "        # input_size = llm.config.d_model\n",
    "        input_size = llm.config.n_embd\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        out_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc = nn.Linear(out_size, 1)\n",
    "        # self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(x.last_hidden_state)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "        \n",
    "class Sentiment_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.max_len = tokenizer.model_max_length\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        nle_input = self.df.nle[idx]\n",
    "        label = self.df.max_shap_value[idx]\n",
    "        # label = self.df.ratio_shap_value[idx]\n",
    "\n",
    "        return nle_input, label  \n",
    "\n",
    "def collate_fn_base(data, tokenizer):\n",
    "    nle_input, label = zip(*data)\n",
    "    nle_input = tokenizer(nle_input, return_tensors=\"pt\", padding=True)\n",
    "    nle_input = {k: v.to(device) for k, v in nle_input.items()}\n",
    "    return nle_input, torch.tensor(label).to(device).reshape(-1,1).float()\n",
    "\n",
    "# def collate_fn_base(data, tokenizer):\n",
    "#     nle_input, label = zip(*data)\n",
    "#     nle_input = tokenizer(nle_input, return_tensors=\"pt\", padding=True)\n",
    "#     nle_input = nle_input.input_ids.to(device)\n",
    "#     return nle_input, torch.tensor(label).to(device).reshape(-1,1)\n",
    "\n",
    "model = LLM_LSTM_regressor().to(device)\n",
    "# model = LSTM_regressor().to(device)\n",
    "\n",
    "# tokenizer = bert_model.tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('nlpcloud/instruct-gpt-j-fp16')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('tiiuae/falcon-7b')\n",
    "org_call_one = tokenizer._call_one\n",
    "\n",
    "from functools import wraps\n",
    "@wraps(org_call_one)\n",
    "def _call_one_wrapped(*x, **y):\n",
    "    y['return_token_type_ids'] = False\n",
    "    return org_call_one(*x, **y)\n",
    "\n",
    "tokenizer._call_one = _call_one_wrapped\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "collate_fn = partial(collate_fn_base, tokenizer=tokenizer)\n",
    "\n",
    "train_size = int(0.7 * len(sample_df))\n",
    "val_size = int(0.15 * len(sample_df))\n",
    "\n",
    "# np.random.seed(42)\n",
    "np.random.seed(66)\n",
    "idxes = np.random.permutation(len(sample_df))\n",
    "\n",
    "train_idxes, val_idxes, test_idxes = idxes[:train_size], idxes[train_size:train_size+val_size], idxes[train_size+val_size:]\n",
    "train_dataset = Sentiment_Dataset(sample_df.loc[train_idxes, :].copy().reset_index(), tokenizer)\n",
    "val_dataset = Sentiment_Dataset(sample_df.loc[val_idxes, :].copy().reset_index(), tokenizer)\n",
    "test_dataset = Sentiment_Dataset(sample_df.loc[test_idxes, :].copy().reset_index(), tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "    batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "    batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "## collate function doesnt work with n workers > 0?\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "\n",
    "# best_val_loss = np.inf\n",
    "from torch import autocast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "for tlm in model_list:\n",
    "    tlm.eval()\n",
    "\n",
    "\n",
    "    best_peasonr = -np.inf\n",
    "    for epoch in (ep_disc:=tqdm(range(n_epochs:=32))):\n",
    "        model.train()\n",
    "        y_true, y_pred, train_loss = [], [], []\n",
    "        for (xs, ys) in tqdm(train_dataloader, leave=True):\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                xs = tlm()\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                out = model(xs)\n",
    "                loss = criterion(out, ys)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            y_true.append(ys.detach().cpu().numpy().flatten())\n",
    "            y_pred.append(out.detach().cpu().numpy().flatten())\n",
    "        y_true, y_pred = np.concatenate(y_true), np.concatenate(y_pred)\n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_PearsonR = scipy.stats.pearsonr(y_pred, y_true)[0]\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred, val_loss = [], [], []\n",
    "        for (xs, ys) in val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                xs = llm(xs['input_ids'])\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    out = model(xs)\n",
    "                    loss = criterion(out, ys)\n",
    "            y_true.append(ys.cpu().numpy().flatten())\n",
    "            y_pred.append(out.cpu().numpy().flatten())\n",
    "            val_loss.append(loss.item())\n",
    "        val_loss = np.mean(val_loss)\n",
    "        y_true, y_pred = np.concatenate(y_true), np.concatenate(y_pred)\n",
    "        PearsonR = scipy.stats.pearsonr(y_pred, y_true)[0]\n",
    "        print(f'epoch: {epoch}, train_loss {train_loss:.3f}, PearsonR: {train_PearsonR:.3f}, val_loss: {val_loss:.3f}, PearsonR: {PearsonR:.3f}')\n",
    "        if PearsonR > best_peasonr:\n",
    "            best_peasonr = PearsonR\n",
    "            torch.save(model.state_dict(), f'../generated_nle/{exp_name}/{file_prefix}_best_model.pth')\n",
    "            print('saved best model')`\n",
    "# start = 'FILE:data/bal/test_10000.txt'\n",
    "# start = config_dict['start']\n",
    "# lines = get_test_data(data_type=data_type, start=start)\n",
    "# print(lines[:10])\n",
    "\n",
    "# prompt = \"et tu brute\"\n",
    "# print(generate_output(model, prompt, max_new_tokens=1))\n",
    "\n",
    "# prompt = \"198+843=\"\n",
    "# prompt = \"$223+221=\"\n",
    "# prompt = \"$ 333+333=\"\n",
    "# prompt = \"$000000000\"\n",
    "for i in range(0, 3):\n",
    "  prompt = \"$\" + f\"{i}\"*6\n",
    "\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  # if compile:\n",
    "  #   model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "    # run generation\n",
    "    \n",
    "  start_ids = encode(prompt)\n",
    "  x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "  with torch.no_grad():\n",
    "    y = model.generate(x, max_new_tokens=5, )\n",
    "    # y = generate(model, x, max_new_tokens=5)\n",
    "    tok_emb = model.transformer.wte(x) # token embeddings of shape (b, t, n_embd)\n",
    "          # pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "          # x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    x = model.transformer.drop(tok_emb)\n",
    "\n",
    "    for block in model.transformer.h:\n",
    "        x = block(x)\n",
    "    x = model.transformer.ln_f(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "    # logits = model.lm_head(x)\n",
    "      # embeddings = model\n",
    "  # decode(y[0].tolist())\n",
    "  from sklearn.decomposition import PCA\n",
    "  # select sample maybe from test set\n",
    "  # but if, different digits seems to be encoded the same way, than it has a patter\n",
    "  pca = PCA(n_components=2)\n",
    "  new_x = pca.fit_transform(x[0].cpu().numpy())\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "  for text, pt in zip(prompt, new_x, ):\n",
    "    plt.scatter(pt[0], pt[1], label=text)\n",
    "    \n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  print(new_x)\n",
    "  print(pca.explained_variance_ratio_)\n",
    "# Testing Model\n",
    "test_model(model, reverse_c=True, precise=True, noise_digit=2)\n",
    "test_model(model, reverse_c=reverse_c, precise=False, noise_digit=2)\n",
    "\n",
    "\n",
    "# Reverse\n",
    "data_type='text'\n",
    "data_format='reverse'\n",
    "operator='+'\n",
    "dataset = 'bal'\n",
    "batch_size = 256\n",
    "block_size = 256 # context of up to 256 previous characters\n",
    "train_data_path = 'train_3digit_10000.txt'\n",
    "# val_data_path = 'val.bin'\n",
    "ckpt_path_name = 'out2/addition_reverse/ckpt_10000_final.pt'\n",
    "reverse_c = True\n",
    "eval_addition = True\n",
    "start = \"FILE:data/bal/test_10000.txt\"\n",
    "eval_addition_train = True\n",
    "model = load_checkpoint(ckpt_path_name, model)\n",
    "\n",
    "lines = get_test_data(data_type=data_type, start=start)\n",
    "print(lines[:10])\n",
    "test_model(model, reverse_c=reverse_c, precise=True, noise_digit=2)\n",
    "test_model(model, reverse_c=reverse_c, precise=False, noise_digit=2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
