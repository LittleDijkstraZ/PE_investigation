{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Basic Experiments\n",
    " - plain, reverse, CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python train.py pe_info/config2_pe/addition/plain/train_addition_bal_nope.py \\\n",
    "#   --ckpt_path_name=\"ckpt_10000.pt\" --out_dir='out/addition_plain_nope' \\\n",
    "#   --data_type='text' --data_format='plain' --dataset='bal' \\\n",
    "#   --train_data_path=\"train_3digit_10000.txt\" --eval_addition=True \\\n",
    "#     --start='FILE:data/bal/test_10000.txt'\n",
    "\n",
    "# !python train.py pe_info/config2_pe/addition/reverse/train_addition_bal_nope.py \\\n",
    "#   --ckpt_path_name=\"ckpt_10000.pt\" --out_dir='out/addition_reverse_nope' \\\n",
    "#   --data_type='text' --data_format='reverse' --dataset='bal' \\\n",
    "#   --train_data_path=\"train_3digit_10000.txt\" --eval_addition=True \\\n",
    "#     --start='FILE:data/bal/test_10000.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Reverse model 10M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python train.py pe_info/config2_pe/addition/ar/train_addition_ar_nope.py \\\n",
    "  --out_dir='out/addition_ar_nope' \\\n",
    "\n",
    "!python train.py config2/addition/ar/train_addition_ar.py \\\n",
    "  --out_dir='out/addition_ar' \\ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain reverse nope\n",
    "!python train.py pe_info/config2_pe/addition/reverse/train_addition_bal_nope.py\n",
    "# retrain reverse \n",
    "!python train.py pe_info/config2_pe/addition/reverse/train_addition_bal.py\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nope Model on reverse with different residual settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse nope\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=True\n",
    "# reverse original\n",
    "# bug\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=True \n",
    "\n",
    "# reverse list\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[0,1,2,]\n",
    "# reverse list\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[0,1,2,2,2,2]\n",
    "# reverse list\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[0,1,2,3,4]\n",
    "# reverse list\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[0,0,0,0,0,0,1,2,3,4]\n",
    "# reverse list\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py  --use_pe='nope' --use_residual=[1,2,3,4,5]\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py  --use_pe='nope' --use_residual=[2,3,4,5]\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py  --use_pe='nope' --use_residual=[1,2,3,4,]\n",
    "\n",
    "## original\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=[1,2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py   --use_pe='original' --use_residual=[1,2,3,4,5]\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py   --use_pe='original' --use_residual=[0,1,2,]\n",
    "\n",
    "\n",
    "## alternatives: no att residual\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=True --no_att_residual=True \\\n",
    "  --out_dir='out2/addition_reverse_newresiduals_101'\\\n",
    "  --wandb_run_name='addition_reverse_newresiduals_101'\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=True --no_mlp_residual=True \\\n",
    "  --out_dir='out2/addition_reverse_newresiduals_110'\\\n",
    "  --wandb_run_name='addition_reverse_newresiduals_110'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=True --no_att_residual=True \\\n",
    "  --out_dir='out2/addition_reverse_newresiduals_101'\\\n",
    "  --wandb_run_name='addition_reverse_newresiduals_101'\n",
    "\n",
    "# !python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "#   --use_pe='nope' --use_residual=True --no_mlp_residual=True \\\n",
    "#   --out_dir='out2/addition_reverse_newresiduals_110'\\\n",
    "#   --wandb_run_name='addition_reverse_newresiduals_110'\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=True \\\n",
    "  --out_dir='out2/addition_reverse_newresiduals_111'\\\n",
    "  --wandb_run_name='addition_reverse_newresiduals_111'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bug Fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[1,2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "# ====\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[0,2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='nope' --use_residual=[0,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=[1,2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=[1,2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed_repeat' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed_repeat'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=[2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=[0,2,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "  \n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=[0,1,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=True --no_att_residual=True \\\n",
    "  --out_dir='out2/addition_reverse_newresiduals_101_bugfixed'\\\n",
    "  --wandb_run_name='addition_reverse_newresiduals_101_bugfixed'\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=True --no_mlp_residual=True \\\n",
    "  --out_dir='out2/addition_reverse_newresiduals_110_bugfixed'\\\n",
    "  --wandb_run_name='addition_reverse_newresiduals_110_bugfixed'\n",
    "\n",
    "# ====\n",
    "\n",
    "\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' --use_residual=[0,3,4,5] \\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3]\n",
    "# map(lambda x: x+1, l).tolist()\n",
    "''.join(map(str, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layerwise pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py pe_info/config2_pe/addition/reverse/jason_train_addition_bal.py \\\n",
    "  --use_pe='original' \\\n",
    "  --use_residual=[0,2,3,4,5] \\\n",
    "  --layerwise_pe=[1]\\\n",
    "  --out_dir='out2/addition_reverse_bugfixed' \\\n",
    "  --wandb_run_name='addition_reverse_bugfixed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# finetuning reverse nope\n",
    "!python train.py pe_info/config2_pe/multi_digit/reverse/finetune_nope.py \\\n",
    "  --resume_dir='out2/addition_reverse_nope/ckpt_10000_final.pt' \n",
    "\n",
    "\n",
    "# finetuining reverse original \n",
    "!python train.py pe_info/config2_pe/multi_digit/reverse/finetune.py \\\n",
    "  --resume_dir='out2/addition_reverse/ckpt_10000_final.pt' \n",
    "  \n",
    "# repeat the above with a different randome seed\n",
    "!python train.py pe_info/config2_pe/multi_digit/reverse/finetune.py \\\n",
    "  --out_dir='out2_multidigit/digit_7/reverse_1' \\\n",
    "  --resume_dir='out2/addition_reverse/ckpt_10000_final.pt'\n",
    "  \n",
    "# repeat with longer training time\n",
    "!python train.py pe_info/config2_pe/multi_digit/reverse/finetune.py \\\n",
    "  --out_dir=\"out2_multidigit/digit_7/reverse_1_long\" \\\n",
    "  --max_iters=20000 --lr_decay_iters=20000 \\\n",
    "  --resume_dir=\"out2/addition_reverse/ckpt_10000_final.pt\"\n",
    "\n",
    "# traiing nope for longer (both overfits)\n",
    "!python train.py pe_info/config2_pe/multi_digit/reverse/finetune_nope.py \\\n",
    "  --out_dir='out2_multidigit/digit_7/reverse_nope_1_long' \\\n",
    "  --resume_dir='out2/addition_reverse_nope/ckpt_10000_final.pt' \\\n",
    "  --max_iters=20000 --lr_decay_iters=20000 --general_seed=42 \\\n",
    "\n",
    "# 7 digit from scratch (changing the inner arguments)\n",
    "!python train.py pe_info/config2_pe/addition/reverse/train_addition_digit.py \n",
    "!python train.py pe_info/config2_pe/addition/reverse/train_addition_digit.py \\\n",
    "  --use_pe='nope'\n",
    "\n",
    "!python train.py pe_info/config2_pe/addition/reverse/train_addition_digit.py \\\n",
    "  --use_pe='nope' --use_residual=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Subtraction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/dijkstraz/Thesis/train.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# max_iters of 10000 is two times the original\n",
    "# evaluation is different from gpt\n",
    "# 0.99 reverse normal gpt2\n",
    "# !python train.py pe_info/config_gpt2_pe/addition/plain/train_addition_bal_nope.py\n",
    "# 0.99 reverse nope gpt2\n",
    "# !python train.py pe_info/config_gpt2_pe/addition/reverse/train_addition_dollar_reverse.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Experiements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- fix 198 not found error\n",
    "- fix parameter size discrepency error\n",
    "- add space to left or right\n",
    "- train on longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "number of parameters: 123.69M\n"
     ]
    }
   ],
   "source": [
    "from teaching_arithmetic_pe.model import GPT, GPTConfig\n",
    "model = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo.ipynb  teaching_arithmetic_pe\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['_orig_mod.transformer.wte.weight', '_orig_mod.transformer.h.0.ln_1.weight', '_orig_mod.transformer.h.0.attn.c_attn.weight', '_orig_mod.transformer.h.0.attn.c_proj.weight', '_orig_mod.transformer.h.0.ln_2.weight', '_orig_mod.transformer.h.0.mlp.c_fc.weight', '_orig_mod.transformer.h.0.mlp.c_proj.weight', '_orig_mod.transformer.h.1.ln_1.weight', '_orig_mod.transformer.h.1.attn.c_attn.weight', '_orig_mod.transformer.h.1.attn.c_proj.weight', '_orig_mod.transformer.h.1.ln_2.weight', '_orig_mod.transformer.h.1.mlp.c_fc.weight', '_orig_mod.transformer.h.1.mlp.c_proj.weight', '_orig_mod.transformer.h.2.ln_1.weight', '_orig_mod.transformer.h.2.attn.c_attn.weight', '_orig_mod.transformer.h.2.attn.c_proj.weight', '_orig_mod.transformer.h.2.ln_2.weight', '_orig_mod.transformer.h.2.mlp.c_fc.weight', '_orig_mod.transformer.h.2.mlp.c_proj.weight', '_orig_mod.transformer.h.3.ln_1.weight', '_orig_mod.transformer.h.3.attn.c_attn.weight', '_orig_mod.transformer.h.3.attn.c_proj.weight', '_orig_mod.transformer.h.3.ln_2.weight', '_orig_mod.transformer.h.3.mlp.c_fc.weight', '_orig_mod.transformer.h.3.mlp.c_proj.weight', '_orig_mod.transformer.h.4.ln_1.weight', '_orig_mod.transformer.h.4.attn.c_attn.weight', '_orig_mod.transformer.h.4.attn.c_proj.weight', '_orig_mod.transformer.h.4.ln_2.weight', '_orig_mod.transformer.h.4.mlp.c_fc.weight', '_orig_mod.transformer.h.4.mlp.c_proj.weight', '_orig_mod.transformer.h.5.ln_1.weight', '_orig_mod.transformer.h.5.attn.c_attn.weight', '_orig_mod.transformer.h.5.attn.c_proj.weight', '_orig_mod.transformer.h.5.ln_2.weight', '_orig_mod.transformer.h.5.mlp.c_fc.weight', '_orig_mod.transformer.h.5.mlp.c_proj.weight', '_orig_mod.transformer.ln_f.weight', '_orig_mod.lm_head.weight'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "Using Flash Attention\n",
      "number of parameters: 29.94M\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'load_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/dijkstraz/Thesis/demo.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/Thesis/demo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m config\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mupdate(config_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/Thesis/demo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m GPT(config)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/Thesis/demo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload_state_dict(ckpt_path))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'load_state_dict'"
     ]
    }
   ],
   "source": [
    "from teaching_arithmetic_pe.pe_info.model_nope import GPT, GPTConfig\n",
    "import torch\n",
    "\n",
    "ckpt_path = './teaching_arithmetic_pe/out/addition_ar_nope/ckpt_acc.pt'\n",
    "\n",
    "config = GPTConfig()\n",
    "# config.__dict__.\n",
    "import yaml\n",
    "with open('./teaching_arithmetic_pe/out/addition_ar_nope/algo_reasoning/config.yaml') as f:\n",
    "  config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# config.__dict__ = config_dict\n",
    "config.__dict__.update(config_dict)\n",
    "model = GPT(config)\n",
    "model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/dijkstraz/Thesis/demo.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/Thesis/demo.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mteaching_arithmetic_pe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmain_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_trained_model\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/Thesis/demo.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dijkstraz/Thesis/demo.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./teaching_arithmetic_pe/out/addition_ar_nope/algo_reasoning/config.yaml\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/home/dijkstraz/Thesis/teaching_arithmetic_pe/main_utils.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m GPTConfig, GPT\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_trained_model\u001b[39m(config, checkpoint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     init_from \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39minit_from\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "from teaching_arithmetic_pe.main_utils import load_trained_model\n",
    "\n",
    "device = 'cuda'\n",
    "with open('./teaching_arithmetic_pe/out/addition_ar_nope/algo_reasoning/config.yaml') as f:\n",
    "  config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "ckpt_name = './teaching_arithmetic_pe/out/addition_ar_nope/ckpt_acc.pt'\n",
    "checkpoint = torch.load(ckpt_name, map_location=device)\n",
    "model = load_trained_model(config, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(ckpt_path, model):\n",
    "        # load ckpt into model\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "        checkpoint_model_args = checkpoint['model_args']\n",
    "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                model_args[k] = checkpoint_model_args[k]\n",
    "        # create the model\n",
    "        gptconf = GPTConfig(**model_args)\n",
    "        model = GPT(gptconf)\n",
    "        state_dict = checkpoint['model']\n",
    "        # fix the keys of the state dictionary :(\n",
    "        # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "                if k.startswith(unwanted_prefix):\n",
    "                        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
