{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 8]\n",
      "[1,2,8]\n"
     ]
    }
   ],
   "source": [
    "x = 128\n",
    "def list_to_string(a):\n",
    "    a = str(a)\n",
    "    return a.replace(' ', '')\n",
    "\n",
    "def num_to_list(num):\n",
    "    return [int(x) for x in str(num)]\n",
    "\n",
    "list_x = num_to_list(x)\n",
    "print(list_x)\n",
    "print(list_to_string(list_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_string(x,y):\n",
    "    x, y = str(x), str(y)\n",
    "    input_str = f'Input:\\n{x}+{y}\\n'\n",
    "\n",
    "    return input_str\n",
    "\n",
    "def get_output_string(x,y):\n",
    "    x, y = str(x), str(y)\n",
    "\n",
    "    len_x, len_y = len(x), len(y)\n",
    "    list_x, list_y = num_to_list(x), num_to_list(y)\n",
    "\n",
    "    output_str = f'Target:\\n'\n",
    "\n",
    "    # output_str += f'{list_to_string(list_x)} has {len_x} digits.\\n'\n",
    "    # output_str += f'{list_to_string(list_y)} has {len_y} digits.\\n'\n",
    "\n",
    "    C=0\n",
    "    A=[]\n",
    "    for i in range(max(len_x, len_y)):\n",
    "        a = list_x[-1] if i < len_x else 0\n",
    "        b = list_y[-1] if i < len_y else 0\n",
    "        c = a + b + C\n",
    "        \n",
    "        output_str += f'A->{c%10} , C->{c//10}\\n'\n",
    "\n",
    "        A.insert(0, c % 10)\n",
    "        C = c // 10\n",
    "\n",
    "        list_x = list_x[:-1]\n",
    "        list_y = list_y[:-1]\n",
    "\n",
    "    # output_str += f'{list_to_string(list_x)} + {list_to_string(list_y)} , A={list_to_string(A)} C={C} , END\\n</scratch>\\n'\n",
    "    if C == 1:\n",
    "        A.insert(0, 1)\n",
    "    output_str = output_str[:-1] + f'.\\n'\n",
    "    for a in A:\n",
    "        output_str += f'{a}'\n",
    "\n",
    "    return output_str+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "128+367\n",
      "Target:\n",
      "A->5 , C->1\n",
      "A->9 , C->0\n",
      "A->4 , C->0.\n",
      "495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x,y = 128, 367\n",
    "input_str = get_input_string(x,y)\n",
    "output_str = get_output_string(x,y)\n",
    "\n",
    "print(input_str+output_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 59,864\n",
      "all the unique characters: \n",
      " +,-.0123456789:>ACITaegnprtu\n",
      "vocab size: 30\n",
      "train has 59,864 tokens\n",
      "length of dataset in characters: 122,881\n",
      "all the unique characters: \n",
      " +,-.0123456789:>ACITaegnprtu\n",
      "vocab size: 30\n",
      "train has 122,881 tokens\n",
      "length of dataset in characters: 185,622\n",
      "all the unique characters: \n",
      " +,-.0123456789:>ACITaegnprtu\n",
      "vocab size: 30\n",
      "train has 185,622 tokens\n",
      "length of dataset in characters: 248,433\n",
      "all the unique characters: \n",
      " +,-.0123456789:>ACITaegnprtu\n",
      "vocab size: 30\n",
      "train has 248,433 tokens\n",
      "length of dataset in characters: 311,354\n",
      "all the unique characters: \n",
      " +,-.0123456789:>ACITaegnprtu\n",
      "vocab size: 30\n",
      "train has 311,354 tokens\n",
      "length of dataset in characters: 1,254,344\n",
      "all the unique characters: \n",
      " +,-.0123456789:>ACITaegnprtu\n",
      "vocab size: 30\n",
      "train has 1,254,344 tokens\n"
     ]
    }
   ],
   "source": [
    "num_sample_list = [1000,2000,3000,4000,5000,20000]\n",
    "\n",
    "for num_samples in num_sample_list:\n",
    "    input_file_path = f'original_balanced_data/add_examples_{num_samples}_shuffled.txt'\n",
    "    output_file_path = f'add_examples_algorithmic_{num_samples}.txt'\n",
    "\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for line in lines:\n",
    "            x,y = re.split('\\$|\\+|=',line)[1:3]\n",
    "            x, y = int(x), int(y)\n",
    "            input_str = get_input_string(x,y)\n",
    "            output_str = get_output_string(x,y)\n",
    "            f.write(input_str+output_str)\n",
    "\n",
    "    input_file_path = f'add_examples_algorithmic_{num_samples}.txt'\n",
    "\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "    # get all the unique characters that occur in this text\n",
    "    chars = sorted(list(set(data)))\n",
    "    vocab_size = len(chars)\n",
    "    print(\"all the unique characters:\", ''.join(chars))\n",
    "    print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    def encode(s):\n",
    "        return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "    def decode(l):\n",
    "        ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "    # create the train and test splits\n",
    "    # n = len(data) # 130,023\n",
    "    # train_data = data[:int(n*0.9)]\n",
    "    # val_data = data[int(n*0.9):]\n",
    "\n",
    "    # encode both to integers\n",
    "    train_ids = encode(data)\n",
    "    # val_ids = encode(val_data)\n",
    "    print(f\"train has {len(train_ids):,} tokens\")\n",
    "    # print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "    # export to bin files\n",
    "    train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "    # val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "    train_ids.tofile(f'train_balanced_{num_samples}.bin')\n",
    "    # val_ids.tofile(f'val_balanced.bin')\n",
    "\n",
    "    # save the meta information as well, to help us encode/decode later\n",
    "    meta = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'itos': itos,\n",
    "        'stoi': stoi,\n",
    "    }\n",
    "    with open(f'meta.pkl', 'wb') as f:\n",
    "        pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2ba5c7570592e86f20458013332ec3c55b0ae10b9d5b24be29cc9bc24858e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
